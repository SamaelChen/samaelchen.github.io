<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>碎碎念</title>
  
  <subtitle>好久不更新，四月重新开始！</subtitle>
  <link href="https://samaelchen.github.io/atom.xml" rel="self"/>
  
  <link href="https://samaelchen.github.io/"/>
  <updated>2021-08-31T16:15:25.581Z</updated>
  <id>https://samaelchen.github.io/</id>
  
  <author>
    <name>Samael Chen</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>NER</title>
    <link href="https://samaelchen.github.io/ner/"/>
    <id>https://samaelchen.github.io/ner/</id>
    <published>2020-05-17T16:00:00.000Z</published>
    <updated>2021-08-31T16:15:25.581Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>暌违一年的更新， 最近用到NER相关的算法，简单记录一下，主要是HMM和CRF。感觉概率图比较牛逼。</p><span id="more"></span><h1 id="ner发展">NER发展</h1><p>NER是NLP里面一个非常基础的任务，从NLP的处理流程上看，NER可以看做是词法分析中未登录词的一种。同时NER也是关系抽取、事件抽取、知识图谱、机器翻译、问答系统等诸多NLP任务的基础。一般而言NER主要是识别人们、地名、组织机构等，常见的NER算法大赛就是这样。实际上任何我们想要的特殊文本片段都可以认为是实体。比如价格、产品型号等。</p><p>NER的发展基本上是四个阶段，最初是基于规则和字典的方法，依赖专家构建复杂的词库，通过分词器和正则表达式等方式抽取。第二阶段就是以HMM和CRF为代表的机器学习时代，第三阶段是CNN+CRF或者RNN+CRF的方式，第四阶段也就是现在基本上是半监督或者Attention等深度学习方法。</p><h1 id="马尔科夫链">马尔科夫链</h1><p>一般而言，我们假设<span class="math inline">\(X\)</span>是一个随机数据集合<span class="math inline">\(\{X_1, X_2, \ldots, X_t\}\)</span>，这些值源自状态集合<span class="math inline">\(S=\{s_1, \ldots, s_N\}\)</span>。一个马尔科夫链满足下面两个条件： <span class="math display">\[\begin{matrix}P(X_{t+1} = s_k|X_1,\ldots,X_t) = P(X_{t+1} = s_k|X_t) &amp; \text{Limited horizon} \\P(X_2=s_k|X_1=s_j) = P(X_{t+1} = s_k|X_t = s_j), \forall t,k,j &amp; \text{Time invariant}\end{matrix}\]</span> 一个马尔科夫链会有一个转移矩阵来表示从每一个状态转移到下一个状态的概率，同时有一个初始概率来表示第一个时刻每个状态的概率。假设我们有两个状态0和1，有一个转移矩阵： <span class="math display">\[\begin{array}{|c|c|c|} \hline \ &amp; 0 &amp; 1 \\\hline0 &amp; 0.3 &amp; 0.7 \\\hline1 &amp; 0.6 &amp; 0.4 \\\hline\end{array}\]</span> 初始概率<span class="math inline">\(P(S = 0)=0.2, P(S=1)=0.8\)</span>，那么对于序列1011，我们就可以很容易算出来概率是<span class="math inline">\(0.8 \times 0.6 \times 0.7 \times 0.4=0.1344\)</span></p><h1 id="hmm">HMM</h1><p>那么隐马尔可夫又是什么呢？上面的马尔科夫是一个可以直接观测到的状态转移序列。那么现在存在一种序列，表面上是我们可以观测到的随机序列，但是背后却有我们无法得知的隐藏序列来生成这一个序列。比如恋爱的经典笑话。 <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">男：你怎么了？</span><br><span class="line">女：没事。</span><br><span class="line">男：你真的没事？</span><br><span class="line">女：真的，你睡吧。</span><br><span class="line">男：你确定没事？</span><br><span class="line">女：真的。</span><br><span class="line">男：好吧，那我睡了。</span><br><span class="line">转头女的发了朋友圈，终究还是一个人扛下了所有。</span><br><span class="line">男：到底发生了什么。</span><br><span class="line">女：没事。</span><br><span class="line">男：你不说我没法睡觉。</span><br><span class="line">女：你睡你的。</span><br><span class="line">男：好吧。</span><br><span class="line">女的发了第二条朋友圈，果然还是没有人理解我。</span><br></pre></td></tr></table></figure> 于是，”没事-真的没事-真的-没事”这种序列背后隐藏了怎样的序列呢？马尔科夫链无法解决，所以需要HMM这样的模型来学习隐藏的状态序列。</p><p>一个HMM有两个序列，一个是观测序列<span class="math inline">\(O\)</span>，一个是隐藏序列<span class="math inline">\(H\)</span>。HMM要满足以下假设： <span class="math display">\[\begin{cases}P(H_t=h_t|H_{1:t-1}=h_{1:t-1}, O_{1:t} = o_{1:t}) = P(H_t=h_t | H_{t-1} = h_{t-1}) &amp; \text{Markovinanity} \\P(O_t = o_t|H_{1:t} = h_{1:t}, O_{1:t-1}=o_{1:t-1}) = P(O_t=o_t|H_t=h_t) &amp; \text{Output independence} \\P(H_t=j|H_{t-1}=i) = P(H{t+s}=j|H_{t+s-1}=i), \forall i,j \in H &amp; \text{Stationarity}\end{cases}\]</span></p><p>一个完整的HMM包含三个要素，transition matrix <span class="math inline">\(A\)</span>，emission matrix <span class="math inline">\(B\)</span>，还有初始状态分布概率<span class="math inline">\(\Pi\)</span>，可以将HMM表示为<span class="math inline">\(\lambda = (A, B, \Pi)\)</span>。</p><p>那么HMM就有三个问题需要解决，一个是概率计算问题，也就是likelihood，第二个是参数学习问题，第三个是序列的解码问题。</p><h2 id="hmm-likelihood">HMM likelihood</h2><p>要计算一个HMM生成序列的概率，首先想到的就是暴力解法，穷举所有可能状态的组合，那么通过暴力运算就可以将所有的可能性算出来。但是暴力运算的问题在于计算复杂度过高，复杂度达到<span class="math inline">\(O(TN^T)\)</span>。所以一般解法有两种，一种是前向算法，另一种是后向算法。</p><p>前向算法的过程很简单，首先初始化各个状态下在时间1时候观测状态为o_1的概率，<span class="math inline">\(\alpha(i) = \pi_i b_i(o_1)\)</span>，然后递归求解，<span class="math inline">\(\alpha_{t+1}(j) = \Big[ \sum\limits_{i=1}^N \alpha_t a_{ij} \Big] b_j(o_{t+1})\)</span>，最后到了<span class="math inline">\(T\)</span>时刻，<span class="math inline">\(P(O|\lambda) = \sum\limits_{i=1}^N \alpha_T(i)\)</span>。这样的话复杂度就降低到了<span class="math inline">\(O(TN^2)\)</span>的水平。因为每次只计算两个时刻之间的所有可能性。</p><p>这里演示一个简单的前向算法计算过程，假设有红白两种颜色的球，分别有三个盒子。我们可以观测到的球的颜色，隐藏的是球来自哪个盒子。初始概率<span class="math inline">\(\Pi = (0.2, 0.4, 0.4)\)</span>，transition matrix <span class="math inline">\(A = \begin{bmatrix} 0.5 &amp; 0.2 &amp; 0.3 \\ 0.3 &amp; 0.5 &amp; 0.2 \\ 0.2 &amp; 0.3 &amp; 0.5 \end{bmatrix}\)</span>，emission matrix <span class="math inline">\(\begin{bmatrix}0.5 &amp; 0.5 \\ 0.4 &amp; 0.6 \\ 0.7 &amp; 0.3 \end{bmatrix}\)</span>，观测到的序列是<span class="math inline">\(O=\{红，白，红\}\)</span>，所以分步计算如下：</p><p>第一步，初始化。 <span class="math display">\[\alpha_1(1) = \pi_1 b_1(o_1) = 0.2 \times 0.5 = 0.1, \ \alpha_1(2) = \pi_2 b_2(o_1) = 0.4 \times 0.4 = 0.16, \ \alpha_1(3) = \pi_3 b_3(o_1) = 0.4 \times 0.7 = 0.28\]</span></p><p>第二步，递归。时刻2的观测状态是白球，所以时刻2来自盒子1的概率是<span class="math display">\[\alpha_2(1) = \Big[\sum\limits_{i=1}^3 \alpha_1(i) a_{i1}\Big] b_2(o_2) = (0.1 \times 0.5 + 0.16 \times 0.3 + 0.28 \times 0.2) \times 0.5 = 0.077\]</span>其他盒子类推，得到<span class="math display">\[\alpha_2(2) = 0.1104, \ \alpha_2(3) = 0.0606\]</span> 重复第二步，<span class="math display">\[\alpha_3(1) = 0.04187, \ \alpha_3(2) = 0.03551, \ \alpha_3(3) = 0.05284\]</span></p><p>最后我们得到序列的概率<span class="math inline">\(P(O|\lambda) = \sum\limits_{i=1}^3 \alpha_3(i) = 0.13022\)</span>。</p><p>那么后向算法与前向算法类似，但是计算起来相对比较反直觉一点。一样的初始化每个状态最后一个时刻的概率<span class="math inline">\(\beta_T(i) = 1, i=1, 2, \ldots, N\)</span>。接着根据<span class="math inline">\(t+1\)</span>时刻的后向概率，递归计算前一个时刻每个隐藏状态的后向概率。也就是<span class="math inline">\(\beta_t(i) = \sum\limits_{j=1}^N a_{ij}b_j(o_{t+1}) \beta_{t+1}(j), i=1,2,\ldots,N\)</span>。最后<span class="math inline">\(P(O|\lambda) = \sum\limits_{i=1}^N \pi_i b_i(o_1) \beta_1(i)\)</span>。</p><p>一般来说用一个解法来算概率就好了，可以将这两种统一到一个公式上，也就是<span class="math inline">\(P(O|\lambda) = \sum\limits_{i=1}^N \sum\limits_{i=1}^N \alpha_{t}(i) a_{ij}b_j(o_{t+1}) \beta_{t+1}(j), t=1,2,\ldots,T-1\)</span>。</p><h2 id="hmm-learning-problem">HMM learning problem</h2><p>HMM的参数学习有两种一种是有监督学习，一种是无监督学习。</p><p>有监督学习比较简单，因为HMM是生成模型，所以有监督学习直接根据标注的隐藏状态计算频率就可以了。也就是<span class="math inline">\(a_{ij} = \frac{A_{ij}}{\sum_{j=1}^N A_{ij}}, i=1,2,\ldots,N; j=1,2,\ldots,N\)</span>，<span class="math inline">\(b_i(k) = \frac{B_{ik}}{\sum_{k=1}^M B_{ik}}, i=1,2,\ldots,N;\)</span>，<span class="math inline">\(\pi_i = \frac{Count(h_i)}{\sum_{j=1}^N Count(h_j)}\)</span>。</p><p>另一种是用EM算法做无监督学习。一般HMM用的是Baum-Welch算法。</p><p>EM算法就包括了两个步骤，一个是E，一个是M。我们假设有一个数据集合是<span class="math inline">\(\{O_1, O_2, \ldots, O_S\}\)</span>，<span class="math inline">\(O_i = o_{i_1}, o_{i_2}, \ldots, o_{i_T}\)</span>，<span class="math inline">\(H_i = h_{i_1}, h_{i_2}, \ldots, h_{i_T}\)</span>，为了方便区分，下面用上标来表示隐藏状态的index。<span class="math inline">\(O=\{o^1, o^2, \ldots, o^M\}\)</span>，<span class="math inline">\(H=\{h^1, h^2, \ldots, h^N\}\)</span>。那么E步就是计算<span class="math display">\[Q(\lambda, \bar{\lambda}) = \sum\limits_{H} P(H|O,\bar{\lambda}) \log P(O,H|\bar{\lambda})\]</span> M步就是找到一个<span class="math inline">\(\bar{\lambda}\)</span>使得上面的期望最大，也就是 <span class="math display">\[\bar{\lambda} = \arg \max_{\lambda} \sum\limits_H P(H|O,\bar{\lambda})\log P(O,H|\lambda)\]</span></p><p>那么<span class="math inline">\(Q\)</span>函数可以改写成： <span class="math display">\[\sum\limits_{H} P(H|O,\bar{\lambda}) \log P(O,H|\bar{\lambda}) = \sum\limits_{H} \frac{P(H,O|\bar{\lambda})}{P(O|\bar{\lambda})} \log P(O,H|\lambda)\]</span> 因为P(O|{})是常数，所以上面等价于 <span class="math display">\[\sum\limits_{H} P(H,O|\bar{\lambda}) \log P(O,H|\bar{\lambda})\]</span> 因为<span class="math inline">\(P(O,H|\lambda) = \pi_{h_1}b_{h_1}(o_1)a_{h_1h_2}b_{h_2}(o_2) \cdots a_{h_{T-1}h_T}b_{h_T}(o_T)\)</span>，所以最后将公式可以替换为： <span class="math display">\[Q(\lambda, \bar{\lambda}) = \sum\limits_{H}P(O,H|\bar{\lambda}) \log \pi_{h_1} + \sum\limits_{H}(\sum\limits_{t=1}^{T-1} \log a_{h_t h_{t+1}})P(O,H|\bar{\lambda}) + \sum\limits_{H}(\sum\limits_{t=1}^T \log b_{h_1}(o_t))P(O,H|\bar{\lambda})\]</span></p><p>那么分步求偏导，我们对第一个部分求偏导， <span class="math display">\[\sum\limits_{H} \log \pi_{h_1} P(O,H| \bar{\lambda}) = \sum\limits_{i=1}^N \log \pi^{i} P(O, h_1 = h^i | \bar{\lambda})\]</span> 由于<span class="math inline">\(\sum_{i=1}^N \pi^i = 1\)</span>，所以这是受限制的求解极值问题，用拉格朗日乘子法构建拉格朗日函数如下： <span class="math display">\[\sum\limits_{i=1}^N \log \pi^i P(O,h_1 = h^i | \bar{\lambda}) + \gamma(\sum\limits_{i=1}^N \pi^i - 1)\]</span> 接着求导： <span class="math display">\[\frac{\partial}{\partial\pi^i}[\sum\limits_{i=1}^N\log\pi^i P(O,h_1=h^i|\bar{\lambda})+\gamma(\sum\limits_{i=1}^N\pi^i-1)]=P(O,h_1=h^i|\bar{\lambda})+\gamma\pi^i\]</span> 让上式等0，且因为有N个，全部求和就可以得到<span class="math inline">\(\gamma\)</span>值也就是<span class="math inline">\(\gamma=-P(O|\bar{\lambda})\)</span>。</p><p>所以<span class="math inline">\(\pi^i = \frac{P(O,h_1 = h^i|\bar{\lambda})}{P(O|\bar{\lambda})}\)</span>。</p><p>然后按照一样的方法求第二部分： <span class="math display">\[\sum\limits_{H}(\sum\limits_{t=1}^{T-1}\log a_{h_th_{t+1}})P(O,H|\bar{\lambda})=\sum\limits_{i=1}^N\sum\limits_{j=1}^N\sum\limits_{t=1}^{T-1}\log a_{ij}P(O,h_t=h^i,h_{t+1}=h^j|\bar{\lambda}),\sum\limits_{j=1}^N a_{ij}=1\]</span></p><p>我们设定拉格朗日函数为 <span class="math display">\[L=\sum\limits_{i=1}^N\sum\limits_{j=1}^N\sum\limits_{t=1}^{T-1}\log a_{ij}P(O,h_t=h^i,h_{t+1}=h^j|\bar{\lambda})+\sum\limits_{i=1}^N\gamma_i(\sum\limits_{j=1}^N a_{ij}-1)\]</span></p><p>然后一样求偏导 <span class="math display">\[\frac{\partial L}{\partial a_{ij}}=\sum\limits_{t=1}^{T-1}P(O,h_t=h^i,h_{t+1}=h^j|\bar{\lambda})+a_{ij}\sum\limits_{i=1}^N\gamma_i=0\]</span></p><p>同样通过求和得到<span class="math inline">\(\sum\limits_{i=1}^N\gamma_i = -\sum\limits_{j=1}^N\sum\limits_{t=1}^{T-1}P(O,h_t=h^i,h_{t+1}=h^j|\bar{\lambda})\)</span></p><p>最后得到 <span class="math display">\[a_{ij}=\frac{\sum\limits_{t=1}^{T-1}P(O,h_t=h^i,h_{t+1}=h^j|\bar{\lambda})}{\sum\limits_{j=1}^N\sum\limits_{t=1}^{T-1}P(O,h_t=h^i,h_{t+1}=h^j|\bar{\lambda})}=\frac{\sum\limits_{t=1}^{T-1}P(O,h_t=h^i,h_{t+1}=h^j|\bar{\lambda})}{\sum\limits_{t=1}^{T-1}P(O,h_t=h^i|\bar{\lambda})}\]</span></p><p>现在求最后一部分 <span class="math display">\[\sum\limits_{H}(\sum\limits_{t=1}^{T}\log b_{h_t}(o_t)) P(O,H|\bar{\lambda})=\sum\limits_{i=1}^N \sum\limits_{k=1}^M \sum\limits_{t=1}^T \log b_{ik}P(O,h_t=h^i,o_t=o^k|\bar{\lambda}),\sum\limits_{k=1}^M b_{ik}=1\]</span></p><p>构造拉格朗日函数 <span class="math display">\[L=\sum\limits_{i=1}^N \sum\limits_{k=1}^M \sum\limits_{t=1}^T \log b_{ik}P(O,h_t=h^i,o_t=o^k|\bar{\lambda})+\sum\limits_{i=1}^N \gamma_i(\sum\limits_{k=1}^Mb_{jk}-1)\]</span></p><p>求偏导 <span class="math display">\[\frac{\partial L}{\partial b_{ik}}=\sum\limits_{t=1}^T P(O,h_t=h^i,o_t=o^k|\bar{\lambda})+b_{ik}\sum\limits_{i=1}^N\gamma_i=0\]</span></p><p>求和得到<span class="math inline">\(\sum\limits_{i=1}^N\gamma_i = -\sum\limits_{k=1}^M\sum\limits_{t=1}^T P(O,h_t=h^i,o_t=o^k|\bar{\lambda})\)</span></p><p>最后可以得到 <span class="math display">\[b_{ik}=\frac{\sum\limits_{t=1}^T P(O,h_t=h^i,o_t=o^k|\bar{\lambda})}{\sum\limits_{k=1}^M\sum\limits_{t=1}^T P(O,h_t=h^i,o_t=o^k|\bar{\lambda})}=\frac{\sum\limits_{t=1}^T P(O,h_t=h^i,o_t=o^k|\bar{\lambda})}{\sum\limits_{t=1}^T P(O,h_t=h^i|\bar{\lambda})}\]</span></p><p>按照之前的向前先后算法，计算<span class="math inline">\(t\)</span>时刻处于隐藏状态<span class="math inline">\(h^i\)</span>的概率为<span class="math inline">\(\gamma\)</span>，而<span class="math inline">\(\xi\)</span>表示<span class="math inline">\(t\)</span>时刻从<span class="math inline">\(h^i\)</span>转移到<span class="math inline">\(h^j\)</span>的概率。 <span class="math display">\[P(O,h_t=h^i|\lambda)=\gamma_t(i)=\frac{\alpha_t(i)\beta_t(i)}{\sum_{j=1}^N\alpha_t(j)\beta_t(j)}\]</span></p><p><span class="math display">\[P(O,h_t=h^i,h_{t+1}=h^j|\lambda)=\xi_t(i,j)=\frac{\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}{\sum\limits_{i=1}^N\sum\limits_{j=1}^N\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}\]</span></p><p>将上面的两个式子带入之前的偏导结果里面，就得到我们想要的参数了。</p><h2 id="hmm-decoding-problem">HMM decoding problem</h2><p>最后就是HMM的解码问题，一般来说解码都是用viterbi算法来完成。实践上就是每一步都取最大的可能性，然后记下上一个时刻是哪一个隐藏状态有最大的可能性转移到当前状态。</p><p>过程就是</p><p>1、<span class="math inline">\(\delta_1(i) = \pi_i b_i(o_1), i = 1,2,\ldots, N \\ \psi_1(i) = 0, i = 1, 2, \ldots, N\)</span></p><p>2、<span class="math inline">\(\delta_t(i) = \max\limits_{1 \leqslant j \leqslant N}[\delta_{t-1}(j) \alpha_{ji}] b_i(o_t) \\ \psi_t(i) = \arg \max\limits_{1 \leqslant j \leqslant N}[\delta_{t-1}(j)\alpha_{ji}]\)</span></p><p>3、<span class="math inline">\(P = \max\limits_{1 \leqslant j \leqslant N} \delta_T(i) \\ i_T = \arg\max\limits_{1 \leqslant j \leqslant N}[\delta_T(i)]\)</span></p><h1 id="crf">CRF</h1><p>CRF与HMM要解决的问题是类似的，都是要从观测序列中推测出隐藏序列。与HMM不同，CRF是一个典型的有监督学习算法。同样的CRF有一个transition matrix和一个emission matrix。</p><p>CRF要优化的loss函数就是一个序列生成的最大概率。 <span class="math display">\[P(y|x) = \frac{1}{Z(x)} \exp \Big(\sum\limits_{i,k} \lambda_k t_k(y_{i-1},y_i, x,i) +\sum\limits_{i,l}\mu_ls_l(y_i, x,i)\Big) \\Z(x) =\sum\limits_{y} \exp \Big(\sum\limits_{i,k} \lambda_k t_k(y_{i-1},y_i, x,i) +\sum\limits_{i,l}\mu_ls_l(y_i, x,i)\Big)\]</span></p><p>CRF的序列概率计算方式与HMM类似，也是前向-后向算法。这里举一个简单的例子来说明，假设有三个单词，两个隐藏状态。那么emission matrix和transition matrix如下： <span class="math display">\[\begin{array}{|c|c|c|} \hline \ &amp; l_1 &amp; l_2 \\\hlinew_0 &amp; x_{01} &amp; x_{02} \\\hlinew_1 &amp; x_{11} &amp; x_{12} \\\hlinew_2 &amp; x_{21} &amp; x_{22} \\\hline\end{array}\]</span></p><p><span class="math display">\[\begin{array}{|c|c|c|} \hline \ &amp; l_1 &amp; l_2 \\\hlinel_1 &amp; t_{11} &amp; t_{12} \\\hlinel_2 &amp; t_{21} &amp; t_{22} \\\hline\end{array}\]</span></p><p>现在一步一步来前向传播运算序列概率，目标是算出<span class="math inline">\(log(e^{S_1} + e^{S_2} + \ldots + e^{S_n})\)</span>。</p><p>首先第一步，第一个词是<span class="math inline">\(w_0\)</span>，我们有两个变量，<span class="math inline">\(obs = [x_{01}, x_{02}]\)</span>, <span class="math inline">\(previous = None\)</span>，所以<span class="math inline">\(\text{total_score} = \log(e^{x_{01}} + e^{x_{02}})\)</span>。</p><p>第二步从<span class="math inline">\(w_0 \to w_1\)</span>，<span class="math inline">\(obs = [x_{11}, x_{12}]\)</span>，<span class="math inline">\(previous = [x_{01}, x_{02}]\)</span>，接下来为了计算方便，我们对obs和previous做一个broadcast，得到下面的结果： <span class="math display">\[previous =\begin{bmatrix}x_{01} &amp; x_{01} \\x_{02} &amp; x_{02}\end{bmatrix}\]</span></p><p><span class="math display">\[obs = \begin{bmatrix}x_{11} &amp; x_{12} \\x_{11} &amp; x_{12}\end{bmatrix}\]</span></p><p><span class="math display">\[score = previous + obs + transition = \begin{bmatrix}x_{01} + x_{11} + t_{11} &amp; x_{01} + x_{12} + t_{12} \\x_{02} + x_{11} + t_{21} &amp; x_{02} + x_{22} + t_{22}\end{bmatrix}\]</span></p><p>然后更新previous，得到 <span class="math display">\[previous = [\log(e^{x_{01} + x_{11} + t_{11}} + e^{x_{02} + x_{11} + t_{21}}), \log(e^{x_{01} + x_{12} + t_{12}} + e^{x_{02} + x_{22} + t_{22}})]\]</span></p><p>然后反复迭代得到所有的结果。</p><p>这里有个文章里面的图示非常明显：</p><p><img data-src='https://i.loli.net/2020/05/23/WfS2vaEUcRpnm4C.jpg'></p><p>这样可以完成全部可能路径的概率计算。</p><p>接下来就是跟HMM一样的解码问题，同样采用维特比算法就可以解开隐藏序列。如下图： <img data-src="https://i.loli.net/2020/05/23/VxkRP2KrJlM4zh9.jpg" alt="viterbi1" /> <img data-src="https://i.loli.net/2020/05/23/7iokSZULbumlg59.jpg" alt="viterbi2" /> <img data-src="https://i.loli.net/2020/05/23/Y5G1fDkOIwVndKL.jpg" alt="viterbi3" /> <img data-src="https://i.loli.net/2020/05/23/SgO9lj6z4MywUcG.jpg" alt="viterbi4" /></p><p>这样就完成了CRF的全过程。至于CRF的参数学习过程，只要用梯度下降去学习那个极大似然函数就可以了。</p><h1 id="bilstmcrf">BiLSTM+CRF</h1>实际上对于命名实体识别任务而言，每一个词后面用BIOES标注，那么是不是直接就可以用LSTM来分类了。实际上也是可以的，用BiLSTM来做如下图：<p align="center"><img data-src='https://i.loli.net/2020/05/23/TQK1ZizLWtjERp7.jpg'></p><p>但是纯粹用LSTM来做的话会有一个问题，就是可能输出的分类是不合理的，比如下图：</p><p align="center"><img data-src='https://i.loli.net/2020/05/23/cRfSJDgZGHm7oCT.jpg'></p><p>那么这种时候，如果在上层补上CRF的转移矩阵来做限制，就可以得到合理的结果。</p><p align="center"><img data-src='https://i.loli.net/2020/05/23/DfpUkGuExnHA9av.png'></p><p>总体而言，我习惯把BiLSTM看做是CRF的改良版，用BiLSTM来替代CRF的emission score学习过程，实现比CRF更好的效果。</p><p>具体的代码可以直接看PyTorch的官方教程，不过里面的是单个记录的训练，大规模训练比较慢，我这里改了一版基于batch训练的可以参考。<a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/pytorch/NER/NER-101.ipynb">notebook地址</a>。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;暌违一年的更新， 最近用到NER相关的算法，简单记录一下，主要是HMM和CRF。感觉概率图比较牛逼。&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://samaelchen.github.io/categories/DeepLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>DBSCAN &amp; OPTICS</title>
    <link href="https://samaelchen.github.io/dbscan_and_optics/"/>
    <id>https://samaelchen.github.io/dbscan_and_optics/</id>
    <published>2019-05-01T16:00:00.000Z</published>
    <updated>2021-08-31T16:15:25.576Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>我如期来更新啦！！！聚类算法是很常用的一种算法，不过最常见的就是KMeans了，虽然很多人都会用，不过讲道理，如果是调用现成机器学习库里面的KMeans的话，我敢保证90%的人答不上来具体的是什么算法。相信我，库里的KMeans跟教科书讲的那个随机取初始点的KMeans不是一个算法哟～</p><p>因为KMeans依赖K，但是我怎么知道K要用多少呢？另外，KMeans受限于算法本身，对于球状的数据效果较好，但是不规则形状的就不行了。这种情况下，相对而言，基于密度的聚类算法就比较好用了。sklearn里面现在是放了一个DBSCAN，下一版会更新OPTICS。刚好最近都用了，这里把DBSCAN跟OPTICS算法复现一遍。</p><span id="more"></span><h1 id="dbscan">DBSCAN</h1><p>DBSCAN算法的原始论文是96年的这篇<a href="https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf">《A Density-Based Algorithm for Discovering Clusters in Large Spatial Database with Noise》</a>。</p><p>DBSCAN是一种基于密度的聚类算法，也就是说，密度高的区域自动聚成一类。这样一来，我们就避免了人为去设定群组数量的问题，算法可以自动发现群组数量。另外用这种方法，如果一个sample不在高密度区域，就有可能被判定为异常值，那么也可以拿来作为异常值检验的方法。</p><p>DBSCAN的思路非常简单，有两个参数，一个是<span class="math inline">\(\varepsilon\)</span>，另一个是minimum points。这里首先定义DBSCAN的几个核心概念，一个是<span class="math inline">\(\varepsilon\)</span>-neighborhood，另一个是core object，还有就是reachable。</p><p>首先是<span class="math inline">\(\varepsilon\)</span>-neighborhood。DBSCAN开始的时候会随机选取一个初始点，然后按照<span class="math inline">\(\varepsilon\)</span>的距离寻找临近点，这些点的集合叫做<span class="math inline">\(\varepsilon\)</span>-neighborhood。参数里面的minimum points就是限定这个点的集合的。minimum points限定了这个集合最小需要包含多少样本点，<span class="math inline">\(\varepsilon\)</span>则是限定了要用多大的范围去框定这些样本点。这里有个小细节要注意，那就是，算neighborhood的时候，中心点自己是算进来的。</p><p>那么这样圈完一波neighborhood后，我们会将符合样本数量大于等于minpts的中心点叫做core object。而core object跟这些neighbor就是reachable的。</p><p>然后为了让算法运作起来，只要neighborhood这个集合里面有点，我们就不断重复这样圈地的动作，然后把中心点从集合中拿掉，直到neighborhood为空。</p><p>那我们很自然就会想到，一定会圈到一些点，它们不是core object，但是也在集合里面。这些点我们叫做border，也就是说，这些点是这个类的边界了。那么我们就很自然会想到，也有一些点压根不在neighborhood里面，也不是core的点，这些点就是noise。那既然样本点多了两个状态，reachable的情况也就变得多了，如果是直接可以在neighborhood里面找到的，我们叫做directly-reachable；如果通过neighborhood一层层找，最后找到的，我们叫density-reachable。</p><p>可以看图说话：</p><p align="center"><img data-src='https://upload.wikimedia.org/wikipedia/commons/thumb/a/af/DBSCAN-Illustration.svg/1280px-DBSCAN-Illustration.svg.png' width=50%></p><p>这个图里面，A是core，B、C是border，N是noise。A跟B、C都是density-reachable。但是有没有发现，B是没法返回去找到A的。所以这种reachable是有方向的。</p><p>如果对SNA有点了解的朋友就知道，就是一度人脉和N度人脉，但是是一个有向图。</p><p>那么算法思路理清了，代码就好写了，这里我就用最常用的欧氏距离了。有人可能会想，如果还是欧氏距离，那跟KMeans还有什么分别，都是画圈圈嘛！请回想一个微积分，只要圈画的够小，就能做出各种形状来。下面是核心部分的代码，详细的可以去看我的<a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/fun/DBSCAN.ipynb">notebook</a>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_euclidean_dist</span>(<span class="params">p, q</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.sqrt(np.power(p - q, <span class="number">2</span>).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_eps_reachable</span>(<span class="params">p, q, eps</span>):</span></span><br><span class="line">    <span class="keyword">return</span> _euclidean_dist(p, q) &lt; eps</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_eps_neighborhood</span>(<span class="params">data, point_id, eps</span>):</span></span><br><span class="line">    neighborhood = []</span><br><span class="line">    point = data[point_id]</span><br><span class="line">    <span class="keyword">for</span> q <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(data)):</span><br><span class="line">        <span class="keyword">if</span> _eps_reachable(point, data[q], eps):</span><br><span class="line">            neighborhood.append(q)</span><br><span class="line">    <span class="keyword">return</span> neighborhood</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_is_core_object</span>(<span class="params">neighborhood, minpts</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(neighborhood) &gt;= minpts:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dbscan</span>(<span class="params">data, eps, minpts</span>):</span></span><br><span class="line">    class_id = <span class="number">0</span></span><br><span class="line">    class_label = np.full(<span class="built_in">len</span>(data), -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> p_id <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(data)):</span><br><span class="line">        <span class="keyword">if</span> class_label[p_id] == -<span class="number">1</span>:</span><br><span class="line">            neighborhood = _eps_neighborhood(data, p_id, eps)</span><br><span class="line">        <span class="keyword">if</span> _is_core_object(neighborhood, minpts):</span><br><span class="line">            class_label[neighborhood] = class_id</span><br><span class="line">            neighborhood.remove(p_id)</span><br><span class="line">            <span class="keyword">while</span> <span class="built_in">len</span>(neighborhood) &gt; <span class="number">0</span>:</span><br><span class="line">                current_p = neighborhood[<span class="number">0</span>]</span><br><span class="line">                current_neighborhood = _eps_neighborhood(data, current_p, eps)</span><br><span class="line">                <span class="keyword">for</span> n <span class="keyword">in</span> current_neighborhood:</span><br><span class="line">                    <span class="keyword">if</span> class_label[n] == -<span class="number">1</span>:</span><br><span class="line">                        class_label[n] = class_id</span><br><span class="line">                        neighborhood.append(n)</span><br><span class="line">                neighborhood = neighborhood[<span class="number">1</span>:]</span><br><span class="line">            class_id += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> class_label</span><br></pre></td></tr></table></figure><h1 id="optics">OPTICS</h1><p>那么DBSCAN本身是一个非常牛逼的算法，它解决了我们找K的问题，这样在海量群组的时候，我们不用像KMeans一样去到处尝试K的大小。但是DBSCAN有个问题，那就是这个算法只能检测一个密度。换句话说，如果现在存在一个数据集有两个类，一个类是方差小的，一个类是方差大的。且这两个群组离得不算太远。如果我们为了照顾方差大的群组将eps设得很大，minpts设得很小，那么可能把两个类聚在一起。反过来，我们就可能找不到方差大的类。</p><p>那么问题来了，有没有办法量化这个距离呢？三年后，同一组作者在DBSCAN的基础上进化出了<a href="http://www.dbs.ifi.lmu.de/Publikationen/Papers/OPTICS.pdf">OPTICS算法</a>。</p><p>既然是DBSCAN的进化版，所以很多概念上都是互通的，只是OPTICS算法多了几个概念，一个是core-distance，一个是reachability-distance。</p><p>我们知道，DBSCAN是不断跑马圈地的一个过程，但是我们很直观想就知道，有些密度大的地方，可能不需要<span class="math inline">\(\varepsilon\)</span>那么大的范围就可以圈到minpts个样本，所以在OPTICS算法里面，我们将满足minpts这么多样本点的<span class="math inline">\(\varepsilon&#39;\)</span>叫做core-distance。而reachability-distance就是中心点与临近点的距离，但是，如果临近点落在<span class="math inline">\(\varepsilon&#39;\)</span>内，reachability-distance就用core-distance来替代。如下图：</p><p align="center"><img data-src='https://github.com/SamaelChen/samaelchen.github.io/raw/hexo/images/blog/optics_001.png' width=50%></p><p>那么用OPTICS的时候我们就需要定义两个列表，一个是seeds，一个是ordered result。seeds就是我们每一轮迭代时候的候选列表，而ordered result就是最终的结果。</p><p>具体的过程是这样的。我们先找到一个点，然后一样跑马圈地，接着计算reachability-distance，然后放到seeds里面从小到大排序。每次取第一个seed出来继续圈地，把被取出来的点以及这个点的reachability distance存在ordered result里面。接着就跟DBSCAN一样，不断重复，直到neighborhood为空。这样做的好处就是，我们可以量化评估每个群的密度大小。效果如下图：</p><p align="center"><img data-src='https://github.com/SamaelChen/samaelchen.github.io/raw/hexo/images/blog/optics_002.jpg' width=50%></p><p>那么我们又会想到，有些seeds里面的点可能随着核心点的移动，reachability distance会不断变小。因为A的core distance里可能是B，而C不在A的core distance里面，但是C在B的core distance里面。如果第一个处理的点是A，第二个处理的点是B，那C其实还是很核心的一个点。那这种时候我们就要跟一它开始的reachability distance做比较，如果新的reachability distance比原来的小，就把原来的值替换掉。</p><p>那么废话不多说，上代码，很多地方跟DBSCAN是可以复用的，我就放了一些OPTICS的核心部分，老规矩，详细见<a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/fun/OPTICS.ipynb">notebook</a>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_eps_neighborhood</span>(<span class="params">data, point_id, eps</span>):</span></span><br><span class="line">    neighborhood = []</span><br><span class="line">    rdist = []</span><br><span class="line">    point = data[point_id]</span><br><span class="line">    <span class="keyword">for</span> q <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(data)):</span><br><span class="line">        dist = _euclidean_dist(point, data[q])</span><br><span class="line">        <span class="keyword">if</span> dist &lt; eps:</span><br><span class="line">            neighborhood.append((q, dist))</span><br><span class="line">    neighborhood = np.array(neighborhood, dtype=[(<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;i&#x27;</span>), (<span class="string">&#x27;dist&#x27;</span>, <span class="string">&#x27;f&#x27;</span>)])</span><br><span class="line">    neighborhood = np.delete(neighborhood, np.where(neighborhood[<span class="string">&#x27;id&#x27;</span>] == point_id))</span><br><span class="line">    <span class="keyword">return</span> neighborhood</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_update_order_seeds</span>(<span class="params">neighborhood, minpts, reach_dists, processed, seeds</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> _is_core_object(neighborhood, minpts):</span><br><span class="line">        <span class="keyword">return</span> seeds</span><br><span class="line">    core_dist = np.sort(neighborhood[<span class="string">&#x27;dist&#x27;</span>])[minpts - <span class="number">2</span>]</span><br><span class="line">    <span class="keyword">for</span> obj <span class="keyword">in</span> neighborhood[<span class="string">&#x27;id&#x27;</span>]:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> processed[obj]:</span><br><span class="line">            obj_dist = neighborhood[neighborhood[<span class="string">&#x27;id&#x27;</span>] == obj][<span class="string">&#x27;dist&#x27;</span>][<span class="number">0</span>]</span><br><span class="line">            newRdist = <span class="built_in">max</span>(core_dist, obj_dist)</span><br><span class="line">            <span class="keyword">if</span> np.isnan(reach_dists[obj]):</span><br><span class="line">                reach_dists[obj] = newRdist</span><br><span class="line">                seeds[obj] = newRdist</span><br><span class="line">            <span class="keyword">elif</span> newRdist &lt; reach_dists[obj]:</span><br><span class="line">                reach_dists[obj] = newRdist</span><br><span class="line">                seeds[obj] = newRdist</span><br><span class="line">    <span class="keyword">return</span> seeds</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optics</span>(<span class="params">data, eps, minpts</span>):</span></span><br><span class="line">    processed = np.array([<span class="literal">False</span>] * <span class="built_in">len</span>(data))</span><br><span class="line">    core_dists = np.full(<span class="built_in">len</span>(data), np.nan)</span><br><span class="line">    reach_dists = np.full(<span class="built_in">len</span>(data), np.nan)</span><br><span class="line">    ordered_res = []</span><br><span class="line">    seeds = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> p_id <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(data)):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> processed[p_id]:</span><br><span class="line">            ordered_res.append(p_id)</span><br><span class="line">            processed[p_id] = <span class="literal">True</span></span><br><span class="line">            neighbors = _eps_neighborhood(data, p_id, eps)</span><br><span class="line">            <span class="keyword">if</span> _is_core_object(neighbors, minpts):</span><br><span class="line">                core_dists[p_id] = np.sort(neighbors[<span class="string">&#x27;dist&#x27;</span>])[minpts - <span class="number">2</span>]</span><br><span class="line">            seeds = _update_order_seeds(neighbors, minpts, reach_dists, processed, seeds)</span><br><span class="line">            <span class="keyword">while</span> <span class="built_in">len</span>(seeds) &gt; <span class="number">0</span>:</span><br><span class="line">                nextId = <span class="built_in">sorted</span>(seeds.items(), key=operator.itemgetter(<span class="number">1</span>))[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">                <span class="keyword">del</span> seeds[nextId]</span><br><span class="line">                processed[nextId] = <span class="literal">True</span></span><br><span class="line">                ordered_res.append(nextId)</span><br><span class="line">                newNeighbors = _eps_neighborhood(data, nextId, eps)</span><br><span class="line">                <span class="keyword">if</span> _is_core_object(newNeighbors, minpts):</span><br><span class="line">                    core_dists[nextId] = np.sort(newNeighbors[<span class="string">&#x27;dist&#x27;</span>])[minpts - <span class="number">2</span>]</span><br><span class="line">                    seeds = _update_order_seeds(newNeighbors, minpts, reach_dists, processed, seeds)</span><br><span class="line">    <span class="comment"># 这里只是我的一个操作，强迫症看不惯有个nan存在。</span></span><br><span class="line">    reach_dists[ordered_res[<span class="number">0</span>]] = core_dists[ordered_res[<span class="number">0</span>]]</span><br><span class="line">    <span class="keyword">return</span> ordered_res, reach_dists, core_dists</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cluster_optics_dbscan</span>(<span class="params">data, eps_reachable, eps, minpts</span>):</span></span><br><span class="line">    orders, reach_dists, core_dists = optics(data, eps, minpts)</span><br><span class="line">    n_samples = <span class="built_in">len</span>(data)</span><br><span class="line">    labels = np.zeros(n_samples, dtype=<span class="built_in">int</span>)</span><br><span class="line">    far_reach = reach_dists &gt; eps_reachable</span><br><span class="line">    near_core = core_dists &lt;= eps_reachable</span><br><span class="line">    labels[orders] = np.cumsum(far_reach[orders] &amp; near_core[orders])</span><br><span class="line">    labels[far_reach &amp; ~near_core] = -<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> labels, orders, reach_dists, core_dists</span><br></pre></td></tr></table></figure><p>OPTICS的优点就是，不管是什么形状的密度，基本上都可以把这个凹槽给跑出来，但是问题就是最后的这个抽取群组小算法。目前我还没找到一个比较好的方法来自动抽取，如果是按照论文里面的分层抽取，我试过会抽的太细，如果是按照论文里面的DBSCAN来抽，就是我实现的这个，不过是一刀切的方式，太复杂的样本效果就不好了。目前还在探索用其他平滑方法来替代，有突破再来更新。</p><p>试了两种平滑方式，最后的做法是，先用一维高斯平滑，然后用max filter抹掉因为高斯平滑最大值向左漂的问题。</p><h1 id="hdbscan">HDBSCAN</h1><p>然后又发现了一个新的聚类算法，类似OPTICS算法，叫<a href="https://link.springer.com/chapter/10.1007/978-3-642-37456-2_14">HDBSCAN</a>，顾名思义，就是H就是hierarchical。事实上就是OPTICS算法的一个改进版本。</p><p>与OPTICS类似，HDBSCAN一样要算core distance和reachability distance。这个算法跟OPTICS是一样的。但是reachability distance的公式是：<span class="math inline">\(d_{\text{reach}-k}(a, b) = \max\{\text{core}_k(a), \text{core}_k(b), d(a, b)\}\)</span>，这里的<span class="math inline">\(d(a, b)\)</span>是两个点用距离公式算的距离。如下图所示：</p><p align="center"><img data-src='https://nbviewer.jupyter.org/github/scikit-learn-contrib/hdbscan/blob/master/notebooks/distance4.svg' width=50%></p><p>蓝绿两点直接的reachability distance就是绿点的core distance，红绿两点的就是红绿两点的距离。</p><p>那么这样一来，就可以得到每个点之间的reachability distance。然后就可以用这个距离作为权重来绘制一个带权重的连接图。<a href="https://arxiv.org/pdf/1506.06422v2.pdf">Beyond Hartigan Consistency</a>这篇论文认为，无论是那种概率密度分布，这种相互距离可以更好的表示单链接聚类的层次结构。</p><p>然后可以用<a href="https://en.wikipedia.org/wiki/Prim%27s_algorithm">Prim算法</a>构建最小生成树，然后转化为层次结构，就类似层次聚类了。这个可以参考一下基于最小生成树的层次聚类。然后通过设定最小簇大小，对这个树进行剪枝。也就从下面的第一个图变成第二个图：</p><p align="center"><img data-src='https://hdbscan.readthedocs.io/en/latest/_images/how_hdbscan_works_12_1.png' width=50%></p><p align="center"><img data-src='https://hdbscan.readthedocs.io/en/latest/_images/how_hdbscan_works_15_1.png' width=50%></p><p>之后就是抽取簇。上面剪枝后的树其实已经有了结果，但是我们希望抽的簇可以自动抽取，且是稳定的。因此我们需要一种度量方式来衡量簇的稳定性。我们定义一个值<span class="math inline">\(f(x) = \lambda = \frac{1}{\text{distance}_{\text{core}}}\)</span>。这里的distance就是一个点。所以用<span class="math inline">\(\lambda_{\text{birth}}\)</span>和<span class="math inline">\(\lambda_{\text{death}}\)</span>分别表示簇生成和簇分裂时候的<span class="math inline">\(\lambda\)</span>大小，也就是<span class="math inline">\(\lambda_{\text{min}}C_i\)</span>和<span class="math inline">\(\lambda_{\text{max}}C_i\)</span>，那么簇的稳定性就是<span class="math inline">\(S = \sum_{p \in \text{cluster}} (\lambda_p - \lambda_{\text{birth}})\)</span>，<span class="math inline">\(\lambda_p = \lambda_{\text{max}}(x, C) = \min(f(x), \lambda_{\text{max}}C)\)</span>每轮优化的方向就是让这个稳定性最大，同时满足最小簇大小。</p><p>那么HDBSCAN跟DBSCAN比的话，DBSCAN实际上就是上面剪枝前的hierarchical tree切一刀，而HDBSCAN会自适应地去寻找合适的划分。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;我如期来更新啦！！！聚类算法是很常用的一种算法，不过最常见的就是KMeans了，虽然很多人都会用，不过讲道理，如果是调用现成机器学习库里面的KMeans的话，我敢保证90%的人答不上来具体的是什么算法。相信我，库里的KMeans跟教科书讲的那个随机取初始点的KMeans不是一个算法哟～&lt;/p&gt;
&lt;p&gt;因为KMeans依赖K，但是我怎么知道K要用多少呢？另外，KMeans受限于算法本身，对于球状的数据效果较好，但是不规则形状的就不行了。这种情况下，相对而言，基于密度的聚类算法就比较好用了。sklearn里面现在是放了一个DBSCAN，下一版会更新OPTICS。刚好最近都用了，这里把DBSCAN跟OPTICS算法复现一遍。&lt;/p&gt;</summary>
    
    
    
    <category term="统计学习" scheme="https://samaelchen.github.io/categories/StatisticalLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>未来一段时间更新计划</title>
    <link href="https://samaelchen.github.io/schedule/"/>
    <id>https://samaelchen.github.io/schedule/</id>
    <published>2019-04-22T16:00:00.000Z</published>
    <updated>2021-08-31T16:15:25.581Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>年前换了个坑，结果一堆事情，那段时间烦心事也多，啥都没更新。大概拍了一下未来一段时间的更新计划。主要是5月以后的。</p><span id="more"></span><p>五一节左右会更新一篇DBSCAN跟OPTICS算法的原理和代码，主要是基于密度的聚类算法。然后如果顺利的话再更新一篇经济思想史笔记。</p><p>六月以后看到李宏毅老师的深度学习课程加了好多新的内容，主要是GAN、DRL这一些，之前都是草草看一遍，打算这之后一周一篇到两篇课程笔记。争取一个季度内全部补全吧。另外立个flag吧，如果一个季度内补全了，顺便复习了一遍机器学习的内容，把之前可能理解错误的内容勘定了，给自己买块2080ti。括弧笑～</p><p>再有是被同事刺激了，感觉要每天提高专业技能呀。后面看看是不是开个专门记录刷吉米多维奇和概率论的栏目。之前刷leetcode的都半途而废了，有点可惜啊，看看是不是也重启一下。要不一三五数学，二四六leetcode，每天背50个单词这样？！讲道理英语词汇量感觉只剩下机器学习相关的了。</p><p>然后是读书方面，简单分了三类：</p><ul><li>经管类，目前主要在看经济思想史，不过是漫漫长路，一点点看。然后打算看一些量化的东西吧，做点策略什么的，不然钱都是死的，血亏。这部分的书单还在打理中，五一着重安排一下，毕竟是一个有专业门槛的领域。</li><li>技术/数学类，主要想研究一下微软研究院出的《分布式机器学习》。换大厂了嘛，还是小机器单机就配不上大厂了。然后其他的机器学习、深度学习教程就当做是看李宏毅老师课程时候的参考资料看好了，可能参考一下邱锡鹏老师的新书，经典的花书这一些。其他技术类讲算法的书随便翻翻，看到之前没实现的算法可能有时间就复现一下，作为彩蛋更新。数学类的书籍就是打算跟同事一样开始刷题了，不然牛逼的人太多，我都不知道以后该怎么混。</li><li>杂书类，杂七杂八的书就随意了，看眼缘了。这一个多月看了《思考快与慢》、《黑天鹅》还有《随机游走的傻瓜》，怎么说呢，基本上都是说认知偏差这一大类的，现在看的《世界观》也差不多，大概扫一扫吧。然后可能想看一些行为心理学方面的书吧，改改拖延症，培养节奏感啥的。最后就是随缘翻一翻《易经》。</li></ul><p>另外就是现在每天记录睡眠时间想逼自己早睡，虽然没啥效果吧，不过每次看看也是挺触目惊心的。尽量把时间排满，不逼自己996么，感觉太懒散了，时间上满一点，有节奏一点，看看会不会以后不熬夜了。</p><p>大概就这么个计划吧，现在觉得吧，还是得逼自己一把。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;年前换了个坑，结果一堆事情，那段时间烦心事也多，啥都没更新。大概拍了一下未来一段时间的更新计划。主要是5月以后的。&lt;/p&gt;</summary>
    
    
    
    <category term="碎碎念" scheme="https://samaelchen.github.io/categories/self-questioning/"/>
    
    
  </entry>
  
  <entry>
    <title>Progressive Growing of GANs</title>
    <link href="https://samaelchen.github.io/pytorch-pggan/"/>
    <id>https://samaelchen.github.io/pytorch-pggan/</id>
    <published>2019-01-23T16:00:00.000Z</published>
    <updated>2021-08-31T16:15:25.581Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>虽然我没看完李嘉图，但是也没闲着呀，我还是在写pggan的呀。代号屁股gan计划。我是不会承认我想拿pggan去生成大长腿的。</p><span id="more"></span><p>这个GAN是NVIDIA在17年发表的<a href="https://arxiv.org/pdf/1710.10196.pdf">论文</a>，文章写的比较糙。一开始官方放出了Theano的版本，后来更新了基于TensorFlow的版本。都不是我喜欢的框架。然后就看到北大的一位很快做了一个PyTorch的版本。不过写的太复杂了，后面找到的其他版本基本上也写得跟官方的差不多复杂得一塌糊涂。最后找到一个我能看懂，并且很直观的实现方案：<a href="https://github.com/rosinality/progressive-gan-pytorch">https://github.com/rosinality/progressive-gan-pytorch</a>。然后我就在这个基础上进行修改，做成我比较舒服的脚本。</p><p>接下来把几个核心部分做个笔记。</p><h1 id="两个trick">两个trick</h1><h2 id="equalized-learning-rate">Equalized learning rate</h2><p>作者这里用了第一个trick，就是让每个weight的更新速度是一样的。用的公式是<span class="math inline">\(\hat{w_i} = w_i/c\)</span>。其中<span class="math inline">\(w_i\)</span>就是权重，而<span class="math inline">\(c\)</span>是每一层用何恺明标准化的一个常数。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EqualLR</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name</span>):</span></span><br><span class="line">        self.name = name</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_weight</span>(<span class="params">self, module</span>):</span></span><br><span class="line">        weight = <span class="built_in">getattr</span>(module, self.name + <span class="string">&#x27;_orig&#x27;</span>)</span><br><span class="line">        fan_in = weight.data.size(<span class="number">1</span>) * weight.data[<span class="number">0</span>][<span class="number">0</span>].numel()</span><br><span class="line">        <span class="keyword">return</span> weight * np.sqrt(<span class="number">2</span> / fan_in)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(<span class="params">module, name</span>):</span></span><br><span class="line">        fn = EqualLR(name)</span><br><span class="line">        weight = <span class="built_in">getattr</span>(module, name)</span><br><span class="line">        <span class="keyword">del</span> module._parameters[name]</span><br><span class="line">        module.register_parameter(name + <span class="string">&#x27;_orig&#x27;</span>, nn.Parameter(weight.data))</span><br><span class="line">        module.register_forward_pre_hook(fn)</span><br><span class="line">        <span class="keyword">return</span> fn</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, module, <span class="built_in">input</span></span>):</span></span><br><span class="line">        weight = self.compute_weight(module)</span><br><span class="line">        <span class="built_in">setattr</span>(module, self.name, weight)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">equal_lr</span>(<span class="params">module, name=<span class="string">&#x27;weight&#x27;</span></span>):</span></span><br><span class="line">    EqualLR.apply(module, name)</span><br><span class="line">    <span class="keyword">return</span> module</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EqualConv2d</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, *args, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        conv = nn.Conv2d(*args, **kwargs)</span><br><span class="line">        conv.weight.data.normal_()</span><br><span class="line">        conv.bias.data.zero_()</span><br><span class="line">        self.conv = equal_lr(conv)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.conv(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure><p>这个很明显是原来作者写的啦，我大蟒蛇还没这么工程化的水平。</p><h2 id="pixelwise-normalization">Pixelwise normalization</h2><p>这个是在生成器中进行normalization。公式也很简单，就是<span class="math inline">\(b_{x,y} = a_{x,y} / \sqrt{\frac{1}{N} \sum_{j=0}{N-1}(a_{x,y}^j)^2 + \epsilon}\)</span>。其中<span class="math inline">\(\epsilon\)</span>是一个常数<span class="math inline">\(10^{-8}\)</span>，<span class="math inline">\(N\)</span>是有多少feature map，<span class="math inline">\(a_{x,y}和b_{x,y}\)</span>是原始feature vector和normalize后的feature vector。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PixelNorm</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">input</span> / torch.sqrt(torch.mean(<span class="built_in">input</span> ** <span class="number">2</span>, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>) + <span class="number">1e-8</span>)</span><br></pre></td></tr></table></figure><p>这是两个文章重点提出来的trick。其他其实还有很多trick，不过是偏向设计网络结构的。</p><p>#PG-GAN主体</p><p>接下来就是最核心的部分，生成器和分类器。生成器和分类器的学习方法就是一步步放大图像的尺寸，从<span class="math inline">\(4\times 4\)</span>最后放大到<span class="math inline">\(1024 \times 1024\)</span>。生成器和分类器也是放大一次增加一个block。而这个block的设计也是参考了resnet，因为突然放大会导致模型不稳定，用这种方法可以平滑过渡。</p><p>然后就是PG-GAN和dcgan不一样的地方，dcgan放大的方式是用conv_transpose而PG-GAN用的是上采样的方法。<a href="https://distill.pub/2016/deconv-checkerboard/">Deconvolution and Checkerboard Artifacts</a>这篇文章讲了为什么用上采样更好，不过我没来得及细看。</p><p>所以我们先定义好一个block：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConvBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channel, out_channel, kernel1, pad1, kernel2, pad2, pixel_norm=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.kernel1 = kernel1</span><br><span class="line">        self.kernel2 = kernel2</span><br><span class="line">        self.stride1 = <span class="number">1</span></span><br><span class="line">        self.stride2 = <span class="number">1</span></span><br><span class="line">        self.pad1 = pad1</span><br><span class="line">        self.pad2 = pad2</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> pixel_norm:</span><br><span class="line">            self.conv = nn.Sequential(EqualConv2d(in_channel, out_channel, self.kernel1, self.stride1, self.pad1),</span><br><span class="line">                                      PixelNorm(),</span><br><span class="line">                                      nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">                                      EqualConv2d(out_channel, out_channel, self.kernel2, self.stride2, self.pad2),</span><br><span class="line">                                      PixelNorm(),</span><br><span class="line">                                      nn.LeakyReLU(<span class="number">0.2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv = nn.Sequential(EqualConv2d(in_channel, out_channel, self.kernel1, self.stride1, self.pad1),</span><br><span class="line">                                      nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">                                      EqualConv2d(out_channel, out_channel, self.kernel2, self.stride2, self.pad2),</span><br><span class="line">                                      nn.LeakyReLU(<span class="number">0.2</span>))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">        out = self.conv(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><h2 id="generator">generator</h2><p>直接上代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, code_dim=<span class="number">512</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.code_norm = PixelNorm()</span><br><span class="line">        self.progression = nn.ModuleList([ConvBlock(<span class="number">512</span>, <span class="number">512</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>),</span><br><span class="line">                                          ConvBlock(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>),</span><br><span class="line">                                          ConvBlock(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>),</span><br><span class="line">                                          ConvBlock(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>),</span><br><span class="line">                                          ConvBlock(<span class="number">512</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>),</span><br><span class="line">                                          ConvBlock(<span class="number">256</span>, <span class="number">128</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>)])</span><br><span class="line">        self.to_rgb = nn.ModuleList([nn.Conv2d(<span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>),</span><br><span class="line">                                     nn.Conv2d(<span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>),</span><br><span class="line">                                     nn.Conv2d(<span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>),</span><br><span class="line">                                     nn.Conv2d(<span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>),</span><br><span class="line">                                     nn.Conv2d(<span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>),</span><br><span class="line">                                     nn.Conv2d(<span class="number">128</span>, <span class="number">3</span>, <span class="number">1</span>),])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>, expand=<span class="number">0</span>, alpha=-<span class="number">1</span></span>):</span></span><br><span class="line">        out = self.code_norm(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">for</span> i, (conv, to_rgb) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(self.progression, self.to_rgb)):</span><br><span class="line">            <span class="keyword">if</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> expand &gt; <span class="number">0</span>:</span><br><span class="line">                upsample = F.interpolate(out, scale_factor=<span class="number">2</span>)</span><br><span class="line">                out = conv(upsample)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                out = conv(out)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i == expand:</span><br><span class="line">                out = to_rgb(out)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> <span class="number">0</span> &lt;= alpha &lt; <span class="number">1</span>:</span><br><span class="line">                    skip_rgb = self.to_rgb[i - <span class="number">1</span>](upsample)</span><br><span class="line">                    out = (<span class="number">1</span> - alpha) * skip_rgb + alpha * out</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><p>这个generator只定义到了<span class="math inline">\(128\times 128\)</span>这个分辨率的，要是想要增大分辨率可以参考文章最后的附录table 2的数据自己一个个加上去就好了，discriminator一样的操作就行。然后就是代码里面的这个skip_rgb，这个操作就是上面讲的平滑操作。</p><h2 id="discriminator">discriminator</h2><p>跟generator差不多。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Distriminator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.progression = nn.ModuleList([ConvBlock(<span class="number">128</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, pixel_norm=<span class="literal">False</span>),</span><br><span class="line">                                          ConvBlock(<span class="number">256</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, pixel_norm=<span class="literal">False</span>),</span><br><span class="line">                                          ConvBlock(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, pixel_norm=<span class="literal">False</span>),</span><br><span class="line">                                          ConvBlock(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, pixel_norm=<span class="literal">False</span>),</span><br><span class="line">                                          ConvBlock(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, pixel_norm=<span class="literal">False</span>),</span><br><span class="line">                                          ConvBlock(<span class="number">513</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">0</span>, pixel_norm=<span class="literal">False</span>),])</span><br><span class="line">        self.from_rgb = nn.ModuleList([nn.Conv2d(<span class="number">3</span>, <span class="number">128</span>, <span class="number">1</span>),</span><br><span class="line">                                       nn.Conv2d(<span class="number">3</span>, <span class="number">256</span>, <span class="number">1</span>),</span><br><span class="line">                                       nn.Conv2d(<span class="number">3</span>, <span class="number">512</span>, <span class="number">1</span>),</span><br><span class="line">                                       nn.Conv2d(<span class="number">3</span>, <span class="number">512</span>, <span class="number">1</span>),</span><br><span class="line">                                       nn.Conv2d(<span class="number">3</span>, <span class="number">512</span>, <span class="number">1</span>),</span><br><span class="line">                                       nn.Conv2d(<span class="number">3</span>, <span class="number">512</span>, <span class="number">1</span>),])</span><br><span class="line">        self.n_layer = <span class="built_in">len</span>(self.progression)</span><br><span class="line">        self.linear = nn.Linear(<span class="number">512</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>, expand=<span class="number">0</span>, alpha=-<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(expand, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">            index = self.n_layer - i - <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> i == expand:</span><br><span class="line">                out = self.from_rgb[index](<span class="built_in">input</span>)</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">                mean_std = <span class="built_in">input</span>.std(<span class="number">0</span>).mean()</span><br><span class="line">                mean_std = mean_std.expand(<span class="built_in">input</span>.size(<span class="number">0</span>), <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">                out = torch.cat([out, mean_std], <span class="number">1</span>)</span><br><span class="line">            out = self.progression[index](out)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i &gt; <span class="number">0</span>:</span><br><span class="line">                out = F.avg_pool2d(out, <span class="number">2</span>)</span><br><span class="line">                <span class="keyword">if</span> i == expand <span class="keyword">and</span> <span class="number">0</span> &lt;= alpha &lt; <span class="number">1</span>:</span><br><span class="line">                    skip_rgb = F.avg_pool2d(<span class="built_in">input</span>, <span class="number">2</span>)</span><br><span class="line">                    skip_rgb = self.from_rgb[index + <span class="number">1</span>](skip_rgb)</span><br><span class="line">                    out = (<span class="number">1</span> - alpha) * skip_rgb + alpha * out</span><br><span class="line"></span><br><span class="line">        out = out.squeeze(<span class="number">2</span>).squeeze(<span class="number">2</span>)</span><br><span class="line">        out = self.linear(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure><p>然后mean_std这个地方就是文章里面的另一个trick，叫minibatch stddev，主要是用来增加差异性的，文章的第三部分。</p><p>最后只要按照wgan的方法训练就好了。不过还要注意一点的就是，wgan是discriminator训练5次，训练一次generator，而pggan是训一次discriminator，一次generator这样交替来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">experiment_path = <span class="string">&#x27;checkpoint/pggan&#x27;</span></span><br><span class="line">img_list = []</span><br><span class="line">G_losses = []</span><br><span class="line">D_losses = []</span><br><span class="line">D_losses_tmp = []</span><br><span class="line">Grad_penalty = []</span><br><span class="line">i = <span class="number">0</span></span><br><span class="line">iters = <span class="number">0</span></span><br><span class="line">total_iters = <span class="number">0</span></span><br><span class="line">expand = <span class="number">0</span></span><br><span class="line">n_critic = <span class="number">1</span></span><br><span class="line">step = <span class="number">0</span></span><br><span class="line">alpha = <span class="number">0</span></span><br><span class="line">CLAMP = <span class="number">0.01</span></span><br><span class="line">one = torch.FloatTensor([<span class="number">1</span>]).cuda()</span><br><span class="line">mone = one * -<span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Training start!&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">if</span> epoch != <span class="number">0</span> <span class="keyword">and</span> epoch % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">        alpha = <span class="number">0</span></span><br><span class="line">        iters = <span class="number">0</span></span><br><span class="line">        expand += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> expand &gt;= <span class="number">3</span>:</span><br><span class="line">            batch_size = <span class="number">16</span></span><br><span class="line">        <span class="keyword">if</span> expand &gt; <span class="number">5</span>:</span><br><span class="line">            alpha = <span class="number">1</span></span><br><span class="line">            expand = <span class="number">5</span></span><br><span class="line">        dataset = modify_data(dataroot, image_size * <span class="number">2</span> ** expand)</span><br><span class="line">        dataloader = udata.DataLoader(dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=workers)</span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">        real_cpu = data[<span class="number">0</span>].to(device)</span><br><span class="line">        b_size = real_cpu.size(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> step &lt; n_critic:</span><br><span class="line">            netD.zero_grad()</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> netD.parameters():</span><br><span class="line">                p.requires_grad = <span class="literal">True</span></span><br><span class="line"><span class="comment">#                 p.data.clamp_(-CLAMP, CLAMP)</span></span><br><span class="line">            output = netD(real_cpu, expand, alpha).view(-<span class="number">1</span>)</span><br><span class="line">            errD_real = (output.mean() - <span class="number">0.001</span> * (output ** <span class="number">2</span>).mean()).view(<span class="number">1</span>)</span><br><span class="line">            errD_real.backward(mone)</span><br><span class="line">            noise = torch.randn(b_size, nz, <span class="number">1</span>, <span class="number">1</span>, device=device)</span><br><span class="line">            fake = netG(noise, expand, alpha)</span><br><span class="line">            output = netD(fake.detach(), expand, alpha).view(-<span class="number">1</span>)</span><br><span class="line">            errD_fake = output.mean().view(<span class="number">1</span>)</span><br><span class="line">            errD_fake.backward(one)</span><br><span class="line">            eps = torch.rand(b_size, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, device=device)</span><br><span class="line">            x_hat = eps * real_cpu.data + (<span class="number">1</span> - eps) * fake.data</span><br><span class="line">            x_hat.requires_grad = <span class="literal">True</span></span><br><span class="line">            hat_predict = netD(x_hat, expand, alpha)</span><br><span class="line">            grad_x_hat = autograd.grad(outputs=hat_predict.<span class="built_in">sum</span>(), inputs=x_hat, create_graph=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line">            grad_penalty = ((grad_x_hat.view(grad_x_hat.size(<span class="number">0</span>), -<span class="number">1</span>).norm(<span class="number">2</span>, dim=<span class="number">1</span>) - <span class="number">1</span>) ** <span class="number">2</span>).mean()</span><br><span class="line">            grad_penalty = <span class="number">10</span> * grad_penalty</span><br><span class="line">            grad_penalty.backward()</span><br><span class="line">            errD = errD_real - errD_fake</span><br><span class="line">            d_optimizer.step()</span><br><span class="line">            D_losses_tmp.append(errD.item())</span><br><span class="line">            step += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> netD.parameters():</span><br><span class="line">                p.requires_grad = <span class="literal">False</span></span><br><span class="line">            netG.zero_grad()</span><br><span class="line">            noise = torch.randn(b_size, nz, <span class="number">1</span>, <span class="number">1</span>, device=device)</span><br><span class="line">            fake = netG(noise, expand, alpha)</span><br><span class="line">            output = netD(fake, expand, alpha).view(-<span class="number">1</span>)</span><br><span class="line">            errG = -output.mean().view(<span class="number">1</span>)</span><br><span class="line">            errG.backward()</span><br><span class="line">            g_optimizer.step()</span><br><span class="line">            D_losses.append(np.mean(D_losses_tmp))</span><br><span class="line">            G_losses.append(errG.item())</span><br><span class="line">            D_losses_tmp = []</span><br><span class="line">            step = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> (total_iters+<span class="number">1</span>) % <span class="number">200</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[%d/%d][%d/%d](%d)\tLoss_D: %.4f\tLoss_G: %.4f\tD(x): %.4f\tD(G(z)): %.4f\tGrad: %.4f&#x27;</span></span><br><span class="line">                  % (epoch+<span class="number">1</span>, num_epochs, i+<span class="number">1</span>, <span class="built_in">len</span>(dataloader), total_iters + <span class="number">1</span>,</span><br><span class="line">                     errD.item(), errG.item(), errD_real.data.mean(), errD_fake.data.mean(), grad_penalty.data))</span><br><span class="line">        <span class="comment"># Check how the generator is doing by saving G&#x27;s output on fixed_noise</span></span><br><span class="line">        <span class="keyword">if</span> (total_iters % <span class="number">5000</span> == <span class="number">0</span>) <span class="keyword">or</span> ((epoch == num_epochs-<span class="number">1</span>) <span class="keyword">and</span> (i == <span class="built_in">len</span>(dataloader)-<span class="number">1</span>)):</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                fake = netG(fixed_noise, expand, alpha).detach().cpu()</span><br><span class="line">            img = vutils.make_grid(fake, padding=<span class="number">2</span>, normalize=<span class="literal">True</span>)</span><br><span class="line">            vutils.save_image(img, <span class="string">&#x27;checkpoint/pggan/fake_image/fake_iter_&#123;0&#125;.jpg&#x27;</span>.<span class="built_in">format</span>(total_iters))</span><br><span class="line">            img_list.append(img)</span><br><span class="line"></span><br><span class="line">        iters += <span class="number">1</span></span><br><span class="line">        total_iters += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            torch.save(netG.state_dict(), <span class="string">&#x27;&#123;0&#125;/netG_epoch_&#123;1&#125;.pth&#x27;</span>.<span class="built_in">format</span>(experiment_path, epoch+<span class="number">1</span>))</span><br><span class="line">            torch.save(netD.state_dict(), <span class="string">&#x27;&#123;0&#125;/netD_epoch_&#123;1&#125;.pth&#x27;</span>.<span class="built_in">format</span>(experiment_path, epoch+<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p>然后这里面最要注意的是，wgan-gp里面用到了一个很重要的方法，就是gradient penalty，也就是训练里面的这一部分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">eps = torch.rand(b_size, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, device=device)</span><br><span class="line">x_hat = eps * real_cpu.data + (<span class="number">1</span> - eps) * fake.data</span><br><span class="line">x_hat.requires_grad = <span class="literal">True</span></span><br><span class="line">hat_predict = netD(x_hat, expand, alpha)</span><br><span class="line">grad_x_hat = autograd.grad(outputs=hat_predict.<span class="built_in">sum</span>(), inputs=x_hat, create_graph=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line">grad_penalty = ((grad_x_hat.view(grad_x_hat.size(<span class="number">0</span>), -<span class="number">1</span>).norm(<span class="number">2</span>, dim=<span class="number">1</span>) - <span class="number">1</span>) ** <span class="number">2</span>).mean()</span><br><span class="line">grad_penalty = <span class="number">10</span> * grad_penalty</span><br><span class="line">grad_penalty.backward()</span><br></pre></td></tr></table></figure><p>别的也就没什么了，坐等结果就好了。具体在我的<a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/pytorch/pggan/pggan-101.ipynb">notebook</a>里面。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;虽然我没看完李嘉图，但是也没闲着呀，我还是在写pggan的呀。代号屁股gan计划。我是不会承认我想拿pggan去生成大长腿的。&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://samaelchen.github.io/categories/DeepLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>经济思想史读书笔记——李嘉图&amp;马尔萨斯</title>
    <link href="https://samaelchen.github.io/economics_thoughts_notes5/"/>
    <id>https://samaelchen.github.io/economics_thoughts_notes5/</id>
    <published>2018-12-17T16:00:00.000Z</published>
    <updated>2021-08-31T16:15:25.578Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>古典经济学，李嘉图与马尔萨斯部分。主要是李嘉图，简直是个宝藏男孩。一个人引导了整个经济界，构建了引导经济思想一百年的理论框架。</p><span id="more"></span><h1 id="一马尔萨斯人口学说">一、马尔萨斯人口学说</h1><p>马尔萨斯的人口学说成因：</p><ol type="1"><li>人口对英国食物造成压力</li><li>工业化带来的收入分化</li><li>为了证明社会制度以及政府制度不是贫困与不幸的成因</li></ol><p>人口学说的基本假设：</p><ol type="1"><li>食物对人类生存是必须的</li><li>性爱需求是必须且不会变化的</li></ol><p>马尔萨斯的人口学说假设实质上否定了技术发展对解决人口问题的可能性，比如说安全套、避孕药这种避孕手段，或者是我国杂交水稻等生命科学手段。因此马尔萨斯认为，若不控制人口，人口增长率将会超过食物供给增长率。</p><p>马尔萨斯在他第一版理论中提出积极的和预防的两种人口控制方法。积极的人口控制方法就是通过战争、饥荒、疾病和类似的灾难来提高死亡率。预防的方法就是通过降低出生率，主要通过延缓婚姻实现。但是马尔萨斯认为，延缓婚姻只会导致恶行、不幸以及人性退化。因为它会产生婚前性行为。因此人类对性和食物的渴求无法通过制度变革。这使得经济学被称为“沉闷的科学”。</p><p>马尔萨斯的第二版理论使用了科学的统计数据来支持他的论点。在第二版中，马尔萨斯引入新的人口控制方法，也就是道德控制，或者说是没有婚前性行为的延缓婚姻。</p><p>马尔萨斯的人口论题并没有讨论通过避孕控制人口的可能性。此外，马尔萨斯对性冲动的本能欲望感到困惑。事实上随着富裕程度和受教育水平的提升，在性欲和生孩子之间人们会自己做出取舍。换而言之，虽然人类的性欲依旧是旺盛的，但是人类可能并不想抚养一个孩子。所以，在这种情况下，人们会采用各种各样的避孕手段来享受性爱，同时避免生育。其实从某种程度上来说，做爱这事情挺麻烦的，至少要两个人，还要两个人互相配合，随着经济水平的发展，性爱频率实际上会下降。最近的一些文章也支持了这种观点，有兴趣的可以自行搜索。</p><p>另一个马尔萨斯面临的难题是马尔萨斯没有考虑农业科技的进步可以能使食物供给提高。不过经济学家从来不发展解释技术进步比率的理论。因此，历史上经济学家容易低估科学进步对经济体的影响。</p><p>结合马尔萨斯的人口学说，以及斯密的工资基金学说，得到经济学中一个非常重要的概念，工资铁律（the iron law of wages）。也就是说，提高低收入人群的经济福利将带来人口规模的增长，那么工资率就又恢复到以前的水平。因此，工资将会一直保持在维持最低生活成本的水平上。</p><h1 id="二李嘉图">二、李嘉图</h1><p>李嘉图代表了忙碌的纯理论家，他从他所处的时代的经济体中进行抽象，构建了基于演绎方法的一种分析。他的主张理论是具体分析现实世界的政策问题的先决条件。李嘉图的方法更像是斯密方法的一脉相承，斯密的方法是：（1）运用演绎理论来分析他所处时代的经济体。（2）呈现同时代人描述性的、非正式的叙述以及呈现历史上的制度。</p><p>李嘉图所处的时代面临着升高的谷物价格，升高的地租以及应该经济结构变革导致的工业的相对增长与农业的相对下降。这些问题最后都集中在一个关键点，也就是实行自由国际贸易还是管制的国际贸易。</p><p>从农场主角度考虑，规制的国际贸易可以免受外来农产品的竞争，而新兴的工业家则可以进口廉价的原材料。</p><p>李嘉图主义的两个成分一直保留到了今天：高度抽象和基于抽象模型的非关联的政策制定。</p><p>李嘉图是经济学基本任务这一观念上的转折点。斯密延续了重商主义对国民财富决定力量的关注，而李嘉图则主张经济学的主要目的是确定支配地主、资本家以及劳动者之间收入分配的法则。</p><p>李嘉图专注于现在被称为收入的功能性分配的研究，收入的功能性分配是指年产量流向劳动、土地以及资本的相对份额。他在由三个阶级组成的社会背景下考虑这一问题：获得利润与利息的资本金、获得地租的地主以及获得工资的劳动者。为了解释利润、利息、地租、工资，李嘉图不得不和斯密一样从经济体的微观层面进行理论阐述。因此，他将后来的经济研究引导到微观经济问题，而不是宏观经济问题，这恰好与他的期望相反。此外，在宏观经济稳定问题上，他对马尔萨斯的胜利，在将近一个世纪中结束了正统理论家对这一问题的进一步争论。</p><h2 id="李嘉图的模型">李嘉图的模型</h2><p>李嘉图的模型有三个主要群体：资本家、劳动者、地主。在经济体中，资本家是主要角色，他们是生产者、指挥者以及最重要的参与者。他们有两个主要功能：一、有助于资源的有效分配；二、通过储蓄和投资来拉动经济增长。</p><p>尽管李嘉图采用劳动成本理论来解释不同时期相对价格的变化，但是在他的模型里面，劳动本质上是被动的。他用工资基金学说和马尔萨斯的人口理论来解释劳动者的实际工资（real wage）：实际工资=工资基金/劳动力。这就是前面提到的工资铁律。</p><p>当然，不同国家地区的最低生活水平是不一样的，因此，维持一个人最基本的生活成本的工资也是不一样的。比如说，同样是“贫困”人口，美国的贫困工人与尼泊尔的贫困工人也是不一样的。</p><p>在李嘉图的体系中，地主就是蛀虫。在李嘉图看来，土地的供给曲线是完全无法动弹的，土地的社会机会成本为0。地主获得地租收入，仅仅是因为拥有一种生产要素，他并没有提供任何对社会有益的作用。古典经济学家认为，地主阶级的行为（主要指地主阶级的财富都用于消费，而不是储蓄和投资）有害于新型工业社会的增长与发展。</p><p>李嘉图的模型表达了国民财富增长与三个主要经济群体之间的关系：总产量或者说经济体的总收入分配给了劳动者、资本家和地主。总产量中的一部分，没有用来支付劳动者维持最低生活水平的工资，也没有用来替换生产过程中报废的资本产品，这一部分可以被称为净收益或者经济剩余。也就是“总收益-（维持最低生活水平的工资+折旧）=净收益”。因此，净收益由利润、地租以及维持最低生活水平的工资之上的部分组成。因为长期均衡中工资铁律的存在，净收益就等于利润加上地租。工人与地主都将他们的全部收入花在消费上，因此，利润成为了储蓄或资本积累的唯一来源。利用他的地租理论，李嘉图断定，作为经济增长率减小的结果，当利润下降、地租上升的时候，随着时间的变化，将会发生有利于地主的收入再分配。</p><p>李嘉图要解决经济问题，面临着一个时代难题，《谷物法》。《谷物法》是对进口到英国的谷物征收关税的规定。对于《谷物法》当时有很多主张。一种主张认为高关税会鼓励农业更多的投资，导致产量的上升，价格的下降。另一种主张认为，谷物的高价格是高地租的结果。但是李嘉图认为，《谷物法》的根本问题在于收入的分配。较高的关税将使得收入分配有利于地主。</p><p>当李嘉图为了处理由《谷物法》引起的许多政策问题的时候，他利用了很多分析工具与假设。这些工具包括：（1）劳动成本理论（labor cost theory）。不同时期相对价格的变化，可以用时间度量的劳动成本的变化来解释。（2）中性货币（neutral money）。货币供给的变化，可以引起绝对价格水平相对价格水平两者的变化。李嘉图的假设是，货币供给的变化不引起相对价格的变化。（3）劳动与资本的固定生产系数（fixed coefficients of production for labor and capital）。只能使用劳动与资本投入的一种联合来生产既定的产量。换句话说，对每种类型的经济生产而言，技术上的考虑使得劳动-资本比例是固定的，且不随着产量的变化而变化。（4）制造业收益不变，农业收益递减（constant returns in manufacturing and diminishing returns in agriculture）。制造业的供给曲线是水平的，或者是富有弹性的（随着产量的增加，边际成本不变）；农业供给曲线是向上倾斜的，也就是随着产量扩张，边际成本增加。（5）充分就业（full employment）。长期中，经济体在资源充分利用的水平上趋向自动运转。（6）完全竞争（perfect competition）。市场包含很多独立的生产者，他们的产品具有同质性，任何一个单独的销售者都不能影响市场的价格。（7）经济参与者（economic actors）。个人在他们的经济活动中都是理性的和精于算计的。在完全竞争市场中，这种社会作用会导致利润率统一、工资统一以及地租统一。（8）马尔萨斯人口论题（Malthusian population thesis）。人口趋向以快于食物供给的速度增加。（9）工资基金学说（wages fund doctrine）。工资率等于工资基金除以劳动力规模。</p><h2 id="李嘉图的地租理论">李嘉图的地租理论</h2><p>李嘉图、马尔萨斯、韦斯特以及托伦斯阐述了收益递减原理，这一原理成为了一个重要的经济学概念。</p><p>收益递减原理表明，当其他生产要素保持不变，一种生产要素稳定增加时候，总产量的增长率最终会变小。</p><p>李嘉图对地租做出了定义，他将地主视为给地主的一种支付，等于不同肥力土地上的利润率。产生地租的原因在于：（1）肥沃土地的稀缺性；（2）收益递减规律。</p><p>这里引入集约边际（intensive margin）和粗放边际（extensive margin），并假设有三种土地，A级土地使用三个单位的劳动和资本组合，B级土地使用2个单位，C级使用一个单位。集约边际描述了连续追加资本与劳动组合对既定地块的影响。例如一个单位的组合投入A级土地，生产出100单位小麦，继续投入第二单位的组合，总产出190单位小麦，那么第二组合的边际产品为90单位。集约边际反映了边际收益递减原理。因为A级土地的边际收益下降，因此次肥沃的土地进入生产。从A级到B级，代表了粗放边际，可以简单类比为从肥沃的山谷挪动到了山腰。</p><p>在这种基础上，我们可以得到一个这样的表格：</p><table><tr><td rowspan='2'><td colspan='3'>粗放边际</tr><tr><td>A<td>B<td>C</tr><tr><td rowspan='3'>集约边际<td>100<td>90<td>80</tr><tr><td>90<td>80<td></tr><tr><td>80<td><td></tr></table><p>因为地租是对地主的支付，因此，它等同于不同级别土地上的利润率，那么A上的地租为30单位，B是10单位，C不产生地租。如果一单位的资本劳动组合投入到三块C土地上，那么总产品将为240单位谷物。三个单位资本劳动组合投入到一块A土地上，将产生270单位谷物。随着农场主的竞争，A土地的价格（地租）将上升，直到地租等于30单位谷物为止，从而使得两种级别土地上的利润率相等。同理，B的地租将等于10单位谷物。</p><p>上面是从生成成本角度考虑地租，从另一个角度，也就是产品或者产量角度来看，随着土地集约化耕种，生产谷物的边际成本上升。边际成本（marginal cost）被定义为生产最终产品的一个增加量所需要的总成本。假设一单位资本劳动组合在市场上价值100美元，那么A土地上第100单位谷物的边际成本等于1美元，第190单位谷物边际成本是1.11美元（100/90），而最后一单位就是1.25美元（100/80）。B与C的最后一单位谷物边际成本也是1.25美元。如果完全竞争市场存在，那么一定是这种结果，因为随着A的边际成本上升，B得以开始生产。如果最后一单位的边际成本不一样，那么通过转移劳动与资本来减少总生产成本就变成可行的。在长期均衡下，当三种土地的边际实物产品相等时，增加量的边际成本必定相等。</p><p>从成本方考虑是通过货币计算地租。对A来说，总收益是270单位谷物乘以1.25美元，也就是337.5美元。那么为什么是1.25美元的单价呢？在竞争市场里，市场价格等于最后的边际成本，也就是说，市场价格是最低效率下生产出来的谷物的边际成本。如果价格比这个成本高，将不会有人购买；比这个成本低，将不会有人出售。因此单价就是1.25美元。那么成本是300美元的资本劳动组合，因此地租是37.5美元。而B就是12.5美元。C是0。</p><p>那么如果我们资本劳动组合中的劳动成本是75美元，假设A和B不产生地租，也就是不产生地租，那么A的利润率是337.5-75×3，也就是112.5美元。那么一单位资本就是37.5美元。同理B一单位资本就是31.25美元。这样一来，C一单位资本只有25美元，就会进入竞价，拉低A和B的资本利润，那么这一部分的利润就变成A和B的地租。竞争最终的结果就是A、B、C三个土地的资本都是25美元。</p><p>这个模型揭露了地租概念和竞争性市场的几个要点：（1）市场中农场主之间的竞争将使得谷物价格等于成本最高的单位产量的边际成本；（2）对土地的竞争，将使得地租支付给拥有最肥沃土地的地主；（3）竞争将导致所有级别土地拥有统一的利润率。在李嘉图的模型中，地租是价格被决定的因素，而不是决定价格的因素。谷物的高价格不是由高地租决定的，而高地租则是由谷物价格决定的。</p><p>因此在《谷物法》所施加的进口限制，是的集约边际与粗放边际向下推进，其原因在于肥沃土地的稀缺性和收益递减原理。新增劳动与资本组合的边际实物产品将下降，也就是说边际成本将上升，其结果是谷物价格与地租都上升了。</p><p>今天大部分经济学家同意李嘉图的如下观点，即将社会作为一个整体来看，地租并不是生产成本，因此也不是价格的决定因素。土地的数量接近固定；因此当供给的数量不增加的时候，需求的增加将导致较高的价格（地租）。李嘉图将社会当做一个整体，从这个角度来考虑地租，土地的机会成本为零。然而，从社会个别成员的角度来看，地租就是生产成本，从而使价格的决定因素。对农场主而言，地租就是价格的决定因素，因为他们需要向地主支付地租。地租数量等于土地的机会成本（opportunity cost）——等于土地在可替代的其他用途上——例如，土地被用来种植不同的农作物，或者再被分——能够获得的地租数量。今天的经济学家在探讨地租性支付是决定价格的因素，还是价格被决定的因素时，区分了把社会作为一个整体来看时形成的观点，与社会个别成员的角度来看时形成的观点。</p><h2 id="李嘉图的价值理论">李嘉图的价值理论</h2><p>关于《谷物法》的争执，李嘉图的理论得到了很多发展。以马尔萨斯为代表的经济学家主张对进口谷物提高关税有利于英国。但是李嘉图赞同自由国际贸易，反对关税。他推论，高关税降低了利润率，随之将意味着较低的资本积累率。而资本积累率决定了经济增长率。</p><p>贸易保护论者运用生产成本价值论，主张较高的关税不会导致较低的利润。一部分贸易保护论者提出，降低或取消谷物关税将使食物价格与货币工资下降，最终导致所有价格的普遍下降，从而导致经济衰退。</p><p>李嘉图试图反驳当时盛行的生产成本价值理论，因为《谷物法》对经济的影响是它对收入分配的影响。而当时的价值理论试图解释既定时间上相对价格的决定力量。李嘉图认为，当时的价值理论无法解释导致不同时期相对价格变化的经济力量。</p><p>比如之前的海狸与野鹿交换的例子。原本2D=1B，后来变成了3D=1B。那么究竟是海狸价值变高了，还是野鹿价值变低了。这两种解释都是对的，但是如果存在一种不变价值的度量，那么我们就可以知道不同时期相对价格变化的真正原因。</p><p>但是李嘉图意识到并不存在这样的商品。个人觉得，即使是金本位时代，如果发现了新的金矿，实际上也是一种通货膨胀。比如哥伦布发现新大陆后的西班牙实际上是发生了通货膨胀。不过李嘉图并没有完美阐释绝对价值的度量。因此李嘉图对价值关注度主要点在于，是什么导致了不同时期相对价格的变化。</p><p>斯密的劳动成本理论认为，支付给劳动者的工资是对必要的劳动时间的度量。但是李嘉图认为这是一个循环推论，因为工资影响了价格，而价格又会反过来影响工资。他认为价值取决于生产所需要的劳动量，而不是支付给劳动者的工资。</p><p>然后，李嘉图着手解决使用价值与交换价值的混淆问题。经典的水-钻石悖论中，斯密没有看到使用价值与交换价值的度量关系。而李嘉图认为，使用价值尽管不是交换价值的度量，但是对交换价值的存在是基本的。也就是说，一个商品具有实际价格之前，必须存在一种需求，但是需求不是价格的度量，稀缺性和生成所需的劳动量才是。</p><p>但是存在部分商品的价格仅仅由它们的稀缺性决定。这种商品属于不能自由再生产的商品，因此它们的供给不能增加，也就是说，他们的供给曲线完全没有弹性。</p><p>李嘉图的价值理论只适用于能自由再生产以及在完全竞争市场所产生的商品。对于制造业而言，李嘉图假设成本不变，对于农业，他假设成本增加。</p><p>李嘉图放弃了劳动支配价值理论与生产成本价值理论，主张劳动成本价值理论才是合适的。但是劳动成本价值理论存在5个基本难题。</p><ol type="1"><li>度量劳动量。李嘉图用生产一件产品有关的时间量来度量劳动量。也就是现在说的工时。但是个人觉得应该指的是有效工时。</li><li>劳动的不同技能。李嘉图用工资来度量不同劳动者的技能熟练度。乍一看跟斯密的循环推断一样，但是李嘉图假设不同熟练度的工人之间的工资比例是不变的。那么在这样的情况下，商品的价格变化就可以用工资之外的因素来解释。如果我们接受李嘉图这样的假设，那么循环推论也就不存在了。</li><li>资本产品。几乎所有的商品都是由劳动和资本生产的。李嘉图将资本认为是储藏起来的劳动，也就是以前被使用的劳动。劳动与资本共同生产的产品包含的劳动量由立即被使用的劳动量加上储藏在用来生产最终产品的资本产品中的劳动量来度量。也就是说，资本在生产过程中贬值了。不过李嘉图的这一解释并不完美，例如两年前生产一件资本产品的一小时劳动与一年前的一小时劳动相比，它们对今年生产最终产品的价格有不同的影响。如果要正确计算，应该要加总过去所有的劳动和利息成本。但是利息成本不包括在劳动成本内。</li><li>地租。李嘉图认为，地租是价格被决定的因素，而不是决定价格的因素。这个在地租理论中有体现。</li><li>利润。尽管最终销售价格中的利润由于各种原因可能不同，例如资本密集型产业，利润是构成价格的主要成分，而劳动密集型产业劳动量是主要成分。但是李嘉图认为，利润率的影响在数量上并不重要。</li></ol><p>李嘉图始终主张劳动量是解释价格变化的最重要成分。总结一下李嘉图的价值理论。</p><ol type="1"><li>与斯密不同，李嘉图主张使用价值对交换价值的存在来说是必要的。</li><li>仅仅对完全竞争市场中能自由再生产的产品来说，他的劳动价值理论才成立。</li><li>他的主要关注点是解释导致不同时期相对价格变化的经济力量。</li><li>尽管市场价格（短期价格）的变化可能由很多需求与供给因素决定，然而自然价格（长期均衡价格）的变化却通过生产产品所要求的劳动量的变化得到解释。</li><li>尽管某些因素修改了这些原理，例如利润，但是它们没有扰乱如下本质结论，即价值理论中，相对价格的变化多半由生产所需的劳动量来解释。</li></ol><h2 id="李嘉图的分配理论">李嘉图的分配理论</h2><p>李嘉图的价值理论和地租理论都是为了他的分配理论而准备的。</p><p>借助简单的图形，我们可以看到李嘉图的主张。李嘉图的模型中，资本与劳动的组合以固定的比例被添加到经济体可以利用的固定数量的土地上。如图：</p><p><img data-src='https://i.loli.net/2019/04/14/5cb35192e1018.png'></p><p>李嘉图的问题是确定地租，利润，工资之间的分配。ABHQM表示边际实际产品，OC表示资本与劳动组合以某种数量投入可利用的土地，从这一位置开始，所投入的最后一单位资本与劳动组合的边际产品由BC表示，总的农业产量等于OABC。</p><p>在边际量上，地租降为0，所以直线BD上的任何产品将被支付给地主，所以地租等DAB。维持最低生活水平的工资通过马尔萨斯的人口理论得出，假定是EFJQN，那么工资率就是FC，总工资是OEFC。因此利润是BF，总利润是EDBF。可以注意到，利润水平取决于最后一单位资本与劳动组合的编辑产品以及维持最低生活水平的实际工资。</p><p>李嘉图巧妙地通过减法来分析，因此收入分配理论又被称为剩余理论（residual theory）。</p><p>李嘉图极大的兴趣是资本家、地主以及劳动者所获得的国民收入的相对份额在不同时间的变化。李嘉图认为斯密随时间变化利率下降的观点是正确的，但是，他否定了斯密的所有论据。</p><p>对于第一个理由，劳动市场竞争太高工资，利润下降。李嘉图认为，按照马尔萨斯的人口学说，工资上升，人口增加，工资又会被拉低到以前的水平。</p><p>通过萨伊定律，李嘉图认为，斯密对利润下降的第二、第三理由意味着存在普通的产出过剩，只有不能按照以前的价格销售由于新的投资而增加的产量才会导致利润下降。也就是现在说的供过于求。但是萨伊定律认为不会存在这种情况。萨伊定律会在后面讨论。</p><p>在农业中，早起的经济体利润很高，资本积累率高。这种资本积累率提高了工资率，以马尔萨斯的人口学说，人口增加，农业的粗放边际和集约边际被向下推进。这导致了地租上升，利润下降，资本积累率降低，直到利润为0。这时经济增长停止，人口增长停止，工资处于维持生活水平的最低位置，地租很高。</p><p>那制造业，如果在完全竞争市场中，由于长期均衡下，整个经济体的利润都会相等，因此农业利润下降，工业利润也会下降。一旦李嘉图模型中的动力，也就是资本积累减少了，整个体系会受到影响，最终达到古典静止状态。</p><p>回到《谷物法》，李嘉图认为，《谷物法》使得英国的谷物产量扩大，集约边际和粗放边际向下推进，利润随地租的上升而下降。《谷物法》加速了经济体达到古典静止状态的过程。</p><h2 id="李嘉图的比较优势理论">李嘉图的比较优势理论</h2><p>利用比较优势，李嘉图强调了自由贸易主张。在讨论比较优势前，先考察每个国家在其中一种商品上具有绝对优势时候的国际贸易。假设每单位劳动产量如下表：</p><table><thead><tr class="header"><th></th><th>酒</th><th>布</th></tr></thead><tbody><tr class="odd"><td>英国</td><td>4</td><td>2</td></tr><tr class="even"><td>葡萄牙</td><td>8</td><td>1</td></tr></tbody></table><p>假设上表存在，英国完全生产布匹，有4个单位，葡萄牙完全生产酒有16单位。这样全世界就多了1单位布和4单位酒。因此交换价格处于8单位酒换一单位布和2单位酒换1单位布之间时候，交易可以存在。</p><p>那么如果一国在各种产品上都更有优势呢？</p><table><thead><tr class="header"><th></th><th>酒</th><th>布</th></tr></thead><tbody><tr class="odd"><td>英国</td><td>12</td><td>6</td></tr><tr class="even"><td>葡萄牙</td><td>8</td><td>1</td></tr></tbody></table><p>比较优势表明，在英国为了多1单位布，需要放弃2单位酒，但是葡萄牙则需要放弃8单位酒，因此葡萄牙生产酒更有优势，英国生产布更有优势。那么英国全部生产布，得到12单位布，葡萄牙转移2单位布的劳动与资本到酒，就得到24单位酒，这样全世界的产量还是增加了。</p><p>因此，比较优势实际上是各国机会成本的比较。如果两国的机会成本一样，贸易就无法存在。</p><p>李嘉图没有考虑到的是贸易收益如何分割，约翰·斯图亚特·穆勒解决了这个问题。他提出，贸易条件或者说国际价格将取决于参与国家的商品需求的相对力量。</p><p>比较优势揭露了一个事实，那就是关税复旦最后还是由本国自己承担。现在的贸易战必然是两败俱伤呀。</p><p>与之前的重农主义或者重商主义不同，比较优势学说具有广泛而重要的意义。实际上，重农主义和重商主义都假设了产品总量是固定的，这也是一些理论暗含的假设。也就是零和博弈？</p><h1 id="三资本主义经济体的稳定与增长">三、资本主义经济体的稳定与增长</h1><p>李嘉图与马尔萨斯之间的对资本主义体质保持资源充分利用能力的争论被认为是对萨伊定律的争论。萨伊定律认为，资本主义体质将自动实现资源的充分利用和较高的经济增长速度。当然，最后这一观点被凯恩斯终结。这一争论的历史进程是从重商主义开始的。</p><h2 id="重商主义的总需求观点">重商主义的总需求观点</h2><p>大多数重商主义者认为，个人节俭与储蓄有利于国家。当然，曼德维尔抨击了这一观点，他认为增加贸易和希望减少奢侈是一种矛盾。</p><h2 id="斯密的总需求观点">斯密的总需求观点</h2><p>斯密认为，资本积累才是繁荣与增长的主要决定力量，对斯密而言，储蓄不是减少了总需求，仅仅是使得需求从消费变为投资。</p><h2 id="马尔萨斯的消费不足主义">马尔萨斯的消费不足主义</h2><p>马尔萨斯在对资本积累过程的讨论中，提出了天真又成熟的想法。他天正的主张是，劳动者没有获得全部产品。因此，单独就劳动者需求而言，不足以按照满意的价格购买全部最终产品。这是正确的，但是资本家如果以生产者产品需求的方式，将他们的储蓄返还市场上，那么就不存在总需求不足。</p><p>他成熟的见解在于，他主张“储蓄——投资”过程无法无限进行下去而不导致长期停滞。他主张，存在一个经济体能够吸收的适宜的资本积累率，过多的储蓄与投资将引起难以对付的问题。同事马尔萨斯意识到，要保持资本主义体制中的资源充分利用，必须保证总产量水平与总消费水平的扩张。</p><p>马尔萨斯断定，因为劳动者与资本家方面存在不充分的有效需求，所以，一定要通过社会上那些只消费不生产的人来填充缺口。也就是第三产业从业人员与地主。</p><h2 id="萨伊定律">萨伊定律</h2><p>正统古典经济学家认为，生产产品的过程中产生了充足的购买力，能按满意的价格将这些产品带离市场，也就是整体市场不会发生供大于求。</p><p>萨伊定律认为，供给创造出它自身的需求。供给创造潜在需求并没有问题，但是潜在需求是否可以变成市场上的有效需求是一个关键问题。李嘉图，詹姆士·穆勒，萨伊认为潜在购买力会作为消费者产品或生产者产品回到市场上。但是他们都只将货币当做交换媒介，而没有考虑到货币的储藏价值。实际上，货币天生的属性之一就是储藏。不过马尔萨斯没有发现并发展这一点。</p><h2 id="货币理论">货币理论</h2><p>离阿基图对萨伊定律的看法是在19世纪中期的争论中得到发展的。这些争论被成为金银争论（Bullion Debates）。争论的焦点是拿破仑战争时期通货膨胀的原因是什么。</p><p>金银通货主义者认为，通胀是发生在战争期间的货币扩张，也就是说是一种货币现象。</p><p>反金银通货主义者认为，通胀的原因很复杂，他们赞同真实票据学说，主张如果货币发行涉及短期金融商业操作，则不会有货币的过度发行。</p><p>李嘉图认为经济体的“活动”是在实体部门发生的，货币只是一种反映。不过亨利·桑顿曾意识到货币不仅是一种反映，还对实体经济有影响。但是受制于李嘉图的威望，最后接受了李嘉图的货币数量理论。</p><h2 id="技术性失业">技术性失业</h2><p>李嘉图在1821年的第三版《原理》中加了一章“论机器”，之前他主张使用新机器不会减少劳动的需求，但是这一版他改变了这一观点。</p><p>他认为，如果最新使用的机器是通过将流动资本转换为固定资本来筹集资金的，那么工资基金就会减少，失业将会发生。虽然李嘉图并没有说明失业会发生多久。</p><h1 id="总结">总结</h1><p>李嘉图代表了从斯密方法——理论与历史描述的松散结合——向高度抽象的经济模型方法的明显突破。李嘉图能够表明劳动成本价值理论的有点和缺点，并说明当时紧迫的政策问题。他利用自己的主张，通过显示来自自由和开放国际贸易中的福利收益，来强调斯密的自由放任情形。他集合了马尔萨斯的人口学说和工资基金理论来表明不可能改进低收入群体的命运。他对萨伊定律的捍卫压制了一些批评家，这些批评家发现了一些资本主义体制的缺陷，其中包括有关储蓄与投资的决策是由私人个别地做出的这种缺陷。他的经济学动摇了地主的地位，在政治力量上地主输给了新兴资本家。他对接近静止状态的分析，为资本主义未来投下了长长的阴影。到了19世纪中期，马克思将李嘉图的工具与其他分析相结合，打造了自己的理论，即资本主义仅仅是历史上的一个阶段，包含了毁灭它自身的种子。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;古典经济学，李嘉图与马尔萨斯部分。主要是李嘉图，简直是个宝藏男孩。一个人引导了整个经济界，构建了引导经济思想一百年的理论框架。&lt;/p&gt;</summary>
    
    
    
    <category term="读书笔记" scheme="https://samaelchen.github.io/categories/ReadingNotes/"/>
    
    
  </entry>
  
  <entry>
    <title>文本生成</title>
    <link href="https://samaelchen.github.io/pytorch_text_generation/"/>
    <id>https://samaelchen.github.io/pytorch_text_generation/</id>
    <published>2018-11-01T16:00:00.000Z</published>
    <updated>2021-08-31T16:15:25.581Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>最近诸事不顺，情绪不佳。继续做文本生成的事情。之前用的Char-RNN存在一定的缺陷，那就是你需要给定一个prefix，然后模型就会顺着prefix开始一个个往下预测。但是这样生成的文本随机性是很大的，所以我们希望能够让句子根据我们的关键词或者topic来生成。看了几篇论文，大框架上都是基于Attention的，其他的都是一些小的细节变化。这里打算实现两篇论文里的框架，一篇是哈工大的<a href="http://ir.hit.edu.cn/~xcfeng/xiaocheng%20Feng&#39;s%20Homepage_files/final-topic-essay-generation.pdf">Topic-to-Essay Generation with Neural Networks</a>，另一篇是百度的<a href="https://arxiv.org/pdf/1610.09889.pdf">Chinese Poetry Generation with Planning based Neural Networks</a>。</p><hr /><p>2018年11月8号更新：认真看了一下百度的那篇paper，模型跟TAV的差不了太多，就是先用一个RNN把关键词做个双向的encoding，然后当做第一个词放进去训练。没什么兴趣弄了。</p><span id="more"></span><p>第一篇论文里面放了三种策略，由简到繁分别是Topic-Averaged LSTM，Attention-based LSTM，以及Multi-Topic-Aware LSTM。</p><p>其实策略上来说，TAV-LSTM就是将topic的embedding做一个平均，然后作为prefix来训练，所以基本上网络设计上也和之前的Char-RNN差不多，比较容易实现。TAT-LSTM就是将topic做一个Attention，然后作为一个feature跟hidden并到一起喂到decoder里面去。MTA-LSTM还包含了一个叫做coverage vector的向量来计算topic的信息是否在训练过程中被喂进去了。</p><p>官方放了一个很久以前的TensorFlow版本的<a href="https://github.com/hit-computer/MTA-LSTM">MTA-LSTM</a>，一方面我不喜欢TF，另一方面版本太老旧了，所以就用只能自己摸索着写PyTorch版本的了。数据就直接用的这个git上面提供的composition和zhihu两个数据。</p><p>然后这里都是用的贪婪法取候选词，没有做束搜索。当然，主要是因为懒，后面糟心事情过去了再说吧。</p><h1 id="tav">TAV</h1><p>TAV的大概工作原理上面也提到了，这里不赘述。然后同样偷懒，用了之前Char-RNN的模型直接修改。</p><p>首先常规套路，训练词向量。说起来，腾讯之前开源了一个800w+的词向量，也可以用用。这个就不多说了，很简单。</p><p>然后就是处理一下数据，首先我们要加入四个特殊字符PAD，BOS，EOS，和UNK。都是常规套路。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fvec = KeyedVectors.load_word2vec_format(<span class="string">&#x27;vec.txt&#x27;</span>, binary=<span class="literal">False</span>)</span><br><span class="line">word_vec = fvec.vectors</span><br><span class="line">vocab = [<span class="string">&#x27;&lt;PAD&gt;&#x27;</span>, <span class="string">&#x27;&lt;BOS&gt;&#x27;</span>, <span class="string">&#x27;&lt;EOS&gt;&#x27;</span>, <span class="string">&#x27;&lt;UNK&gt;&#x27;</span>]</span><br><span class="line">vocab.extend(<span class="built_in">list</span>(fvec.vocab.keys()))</span><br><span class="line">word_vec = np.concatenate((np.array([[<span class="number">0</span>]*word_vec.shape[<span class="number">1</span>]] * <span class="number">4</span>), word_vec))</span><br><span class="line">word_vec = torch.tensor(word_vec)</span><br></pre></td></tr></table></figure><p>然后就是要做idx to word和word to idx的转换器。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">word_to_idx = &#123;ch: i <span class="keyword">for</span> i, ch <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)&#125;</span><br><span class="line">idx_to_word = &#123;i: ch <span class="keyword">for</span> i, ch <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)&#125;</span><br></pre></td></tr></table></figure><p>然后就是读数据，做iterator。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">essays = []</span><br><span class="line">topics = []</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;composition.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">        essay, topic = line.replace(<span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;&#x27;</span>).split(<span class="string">&#x27; &lt;/d&gt; &#x27;</span>)</span><br><span class="line">        essays.append(essay.split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line">        topics.append(topic.split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line"></span><br><span class="line">corpus_indice = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: [word_to_idx[w] <span class="keyword">for</span> w <span class="keyword">in</span> x], essays[:<span class="number">8000</span>]))</span><br><span class="line">topics_indice = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: [word_to_idx[w] <span class="keyword">for</span> w <span class="keyword">in</span> x], topics[:<span class="number">8000</span>]))</span><br><span class="line">length = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: <span class="built_in">len</span>(x), corpus_indice))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tav_data_iterator</span>(<span class="params">corpus_indice, topics_indice, batch_size, num_steps</span>):</span></span><br><span class="line">    epoch_size = <span class="built_in">len</span>(corpus_indice) // batch_size</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch_size):</span><br><span class="line">        raw_data = corpus_indice[i*batch_size: (i+<span class="number">1</span>)*batch_size]</span><br><span class="line">        key_words = topics_indice[i*batch_size: (i+<span class="number">1</span>)*batch_size]</span><br><span class="line">        data = np.zeros((<span class="built_in">len</span>(raw_data), num_steps+<span class="number">1</span>), dtype=np.int64)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">            doc = raw_data[i]</span><br><span class="line">            tmp = [<span class="number">1</span>]</span><br><span class="line">            tmp.extend(doc)</span><br><span class="line">            tmp.extend([<span class="number">2</span>])</span><br><span class="line">            tmp = np.array(tmp, dtype=np.int64)</span><br><span class="line">            _size = tmp.shape[<span class="number">0</span>]</span><br><span class="line">            data[i][:_size] = tmp</span><br><span class="line">        key_words = np.array(key_words, dtype=np.int64)</span><br><span class="line">        x = data[:, <span class="number">0</span>:num_steps]</span><br><span class="line">        y = data[:, <span class="number">1</span>:]</span><br><span class="line">        mask = np.float32(x != <span class="number">0</span>)</span><br><span class="line">        x = torch.tensor(x)</span><br><span class="line">        y = torch.tensor(y)</span><br><span class="line">        mask = torch.tensor(mask)</span><br><span class="line">        key_words = torch.tensor(key_words)</span><br><span class="line">        <span class="keyword">yield</span>(x, y, mask, key_words)</span><br></pre></td></tr></table></figure><p>这里也是简单处理了，很多细节慢慢修改吧，然后就是这里的mask，我也是偷懒不去弄了，其实是标识那些词用来训练，哪些是padding的，我后面在loss function那里直接将PAD的权重改成0了。</p><p>然后就是定义网络。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TAVLSTM</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_dim, embed_dim, num_layers, weight,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_labels, bidirectional, dropout=<span class="number">0.5</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(TAVLSTM, self).__init__(**kwargs)</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.num_labels = num_labels</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        <span class="keyword">if</span> num_layers &lt;= <span class="number">1</span>:</span><br><span class="line">            self.dropout = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.dropout = dropout</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(weight)</span><br><span class="line">        self.embedding.weight.requires_grad = <span class="literal">False</span></span><br><span class="line">        self.rnn = nn.GRU(input_size=self.embed_dim, hidden_size=self.hidden_dim,</span><br><span class="line">                          num_layers=self.num_layers, bidirectional=self.bidirectional,</span><br><span class="line">                          dropout=self.dropout)</span><br><span class="line">        <span class="keyword">if</span> self.bidirectional:</span><br><span class="line">            self.decoder = nn.Linear(hidden_dim * <span class="number">2</span>, self.num_labels)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.decoder = nn.Linear(hidden_dim, self.num_labels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs, topics, hidden=<span class="literal">None</span></span>):</span></span><br><span class="line">        embeddings = self.embedding(inputs)</span><br><span class="line">        topics_embed = self.embedding(topics)</span><br><span class="line">        topics_embed = topics_embed.mean(dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(embeddings.shape[<span class="number">0</span>]):</span><br><span class="line">            embeddings[i][<span class="number">0</span>] = topics_embed[i]</span><br><span class="line">        states, hidden = self.rnn(embeddings.permute([<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]).<span class="built_in">float</span>(), hidden)</span><br><span class="line">        outputs = self.decoder(states.reshape((-<span class="number">1</span>, states.shape[-<span class="number">1</span>])))</span><br><span class="line">        <span class="keyword">return</span>(outputs, hidden)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span>(<span class="params">self, num_layers, batch_size, hidden_dim, **kwargs</span>):</span></span><br><span class="line">        hidden = torch.zeros(num_layers, batch_size, hidden_dim)</span><br><span class="line">        <span class="keyword">return</span> hidden</span><br></pre></td></tr></table></figure><p>基本结构没变化，就是forward的时候做了一点小修改，把第一个词变成topic average。</p><p>然后定义预测函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_rnn</span>(<span class="params">topics, num_chars, model, device, idx_to_word, word_to_idx</span>):</span></span><br><span class="line">    output = [<span class="number">1</span>]</span><br><span class="line">    topics = [word_to_idx[x] <span class="keyword">for</span> x <span class="keyword">in</span> topics]</span><br><span class="line">    topics = torch.tensor(topics)</span><br><span class="line">    hidden = torch.zeros(num_layers, <span class="number">1</span>, hidden_dim)</span><br><span class="line">    <span class="keyword">if</span> use_gpu:</span><br><span class="line">        hidden = hidden.to(device)</span><br><span class="line">        topics = topics.to(device)</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(num_chars):</span><br><span class="line">        X = torch.tensor(output).reshape((<span class="number">1</span>, <span class="built_in">len</span>(output)))</span><br><span class="line">        <span class="keyword">if</span> use_gpu:</span><br><span class="line">            X = X.to(device)</span><br><span class="line">        pred, hidden = model(X, topics, hidden)</span><br><span class="line">        <span class="keyword">if</span> pred.argmax(dim=<span class="number">1</span>)[-<span class="number">1</span>] == <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output.append(<span class="built_in">int</span>(pred.argmax(dim=<span class="number">1</span>)[-<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">return</span>(<span class="string">&#x27;&#x27;</span>.join([idx_to_word[i] <span class="keyword">for</span> i <span class="keyword">in</span> output[<span class="number">1</span>:]]))</span><br></pre></td></tr></table></figure><p>设定一下参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">embedding_dim = <span class="number">300</span></span><br><span class="line">hidden_dim = <span class="number">256</span></span><br><span class="line">lr = <span class="number">1e2</span></span><br><span class="line">momentum = <span class="number">0.0</span></span><br><span class="line">num_epoch = <span class="number">100</span></span><br><span class="line">use_gpu = <span class="literal">True</span></span><br><span class="line">num_layers = <span class="number">1</span></span><br><span class="line">bidirectional = <span class="literal">False</span></span><br><span class="line">batch_size = <span class="number">8</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">loss_function = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">model = TAVLSTM(hidden_dim=hidden_dim, embed_dim=embedding_dim, num_layers=num_layers,</span><br><span class="line">                num_labels=<span class="built_in">len</span>(vocab), weight=word_vec, bidirectional=bidirectional)</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)</span><br><span class="line"><span class="keyword">if</span> use_gpu:</span><br><span class="line">    model.to(device)</span><br></pre></td></tr></table></figure><p>接着训练就好了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">since = time.time()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epoch):</span><br><span class="line">    start = time.time()</span><br><span class="line">    num, total_loss = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"><span class="comment">#     if epoch == 5000:</span></span><br><span class="line"><span class="comment">#         optimizer.param_groups[0][&#x27;lr&#x27;] = lr * 0.1</span></span><br><span class="line">    data = tav_data_iterator(corpus_indice, topics_indice, batch_size, <span class="built_in">max</span>(length)+<span class="number">1</span>)</span><br><span class="line">    hidden = model.init_hidden(num_layers, batch_size, hidden_dim)</span><br><span class="line">    weight = torch.ones(<span class="built_in">len</span>(vocab))</span><br><span class="line">    weight[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> X, Y, mask, topics <span class="keyword">in</span> tqdm(data):</span><br><span class="line">        num += <span class="number">1</span></span><br><span class="line">        hidden.detach_()</span><br><span class="line">        <span class="keyword">if</span> use_gpu:</span><br><span class="line">            X = X.to(device)</span><br><span class="line">            Y = Y.to(device)</span><br><span class="line">            mask = mask.to(device)</span><br><span class="line">            topics = topics.to(device)</span><br><span class="line">            hidden = hidden.to(device)</span><br><span class="line">            weight = weight.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output, hidden = model(X, topics, hidden)</span><br><span class="line">        l = F.cross_entropy(output, Y.t().reshape((-<span class="number">1</span>,)), weight)</span><br><span class="line">        l.backward()</span><br><span class="line">        norm = nn.utils.clip_grad_norm_(model.parameters(), <span class="number">1e-2</span>)</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += l.item()</span><br><span class="line">    end = time.time()</span><br><span class="line">    s = end - since</span><br><span class="line">    h = math.floor(s / <span class="number">3600</span>)</span><br><span class="line">    m = s - h * <span class="number">3600</span></span><br><span class="line">    m = math.floor(m / <span class="number">60</span>)</span><br><span class="line">    s -= m * <span class="number">60</span></span><br><span class="line">    <span class="keyword">if</span>(epoch % <span class="number">10</span> == <span class="number">0</span>) <span class="keyword">or</span> (epoch == (num_epoch - <span class="number">1</span>)):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch %d/%d, loss %.4f, norm %.4f, time %.3fs, since %dh %dm %ds&#x27;</span></span><br><span class="line">              %(epoch+<span class="number">1</span>, num_epoch, total_loss / num, norm, end-start, h, m, s))</span><br><span class="line">        <span class="built_in">print</span>(predict_rnn([<span class="string">&#x27;妈妈&#x27;</span>, <span class="string">&#x27;希望&#x27;</span>, <span class="string">&#x27;长大&#x27;</span>, <span class="string">&#x27;孩子&#x27;</span>, <span class="string">&#x27;母爱&#x27;</span>], <span class="number">100</span>, model, device, idx_to_word, word_to_idx))</span><br></pre></td></tr></table></figure><p>具体还是看我的<a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/pytorch/text%20generater/Topic2Essay_train.ipynb">notebook</a>。不过梯度还是爆炸了，哎。</p><h1 id="tat">TAT</h1><p>就是在TAV的基础上修改，直接看<a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/pytorch/text%20generater/Topic2Essay_TAT.ipynb">notebook</a>吧。这个模型深刻地表达了我的内心正处在TAT的状态。</p><h1 id="mat">MAT</h1><p>论文里面的<span class="math inline">\(U_f\)</span>没看懂是什么意思，所以就自己演绎了一下，简单来说，为了让每个topic都有机会出现，那么很自然会想到要去调整Attention的权重，高的压低一点，低的抬高一点。所以我在每个epoch结束后调整一下： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">params = model.state_dict()</span><br><span class="line">params[<span class="string">&#x27;attn.weight&#x27;</span>].clamp_(<span class="number">0</span>)</span><br><span class="line">params[<span class="string">&#x27;attn.weight&#x27;</span>] *= <span class="number">1</span> / -torch.log(params[<span class="string">&#x27;attn.weight&#x27;</span>] / torch.<span class="built_in">sum</span>(params[<span class="string">&#x27;attn.weight&#x27;</span>]) + <span class="number">0.000001</span>)</span><br></pre></td></tr></table></figure> 也就是说，每个Attention前面加了一个weight，这个weight是<span class="math inline">\(-\log(p+\lambda)\)</span>。加个lambda是避免变成0，而p就是这个topic的Attention在所有topic的Attention的比重。这个其实也可以试试每一个iteration就变化会怎么样。</p><p>具体看<a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/pytorch/text%20generater/Topic2Essay_MAT_bleu-adaptive.ipynb">notebook</a>。</p><p>另外要说的就是，用全量softmax速度太慢了，改用了PyTorch的adaptive softmax。0.41版本以上自带的一个功能，是一个softmax的优化方案，论文说比hierarchical softmax在GPU上的表现更好，虽然论文没太看懂。这里有篇老外的<a href="https://towardsdatascience.com/speed-up-your-deep-learning-language-model-up-to-1000-with-the-adaptive-softmax-part-1-e7cc1f89fcc9">博客</a>大概讲了一下原理，不过也没证明为什么会更好。大概意思就是将所有的词按照词频排序，然后分成高频和低频两组，然后低频组再拆成两到四个组，然后判断这个词是在哪个组里面。</p><p>还有就是TAT的模型改了一下要求Attention必须都是大于等于0的，MAT也是在这个基础上搞的。</p><p>差不多就这么一回事吧。诸事不顺，近期要看个病，但愿不是重病。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;最近诸事不顺，情绪不佳。继续做文本生成的事情。之前用的Char-RNN存在一定的缺陷，那就是你需要给定一个prefix，然后模型就会顺着prefix开始一个个往下预测。但是这样生成的文本随机性是很大的，所以我们希望能够让句子根据我们的关键词或者topic来生成。看了几篇论文，大框架上都是基于Attention的，其他的都是一些小的细节变化。这里打算实现两篇论文里的框架，一篇是哈工大的&lt;a href=&quot;http://ir.hit.edu.cn/~xcfeng/xiaocheng%20Feng&amp;#39;s%20Homepage_files/final-topic-essay-generation.pdf&quot;&gt;Topic-to-Essay Generation with Neural Networks&lt;/a&gt;，另一篇是百度的&lt;a href=&quot;https://arxiv.org/pdf/1610.09889.pdf&quot;&gt;Chinese Poetry Generation with Planning based Neural Networks&lt;/a&gt;。&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;2018年11月8号更新：认真看了一下百度的那篇paper，模型跟TAV的差不了太多，就是先用一个RNN把关键词做个双向的encoding，然后当做第一个词放进去训练。没什么兴趣弄了。&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://samaelchen.github.io/categories/DeepLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>tiny XGBoost以及集成算法回顾</title>
    <link href="https://samaelchen.github.io/tinyxgb/"/>
    <id>https://samaelchen.github.io/tinyxgb/</id>
    <published>2018-10-07T16:00:00.000Z</published>
    <updated>2021-08-31T16:15:25.582Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>XGBoost是GBDT的一个超级加强版，用了很久，一直没细看原理。围观了一波人家的实现，自己也来弄一遍。以后面试上来一句，要不要我现场写个XGBoost，听上去是不是很霸气。</p><p>在开源代码的基础上进行了一点修改，大概跑通了，但是有些地方感觉有点诡异。后面会讲。</p><span id="more"></span><h1 id="回顾一下ensemble算法">回顾一下ensemble算法</h1><p>之前也写了一篇笔记，<a href="https://samaelchen.github.io/machine_learning_step17/">台大李宏毅机器学习——集成算法</a>，最近把一些错误做了修改。</p><h2 id="bagging">bagging</h2><p>bagging的经典算法就是随机森林了，bagging的思路其实非常非常的简单，就是同时随机抽样本和feature，然后建立n个分类器，接着投票就好了。</p><p>bagging的做法本质上不会解决bias太大的问题，所以该过拟合还会过拟合。但是bagging会解决variance太大的问题。这个其实是非常直观的一件事情。</p><p>然后我突然开了一个脑洞，RF选择feature这种事情吧，如果用一个weight来表示，把树换成逻辑回归，最后voting的事情也用一个weight来表示，感觉，似乎就是单层神经网络的既视感。anyway，无脑揣测，有待探究。</p><h2 id="boosting">boosting</h2><p>boosting跟bagging的套路就不一样，bagging是同时并行很多分类器，但是boosting是串行多个分类器。bagging的分类器之间没有依赖关系，boosting的分类器是有依赖关系的。</p><p>boosting算法比较知名的就是GBDT和Adaboost两个。不过其实Adaboost就是一个特殊的GBDT。</p><p>GB的一般流程是这样的：</p><blockquote><p>初始化一个函数<span class="math inline">\(g_0(x) = 0\)</span><br />然后按照迭代次数从<span class="math inline">\(t=1\)</span>到<span class="math inline">\(T\)</span>循环，我们的目标是找到一个函数<span class="math inline">\(f_t(x)\)</span>和权重<span class="math inline">\(\alpha_t\)</span>使得我们的函数<span class="math inline">\(g_{t-1}(x)\)</span>的效果更好，也就是说：<br /><span class="math display">\[g_{t-1}(x) = \sum_{i=1}^{t-1} \alpha_i f_i(x)\]</span> 换个角度来看就是<span class="math inline">\(g_t(x) = g_{t-1}(x) + \alpha_t f_t(x)\)</span><br />而最后我们优化的损失函数<span class="math inline">\(L(g) = \sum_n l(y_n, g(x_n))\)</span></p></blockquote><p>那Adaboost就是GB的损失函数<span class="math inline">\(l\)</span>用exponential表示，也就是<span class="math inline">\(\exp(-y_n g(x_n))\)</span>。很美妙的一家子。</p><p>那实际上我们看<span class="math inline">\(g_t(x) = g_{t-1}(x) + \alpha_t f_t(x)\)</span>这里，非常像梯度下降，那么如果要做梯度下降的话，其实我们就是对<span class="math inline">\(g(x)\)</span>做偏导，所以我们得到的是<span class="math inline">\(g_t(x) = g_{t-1}(x) - \eta \frac{\partial L(g)}{\partial g(x)} \bigg|_{g(x)=g_{t-1}(x)}\)</span>。那其实只要我们想办法让尾巴后面的那一部分是同一个方向的，我们不就达到了梯度下降的目的了吗？！步子的大小是可以用<span class="math inline">\(\eta\)</span>调的，同样这边也可以调整<span class="math inline">\(\alpha\)</span>。总之，保证他们的方向一致，就可以做梯度下降了。</p><h3 id="adaboost">Adaboost</h3><p>现在将Adaboost的损失函数放进来推算一下： <span class="math display">\[\begin{align}L(g) &amp;= \sum_n \exp(-y_n g_t(x_n)) \\&amp;= \sum_n \exp(-y_n (g_{t-1}(x_n) + \alpha_t f_t(x))) \\&amp;= \sum_n \exp(-y_n g_{t-1}(x_n)) \exp(-y_n \alpha_t f_t(x_n)) \\&amp;= \sum_{f_t(x) \ne y} \exp(-y_n g_{t-1}(x_n)) \exp(\alpha_t) + \sum_{f_t(x) = y} \exp(-y_n g_{t-1}(x_n)) \exp(-\alpha_t)\end{align}\]</span></p><p>这样一来，我们需要同时寻找一个<span class="math inline">\(\alpha\)</span>和<span class="math inline">\(f(x)\)</span>使得我们的损失函数最小。找一个最优的<span class="math inline">\(f(x)\)</span>这个事情只要让决策树去学习就可以了，那么<span class="math inline">\(\alpha\)</span>怎么办呢？如果我们对<span class="math inline">\(\alpha\)</span>求偏导，让偏导数为0，那么理论上，我们在<span class="math inline">\(\alpha\)</span>这个方向上就是最优的。</p><p>具体求偏导这个之前的博客写了，这里就不多搞了。简单说就是刚好会等于<span class="math inline">\(\ln \sqrt{(1-\varepsilon_t) / \varepsilon_t}\)</span>，而这个值刚好是每一轮调整样本权重时候的系数取对数。</p><h3 id="gbdt">GBDT</h3><p>至于GBDT，实际上跟Adaboost没啥区别，也是一样的搞法，无非是损失函数不一样，优化策略不太一样而已。</p><p>GBDT的玩法是用每一棵树去学习上一棵树的残差，通俗的说就是下一棵树学习如何矫正上一棵树的错误。</p><p>但是残差这东西也是很妙的一个事情，如果我们损失函数是RMSE，其实我们的残差就是一阶导数。但是呢，如果损失函数是别的函数的时候，其实“残差”这个东西就很难去计算，比如分类的时候，残差究竟是个什么意思？！所以Freidman的梯度提升算法，也就是GBDT前面的GB就是用损失函数对<span class="math inline">\(f(x)\)</span>的一阶导数来替代残差的。</p><p>总体来说，我们想优化的损失函数是： <span class="math display">\[\Theta_t = \arg \min_{\Theta_t} \sum_{t=1}^{T} l(y_n, g_{t-1}(x_n) + \alpha_t f_t(x))\]</span></p><p>如果我们做回归问题，用RMSE为loss function，那么<span class="math inline">\(l(y_n, g_t(x_n))=(y_n - g_t(x_n))^2\)</span>，而<span class="math inline">\(y_n - g_t(x_n)\)</span>就是传说中的残差。所以用一阶导数来替代残差真的是神来之笔。这样每次的树直接去学习梯度，而在回归的时候残差跟一阶导数还是一样的，美滋滋。</p><p>然后突然有个很不成熟的想法，GBDT是不是很怕one hot的feature呢？！举个例子，如果所有的feature都是one hot的，同时我们每个DT都只有一层，是不是理论上来说，最后有多少feature就有多少树。</p><h1 id="xgboost">XGBoost</h1><p>嗯，就是一个增强版GBDT。</p><p>强在哪呢，boosting是一个加法训练，原来我们用的是一阶导数，而陈天奇则是用了二阶导并加了个正则项。</p><p>但是陈天奇做二阶导的目的怎么说呢，我不是非常理解。是inspired by GBDT的RMSE？！这个手算一下，我们的损失函数用RMSE的时候会有一个<span class="math inline">\((\alpha_t f_t(x))^2\)</span>，而这个东西类比泰勒展开的时候就是那个<span class="math inline">\(\Delta x\)</span>，所以怎么说呢，可能大牛是因为看到这个时候灵感乍现，然后尝试用了二阶导。</p><p>然后在XGBoost里面的损失函数就可以用二阶泰勒展开来表示： <span class="math display">\[\Theta = \sum_{i=1}^n [g_i f_t(x_i) + \frac{1}{2} h_i f^2_t(x_i)] + \Omega(f_t)\]</span> 其中的<span class="math inline">\(g_i\)</span>和<span class="math inline">\(h_i\)</span>分别是一阶导数和二阶导数。这样定义的好处是，以后不论用什么损失函数，总体的优化方向仅仅跟一阶导数和二阶导数有关。那么在这个大框架底下，任意可以二阶导的损失函数都可以放进来了。那么我看了XGB的<a href="https://arxiv.org/pdf/1603.02754.pdf">论文</a>关于为什么取二阶导的描述： <img data-src='https://i.loli.net/2018/10/11/5bbec3573482d.png'> 直观感受上来说吧，就有点像拟牛顿法是梯度下降的一个功能性提升的样子。</p><p>另外就是正则项，这里<span class="math inline">\(\Omega(f) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_j^2\)</span>。这里的<span class="math inline">\(T\)</span>是我们有多少叶节点，<span class="math inline">\(w\)</span>是叶节点的权重。这两个都在XGB的参数里面可以调整的，我们都知道正则项会改变模型的保守程度，或者说就是variance。而<span class="math inline">\(w\)</span>在这里就是<span class="math inline">\(f_t(x)\)</span>。这里要是不理解，可以看<a href="http://www.52cs.org/?p=429">陈天奇的这篇中文解释</a>。</p><p>其他XGB的优化很多是工程上的优化，这个不是CS科班出身就看不懂了。如何并行化什么的，一脸懵逼。具体可以看陈天奇的论文，数学的部分写的非常美妙。</p><h1 id="tiny-xgb的一些坑">tiny XGB的一些坑</h1><p>完全自己写太可怕了，在<a href="https://github.com/lancifollia/tinygbt/blob/master/tinygbt.py">tinygbt</a>的基础上直接修改了一下，原作的代码逻辑非常清晰。anyway，虽然我觉得有些地方貌似是有问题的，做了一点小改动，这个是<a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/fun/tinyxgb.ipynb">我的修改版</a>。</p><p>一个事情就是原作的模型里面learning rate是会越跑越小的，我改成了固定的。另外算split gain的时候我按照论文的公式写的。</p><p>还有就是我改了原作的梯度和loss，这样我可以传一些其他损失函数进去，虽然我没试过行不行得通吧。理论上应该OK的吧。XD</p><p>然后我觉得原作一个地方我没看懂，就是计算weight的地方，按照论文的公式是<span class="math inline">\(w_j = -\frac{G_j}{H_j + \lambda}\)</span>，但是在这里实现的时候是<span class="math inline">\(\frac{G_j}{H_j + \lambda}\)</span>，就是正负之差。但是很神奇的是我用负号的时候，就不能收敛了。没懂为什么，明明我按照公式求偏导就是负的，实现的时候就不对了。</p><p>最后算是一个坑吧，原作数据集用的是LightGBM的<a href="https://github.com/Microsoft/LightGBM/tree/master/examples/regression">测试数据</a>。但是！！！LightGBM的分类和回归用的是同一个数据集，所以，实际上这个回归吧，参考意义也就那样吧。我用的是kaggle上面的pokemon数据集来做回归。</p><p>anyway，有人可以发现为什么weight那里会那样的话，给条明路。：)</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;XGBoost是GBDT的一个超级加强版，用了很久，一直没细看原理。围观了一波人家的实现，自己也来弄一遍。以后面试上来一句，要不要我现场写个XGBoost，听上去是不是很霸气。&lt;/p&gt;
&lt;p&gt;在开源代码的基础上进行了一点修改，大概跑通了，但是有些地方感觉有点诡异。后面会讲。&lt;/p&gt;</summary>
    
    
    
    <category term="统计学习" scheme="https://samaelchen.github.io/categories/StatisticalLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>一个挂逼</title>
    <link href="https://samaelchen.github.io/guabi/"/>
    <id>https://samaelchen.github.io/guabi/</id>
    <published>2018-10-02T16:00:00.000Z</published>
    <updated>2021-08-31T16:15:25.578Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>挂逼是形容深圳三和人才市场里面一些生活极其困难的打工者，有时候也代表这个人死了。</p><span id="more"></span><h1 id="三和大神">三和大神</h1><p>这是我实在无所事事的时候偶然发现的一个神奇存在，在看到三和大神之前，你永远不会相信，有人能够这样活着。当然，在看三和大神之前，我也不会想到，自己也就是个三和大神。</p>关于三和大神的一段描述：<p align="center"><img data-src='https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/blog_0001.png'></p><p>反观自己的标准，惊讶地发现，似乎也没什么差别。996的不要，大饼画得太大的不要，考勤严格的不要，没有奖金的不要，活干不完的不要……</p>三和大神的人物画像：<p align="center"><img data-src='https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/blog_0002.png'></p><p>再反观自己，似乎也并没有什么两样。喜欢算的工资是时薪；下班就是看片游戏；吃饭叫个外卖；能找到跟三四个人挤在一起的自如就谢天谢地，还在乎甲醛？……</p><p>除了物质层面的差别，在精神上，可能早就是一个三和大神。对，自己就是个每天洗澡的挂逼。</p><h1 id="小聚">小聚</h1><p>回乡同学婚礼小聚，谁能想到，时间是这个世界上最神奇的东西。丧几乎快成为我的个人代名词，但是我不能懂的是，为什么有人能不丧。在他们身上，我看到一种非常奇怪的感觉。可能是他们身上不同于日常见到的乐观积极，这种只是为了过日子的积极态度，让我充满了怀疑。当然，也可能是挂逼对所有积极生活的人都充满了怀疑。</p><p>那些在一线城市的互联网民工们，可能你们在为了改进APP图标的几个像素点能够带来多大的ROI而兴奋不已，但是，这些事情对自己的意义又在哪里。为了升职加薪？还是所谓的为了改变世界？</p><p>不知道是在魔都呆久了的原因，还是真的被洗脑了。我曾经觉得我还年轻，但是当一些只比自己大两三岁的同学，以一种在魔都被嘲讽的中年人身姿出现在我面前的时候，突然一阵恍惚。</p><p>我很难描述这次小聚的感受。我看着他们在KTV里面嘶吼，我总是会想起来三和大神200舞。如果不论生活质量，活着，到底意味着什么？</p><h1 id="想上岸的挂逼">想上岸的挂逼</h1><p>现实是，你的热血是可以被利用的，你的未来也可以被拿来成为他们的垫脚石。你以为，他们款款深情让你将所有托付给他们的以后能够让你为荣耀而活。但是你马上发现，自己的体力、脑力、技能、资源、能力都与这一切都毫无关系，你在他们的棋局里面已经被淘汰了。</p><p>非常感谢这段经历，我终于也成为众多三和大神中的一员。它教会了我，其实他们所说的未来一直都与你无关，那是他们的未来。当然，正因为他们的未来与自己无关，所以自己也不必有什么负罪感。</p><p>在三和，不是每个人都甘心成为挂逼，开始在快手直播的200舞，也许某一天他也就火了，然后也就上岸了。</p><p>时间有限，一生可以用来上岸的的时间其实没有多久，所以，虽然现实很丧，但是不要因为一时被人利用就做挂逼。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;挂逼是形容深圳三和人才市场里面一些生活极其困难的打工者，有时候也代表这个人死了。&lt;/p&gt;</summary>
    
    
    
    <category term="碎碎念" scheme="https://samaelchen.github.io/categories/self-questioning/"/>
    
    
  </entry>
  
  <entry>
    <title>Char-RNN生成古诗</title>
    <link href="https://samaelchen.github.io/pytorch-char-rnn/"/>
    <id>https://samaelchen.github.io/pytorch-char-rnn/</id>
    <published>2018-09-27T16:00:00.000Z</published>
    <updated>2021-08-31T16:15:25.581Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>尝试用char-RNN生成古诗，本来是想要尝试用来生成广告文案的，测试一波生成古诗的效果。嘛，虽然我对业务兴趣不大，不过这个模型居然把我硬盘跑挂了，也是醉。</p><span id="more"></span><p>其实Char-RNN来生成文本的逻辑非常简单，就是一个字一个字放进去，让RNN开始学，按照前面的字预测下面的字。所以就要想办法把文本揉成我们需要的格式。</p><p>比如说，我们现在有一句诗“床前明月光，疑是地上霜”。那么我们的输入就是“床前明月光”，那么我们的预测就是“前明月光，”，其实就是错位一位。</p><p>然后我们要考虑的是如何批量的把数据喂进去，这里参考了<a href="http://zh.gluon.ai/chapter_recurrent-neural-networks/lang-model-dataset.html">gluon的教程</a>上面的一个操作，因为诗歌是有上下文联系的，如果我们用随机选取的话，很可能就会丢掉很多有用的信息，所以我们还要想办法将诗歌的这种连续性保留下来。</p><p>mxnet教程的方法是先将所有的文本串成一行。所有的换行符替换为空格，所以空格在这里起到了分段的作用，空格也就有了意义。然后我们因为我们要批量训练，所以先按照我们每批打算训练多少行文本，将这一个超长的文本截断成这样，然后按照我们一次想看多少个字的窗口扫描过去。代码实现上如下： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_consecutive</span>(<span class="params">corpus_indices, batch_size, num_steps</span>):</span></span><br><span class="line">    corpus_indices = torch.tensor(corpus_indices)</span><br><span class="line">    data_len = <span class="built_in">len</span>(corpus_indices)</span><br><span class="line">    batch_len = data_len // batch_size</span><br><span class="line">    indices = corpus_indices[<span class="number">0</span>: batch_size*batch_len].reshape((</span><br><span class="line">        batch_size, batch_len))</span><br><span class="line">    epoch_size = (batch_len - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch_size):</span><br><span class="line">        i = i * num_steps</span><br><span class="line">        X = indices[:, i: i + num_steps]</span><br><span class="line">        Y = indices[:, i + <span class="number">1</span>: i + num_steps + <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">yield</span> X, Y</span><br></pre></td></tr></table></figure></p><p>这样有一个好处就是可以保持诗句的连续性，效果上大概是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 所有诗句拼成一行</span></span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>， <span class="number">10</span>， <span class="number">11</span>， <span class="number">12</span>]</span><br><span class="line"><span class="comment"># batch_size = 2, num_steps = 3</span></span><br><span class="line"><span class="comment"># batch 1</span></span><br><span class="line">[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]</span><br><span class="line"><span class="comment"># batch 2</span></span><br><span class="line">[[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]]</span><br></pre></td></tr></table></figure><p>这样一来，一句诗[1, 2, 3, 4, 5, 6]就能在不同batch里面保持连贯性了。</p><p>然后就是很简单设计网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">lyricNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_dim, embed_dim, num_layers, weight,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_labels, bidirectional, dropout=<span class="number">0.5</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(lyricNet, self).__init__(**kwargs)</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.num_labels = num_labels</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        <span class="keyword">if</span> num_layers &lt;= <span class="number">1</span>:</span><br><span class="line">            self.dropout = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.dropout = dropout</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(weight)</span><br><span class="line">        self.embedding.weight.requires_grad = <span class="literal">False</span></span><br><span class="line"><span class="comment">#         self.embedding = nn.Embedding(num_labels, self.embed_dim)</span></span><br><span class="line">        self.rnn = nn.GRU(input_size=self.embed_dim, hidden_size=self.hidden_dim,</span><br><span class="line">                          num_layers=self.num_layers, bidirectional=self.bidirectional,</span><br><span class="line">                          dropout=self.dropout)</span><br><span class="line">        <span class="keyword">if</span> self.bidirectional:</span><br><span class="line">            self.decoder = nn.Linear(hidden_dim * <span class="number">2</span>, self.num_labels)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.decoder = nn.Linear(hidden_dim, self.num_labels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs, hidden=<span class="literal">None</span></span>):</span></span><br><span class="line">        embeddings = self.embedding(inputs)</span><br><span class="line">        states, hidden = self.rnn(embeddings.permute([<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]), hidden)</span><br><span class="line">        outputs = self.decoder(states.reshape((-<span class="number">1</span>, states.shape[-<span class="number">1</span>])))</span><br><span class="line">        <span class="keyword">return</span>(outputs, hidden)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span>(<span class="params">self, num_layers, batch_size, hidden_dim, **kwargs</span>):</span></span><br><span class="line">        hidden = torch.zeros(num_layers, batch_size, hidden_dim)</span><br><span class="line">        <span class="keyword">return</span> hidden</span><br></pre></td></tr></table></figure><p>这里我用的是很简单的one-hot做词向量，当然数据量大一点可以考虑pretrained的字向量。不过直观感受上用白话文训练的字向量应该效果不会太好吧。</p><p>接着就可以开始训练了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epoch):</span><br><span class="line">    start = time.time()</span><br><span class="line">    num, total_loss = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    data = data_iter_consecutive(corpus_indice, batch_size, <span class="number">35</span>)</span><br><span class="line">    hidden = model.init_hidden(num_layers, batch_size, hidden_dim)</span><br><span class="line">    <span class="keyword">for</span> X, Y <span class="keyword">in</span> data:</span><br><span class="line">        num += <span class="number">1</span></span><br><span class="line">        hidden.detach_()</span><br><span class="line">        <span class="keyword">if</span> use_gpu:</span><br><span class="line">            X = X.to(device)</span><br><span class="line">            Y = Y.to(device)</span><br><span class="line">            hidden = hidden.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output, hidden = model(X, hidden)</span><br><span class="line">        l = loss_function(output, Y.t().reshape((-<span class="number">1</span>,)))</span><br><span class="line">        l.backward()</span><br><span class="line">        norm = nn.utils.clip_grad_norm_(model.parameters(), <span class="number">1e-2</span>)</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += l.item()</span><br><span class="line">    end = time.time()</span><br><span class="line">    s = end - since</span><br><span class="line">    h = math.floor(s / <span class="number">3600</span>)</span><br><span class="line">    m = s - h * <span class="number">3600</span></span><br><span class="line">    m = math.floor(m / <span class="number">60</span>)</span><br><span class="line">    s -= m * <span class="number">60</span></span><br><span class="line">    <span class="keyword">if</span> (epoch % <span class="number">10</span> == <span class="number">0</span>) <span class="keyword">or</span> (epoch == (num_epoch - <span class="number">1</span>)):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch %d/%d, loss %.4f, norm %.4f, time %.3fs, since %dh %dm %ds&#x27;</span></span><br><span class="line">              %(epoch+<span class="number">1</span>, num_epoch, total_loss / num, norm, end-start, h, m, s))</span><br></pre></td></tr></table></figure><p>这里的训练过程需要注意两个点，一个是hidden的initial，因为我们想要保持句子的连续性，所以我们hidden的initial只要每个epoch的第一次initial一下就可以了，后面训练的过程中需要从计算图中拿掉。另外就是因为有梯度爆炸的问题，所以我们需要对梯度进行修剪。</p><p>最后一个是我自己最容易犯错的地方，死活记不住的就是RNN的输入输出每个dimension都代表了什么含义。原始的RNN接受的输入是(seq_len, batch_size, embedding_dimension)，输出的是(seq_len, batch_size, num_direction * hidden_dim)。所以我们习惯的batch在先的数据需要在这里做一个permute，将batch和seq做一下调换。然后就是我们做分类的时候，直接flatten成为一个长向量的时候，其实已经变成了[seq_len, seq_len, ...]这样的样子。简单理解就是本来我们都是横着看诗歌的，现在模型的输出是竖着输出的。所以我们后面算loss的时候，y也需要做一个转置再flatten。</p><p>具体的可以看我的这个<a href="&#39;https://github.com/SamaelChen/hexo-practice-code/blob/master/pytorch/text%20generater/generate%20poem.ipynb&#39;">notebook</a>。</p><p>接下来可能想试一下的是如果不用这种方法的话，是不是可以用padding的方法把句子长度统一再训练。</p><p>另外强势推荐<a href="https://github.com/chinese-poetry/chinese-poetry">最全中华古诗词数据库</a>。数据非常非常全了。</p><p>后面如果要做到很好的效果可以做的方向一个是做韵脚的信息，还有就是平仄的信息也带进去。</p><p>anyway，想了一下，这样训练完的hidden是不是就包含了一个作者的文风信息？！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;尝试用char-RNN生成古诗，本来是想要尝试用来生成广告文案的，测试一波生成古诗的效果。嘛，虽然我对业务兴趣不大，不过这个模型居然把我硬盘跑挂了，也是醉。&lt;/p&gt;</summary>
    
    
    
    <category term="PyTorch" scheme="https://samaelchen.github.io/categories/PyTorch/"/>
    
    <category term="深度学习" scheme="https://samaelchen.github.io/categories/DeepLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>日常的丧</title>
    <link href="https://samaelchen.github.io/depressed/"/>
    <id>https://samaelchen.github.io/depressed/</id>
    <published>2018-09-20T16:00:00.000Z</published>
    <updated>2021-08-31T16:15:25.577Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>日常很丧的各种不开心，小确丧。</p><span id="more"></span><h1 id="section">2018-09-21</h1><p>一直在试char-rnn生成，可能notebook硬盘io频繁了一点，终于把工作站的硬盘搞到写保护了。现在整个硬盘全是坏道。情绪稳定。</p><p>顺便，Linux检查硬盘坏道的方法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo badblocks -s -v /dev/sdXX &gt; badblocks.txt</span><br></pre></td></tr></table></figure><p>-s可以显示检查进度，不过一般显示进度的话实际检查速度貌似会变慢。</p><p>然后可以用recovery模式去修复一下，fsck -a /dev/sdXX。运气好是逻辑坏道的话能修复好，如果跟我一样修复好一会儿会儿就又写保护了，估计十有八九是物理坏道。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;日常很丧的各种不开心，小确丧。&lt;/p&gt;</summary>
    
    
    
    <category term="今天份的不开心" scheme="https://samaelchen.github.io/categories/wtf/"/>
    
    
  </entry>
  
  <entry>
    <title>台大李宏毅深度学习——seq2seq</title>
    <link href="https://samaelchen.github.io/deep_learning_step6/"/>
    <id>https://samaelchen.github.io/deep_learning_step6/</id>
    <published>2018-09-05T16:00:00.000Z</published>
    <updated>2021-08-31T16:15:25.577Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>这节课的内容讲的有点浅，所以我看到是李沐的gluon教程，配合这节课的内容。</p><span id="more"></span><h1 id="seq2seq">Seq2Seq</h1><p>这个是encode-decode的过程。之前写的LSTM做文档分类是限定了输入的长度。超出规定长度的句子我们是截断，没达到长度的我们是padding。但是用seq2seq可以接受不定长的输入和不定长的输出。</p>实际上seq2seq是有两个循环神经网络，一个处理输入序列，另一个处理输出序列。处理输入序列的叫编码器，处理输出序列的叫解码器。流程上如下图：<p align="center"><img data-src='https://i.loli.net/2018/09/06/5b90c7ddec62a.png'></p><h2 id="encoder">encoder</h2><p>编码器是将一个不定长的输入序列变换成一个定长的背景向量<span class="math inline">\(c\)</span>。根据不一样的任务，编码器可以是不一样的网络。例如在对话系统或者机器翻译的场景下，我们用的编码器可以是LSTM，如果在caption的场景下，CNN就是编码器。</p><p>现在假设我们做一个机器翻译的任务，那么有一句话可以拆成<span class="math inline">\(x_1, \dots, x_T\)</span>个词的序列。下一个时刻的隐藏状态可以表示为<span class="math inline">\(h_t = f(x_t, h_{t-1})\)</span>。<span class="math inline">\(f\)</span>是循环网络隐藏层的变换函数。</p><p>然后我们定义一个函数<span class="math inline">\(q\)</span>将每个时间步的隐藏状态变成背景向量：<span class="math inline">\(c=q(h_1, \dots, h_T)\)</span>。</p><h2 id="decoder">decoder</h2><p>之前的编码器将整个输入序列的信息编码成了背景向量<span class="math inline">\(c\)</span>。而解码器就是根据背景信息输出序列<span class="math inline">\(y_1, y_2, \dots, y_{T&#39;}\)</span>。解码器每一步的输出要基于上一步的输出和背景向量，所以表示为<span class="math inline">\(P(y_{t&#39;}|y_1, \dots, y_{t&#39;-1}, c)\)</span>。</p><p>像机器翻译的时候，我们的解码器也会是一个循环网络。我们用<span class="math inline">\(g\)</span>表示这个循环网络的函数，那么当前步的隐藏状态<span class="math inline">\(s_{t&#39;}=g(y_{t&#39;-1}, c, s_{t&#39;-1})\)</span>。然后我就可以自定义一个输出层来计算输出序列的概率分布。</p><h2 id="损失函数">损失函数</h2><p>一般而言，会用最大似然法来最大化输出序列基于输入序列的条件概率： <span class="math display">\[\begin{split}\begin{aligned}\mathbb{P}(y_1, \ldots, y_{T&#39;} \mid x_1, \ldots, x_T)&amp;= \prod_{t&#39;=1}^{T&#39;} \mathbb{P}(y_{t&#39;} \mid y_1, \ldots, y_{t&#39;-1}, x_1, \ldots, x_T)\\&amp;= \prod_{t&#39;=1}^{T&#39;} \mathbb{P}(y_{t&#39;} \mid y_1, \ldots, y_{t&#39;-1}, \boldsymbol{c}),\end{aligned}\end{split}\]</span></p><p>因此损失函数可以表示为： <span class="math display">\[- \log\mathbb{P}(y_1, \ldots, y_{T&#39;} \mid x_1, \ldots, x_T) = -\sum_{t&#39;=1}^{T&#39;} \log \mathbb{P}(y_{t&#39;} \mid y_1, \ldots, y_{t&#39;-1}, \boldsymbol{c})\]</span></p><h1 id="beam-search">beam search</h1><p>通常情况下，我们会在输入和输出序列前后分别加一个特殊符号'&lt;bos&gt;'和'&lt;eos&gt;'，分别表示句子的开始和结束。不过很多时候好像'&lt;bos&gt;'不是必须加的，虽然我觉得不加很奇怪。</p><p>假设我们输出一段文本序列，那么输出辞典<span class="math inline">\(\mathcal{Y}\)</span>，大小为<span class="math inline">\(|\mathcal{Y}|\)</span>，输出的序列长度为<span class="math inline">\(T&#39;\)</span>，那么我们一共有<span class="math inline">\(|\mathcal{Y}|^{T&#39;}\)</span>种可能。</p><p>那么如果按照穷举检索，我们要评估的序列数量就是全部的可能性。假设我们有10000个词，输出长度为10的序列，那么我们的可能性就是<span class="math inline">\(10000^{10}\)</span>这么多种可能性。这几乎是不可能评估完的。</p><p>那么换个思路，如果每一次我们都只拿概率最高的那一个词，也就是说每一次拿的是<span class="math inline">\(y_{t&#39;} = \arg\max_{y_{t&#39;} \in \mathcal{Y}} P(y_{t&#39;}|y_1, \dots, y_{t&#39;-1}, c)\)</span>。只要遇到'&lt;eos&gt;'就停止检索。这就是一个非常典型的贪婪算法。这样的话我们的计算开销会显著下降。</p>但是贪婪算法会有典型的问题，就是检索空间太小，无法保证最优解。比如下图：<p align="center"><img data-src='https://i.loli.net/2018/09/06/5b90ebf798140.png'></p><p>这里的数字表示每一个state，ABC表示每一个词。中间的数字是条件概率，比如B2这里的0.4表示在<span class="math inline">\(P(B|A)\)</span>，而A2就是表示<span class="math inline">\(P(A|A)\)</span>。如果我们按照贪婪算法的话，我们会得到的结果是ABC，那么概率是<span class="math inline">\(0.5 \times 0.4 \times 0.2 \times 0.6\)</span>，而如果不是贪婪算法的话，我们得到ACB，概率是<span class="math inline">\(0.5 \times 0.3 \times 0.6 \times 0.6\)</span>明显概率更大。</p><p>所以我们为了保证有更大的概率可以检索到较多的可能性，我们可以采用束搜索的方法，也就是说，我们每一次不再只看概率最高的那一个词，而是看概率最高的数个词。我们用束宽（beam size）<span class="math inline">\(k\)</span>来表示。之后根据<span class="math inline">\(k\)</span>个候选词输出下一个阶段的序列，接着再选出概率最高的<span class="math inline">\(k\)</span>个序列，不断重复这件事情。最后我们会在各个状态的候选序列中筛选出包含特殊符号'&lt;eos&gt;'的序列，并将这个符号后的子序列舍弃，得到最后的输出序列。然后再在这些序列中选择分数最高的作为最后的输出序列： <span class="math display">\[\frac{1}{L^\alpha} \log \mathbb{P}(y_1, \ldots, y_{L}) = \frac{1}{L^\alpha} \sum_{t&#39;=1}^L \log \mathbb{P}(y_{t&#39;} \mid y_1, \ldots, y_{t&#39;-1}, \boldsymbol{c}),\]</span> 其中<span class="math inline">\(L\)</span>是最终序列的长度，<span class="math inline">\(\alpha\)</span>一般选0.75。这<span class="math inline">\(L\)</span>的系数起到的作用是惩罚太长的序列得分过高的情况。</p>事实上，贪婪搜索可以看做是beam size为1的束搜索。过程上就像下图：<p align="center"><img data-src='https://i.loli.net/2018/09/06/5b90f03697a3c.png'></p><p>那么不同于贪婪搜索，束搜索其实并不知道什么时候停下来，所以一般来说要定义一个最长的输出序列长度。</p><h1 id="attention">Attention</h1><p>前面说的解码器是将编码器的整个序列都作为背景来学习。那比如说机器翻译里面，我们翻译的时候其实可能没必要全部都看一遍，只要看一部分，然后就可以将这部分翻译出来。比如说“机器学习”翻译为“machine learning”，“机器”对应的是“machine”，而“学习”是“learning”，所以翻译machine的时候只要关注机器就可以了。</p>其实所谓的关注点，如果用数据来表示也就是权重大小，关注度越高权重越高。如下图：<p align="center"><img data-src='https://i.loli.net/2018/09/06/5b90fb86eac3c.png' width=70%></p><p>我们在输出背景向量的时候做一个softmax，然后每一个state给一个权重，作为<span class="math inline">\(t&#39;\)</span>时刻的输入，这样jointly训练就可以学出一个attention的形式。</p>那么这里的<span class="math inline">\(\alpha\)</span>是这样计算出来的：<p align="center"><img data-src='https://i.loli.net/2018/09/06/5b90fd0f9519e.png' width=70%></p><p>其实就是每一个state的decoder的input拿来和encoder的hidden做一个match。至于match的函数可以自己随意定义。</p><p>这样一来，我们就可以让解码器在不同的state的时候关注输入序列的不同部分。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;这节课的内容讲的有点浅，所以我看到是李沐的gluon教程，配合这节课的内容。&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://samaelchen.github.io/categories/DeepLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>台大李宏毅深度学习——Batch normalization &amp; SELU</title>
    <link href="https://samaelchen.github.io/deep_learning_step5/"/>
    <id>https://samaelchen.github.io/deep_learning_step5/</id>
    <published>2018-09-02T16:00:00.000Z</published>
    <updated>2021-08-31T16:15:25.577Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>学习一下batch normalization和SELU，顺便看点深度学习的八卦。</p><span id="more"></span><h1 id="bn">BN</h1><p>Batch normalization是一个比较新的深度学习技巧，但是在深度学习的实作中有非常迅速成为中流砥柱。</p><p>normalization是以前统计学习比较常用的一种方法，因为对于损失函数而言，<span class="math inline">\(L(y, \hat{y})\)</span>会受到输入数据的影响。这个其实是非常直观的，比如说一个数据有两个维度，一个维度都是1-10的范围内波动的，另一个维度是1000-10000之间波动的，那么如果<span class="math inline">\(y=x_1 + x_2\)</span>很明显后一个维度的数据对<span class="math inline">\(y\)</span>的影响非常大。</p><p>那么在这种情况下，我们做梯度下降，在scale大的维度上梯度就比较大，但是在scale小的地方梯度就比较小。这个在我之前学<a href="‘https://samaelchen.github.io/machine_learning_step3/’">梯度下降的博客</a>里面也有。大概图形上看就是下面这样：</p><p><img data-src='https://i.imgur.com/mb0vi91.png'></p><p>那这样我们在不同维度上的梯度下降步长是不一样的。所以在统计学习或者传统的机器学习里面，为了加快收敛的速度，虽然用二阶导可以解决，但是一般用feature scaling就可以了。</p><p>而batch normalization其实也是使用了这样的理念。一般而言，我们做normalization就是<span class="math inline">\(\frac{x-\mu}{\sigma}\)</span>，那batch normalization其实就是在每一个layer的input前做这么一下操作。</p><p>那batch normalization和normalization的差别其实就在于batch这个地方。我们知道平时我们训练深度学习网络的时候避免炸内存，会将数据分批导进去训练，在这种情况下，我们其实是没有办法得到全局的<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\sigma\)</span>的。所以事实上，batch normalization每一次算的都是一个batch的<span class="math inline">\(\mu \ \&amp; \ \sigma\)</span>。</p>那整个流程看上去就是下图这样的：<p align="center"><img data-src='https://i.loli.net/2018/09/04/5b8e3c359e425.png' width=70%></p><p>那实际上可以将这个过程看作是一个hidden layer来处理。</p>如果说觉得这样全部normalization到0，1这样的形式可能有些activation function效果不好，所以我们可以考虑一下再加一层linear layer来转换一下，那流程上就是：<p align="center"><img data-src='https://i.loli.net/2018/09/04/5b8e3e03c9485.png' width=70%></p><p>当然，如果好巧不巧，机器学着学着，刚好<span class="math inline">\(\beta\)</span>和<span class="math inline">\(\gamma\)</span>跟前面的一样，那么这轮的batch normalization就白做了。不过一般来说不会这么巧。</p><p>那么在训练过程中，我们一般都是一个batch一个batch喂进去，但是test的时候，我们一般是一口气全部过模型一遍，那么我们并没有办法得到一个合适的<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\sigma\)</span>。那么一种解决方法是计算一下全部training set的均值和标准差，另一种方法是，每次训练后，我们都保留最后一个batch的均值和标准差。</p><p>BN的好处非常显而易见，一个是可以减少covariate shift。也就是说，以前为了避免每个layer的方差太大，我们会减小步长，但是用了BN以后就可以用大的步长加速训练。此外，对于sigmoid或者tanh这样的激活函数来说，可以有效减少深层网络的梯度爆炸或者消失的问题。另外BN的一个副产物是可以减少过拟合。</p><h1 id="selu">SELU</h1><p>ReLu是一种比较特殊的激活函数，本身是为了解决sigmoid在叠加多层后会出现梯度消失的问题。ReLu的函数其实非常简单，就是： <span class="math display">\[a =\begin{cases}0, &amp;\mbox{if }z&lt;0 \\z, &amp;\mbox{if }z&gt;0\end{cases}\]</span> 不过现在回过头看ReLu，其实某种程度上效果很像是dropout？！</p><p>但是ReLu相对来说还是比较激进的，所以后来有各种各样的变种，比如说Leaky ReLu，就是： <span class="math display">\[a =\begin{cases}0.01z, &amp;\mbox{if }z&lt;0 \\z, &amp;\mbox{if }z&gt;0\end{cases}\]</span> 还有parametric ReLu： <span class="math display">\[a =\begin{cases}\alpha z, &amp;\mbox{if }z&lt;0 \\z, &amp;\mbox{if }z&gt;0\end{cases}\]</span></p><p>再后来在竞赛中还有人提出了randomized relu，其实就是上面的parametric relu的<span class="math inline">\(\alpha\)</span>每次训练的时候都随机生成一个，而不是让机器去学习，然后test的时候再固定一个就可以了。据说效果还不错。</p><p>但是这种形式的ReLu都是负无穷到正无穷的值域，于是又有人修正为ELU（exponential linear unit），函数是： <span class="math display">\[a =\begin{cases}\alpha(e^z - 1), &amp;\mbox{if }z&lt;0 \\z, &amp;\mbox{if }z&gt;0\end{cases}\]</span> 这样一来，ELU的值域就是<span class="math inline">\(\alpha\)</span>到正无穷。</p><p>之后横空出世了一个SELU，其实就是ELU前面乘了一个参数<span class="math inline">\(\lambda\)</span>，函数表示为： <span class="math display">\[a =\lambda \begin{cases}\alpha(e^z - 1), &amp; \mbox{if }z&lt;0 \\z, &amp; \mbox{if }z&gt;0\end{cases}\]</span> 不过，这里的两个参数是有确定值的，而不是随便学习出来的。这里<span class="math inline">\(\alpha=1.6732632423543772848170429916717\)</span>，<span class="math inline">\(\lambda=1.0507009873554804934193349852946\)</span>。</p><p>这两个非常神奇的数据说是可以推导出来的，有兴趣的同学可以去看一下原文93页的证明。看不下去的可以看一下作者放出来的<a href="https://github.com/bioinf-jku/SNNs">源码</a>。</p><p>那么为什么要定这样两个实数，其实目的是保证每次的layer吐出来的都是一个标志正态分布的数据。</p><h1 id="花式调参">花式调参</h1><p>最后是现在有的一些花式调参的方法。毕竟实作的时候基本上也就是调参了，菜如我这种也不可能提出什么突破性的方法。</p><p>深度学习说白了也就是机器学习的一种，所以传统机器学习中的grid search这种非常暴力的方法当然也适用。不过为了加速搜索，一般会用random search的方法，通常也不会太差。</p>另外现在有一些非常非常骚气的方法，一种就是learn to learn。其实就是用一个RNN去学习另一个网络的所有参数。看上去就是下图的样子：<p align="center"><img data-src='https://i.loli.net/2018/09/05/5b8f7f31490b4.png' width=70%></p><p>还有一个很重要的调参方向其实就是learning rate，因为深度学习很多时候是一个非凸优化的问题，所以我们以为loss下不去了可能待在了saddle point，实际上也可能是在一个local minimum的山谷里来回震荡。这种时候只要降低lr就可以继续收敛了。所以很多时候我们在训练的过程中，每50个epoch或者100个epoch就缩小一下lr，很多时候loss会出现一次很明显的降低。</p><p>最后是Google brain提出了一些非常神奇的激活函数，具体可以看看这篇<a href="https://arxiv.org/pdf/1710.05941.pdf">论文</a>。</p><h1 id="深度学习究竟有没有学到东西">深度学习究竟有没有学到东西</h1>这个其实是非常有意思的一个争论点。很多人质疑深度学习其实只是强行记忆了数据的特征，并没有学到潜在的规律。于是有人做了相关的研究，<a href="https://arxiv.org/pdf/1706.05394.pdf">A Closer Look at Memorization in Deep Networks</a>这篇论文就是相关的研究，里面有一个很有意思的地方就是对label加noise。不论加了多少noise，模型都可以train到一个百分百正确的地方。但是test上的表现很自然会变得很差。过程如下图：<p align="center"><img data-src='https://i.loli.net/2018/09/05/5b8f861ddf832.png' width=70%></p><p>这个其实是非常风骚的一个操作，就是说故意给一些错误的信息让机器去学习。这个图里面的实线是train，虚线是test，我们可以看到其实一开始test是上升的，然后才下降。所以实际上一开始模型还是正常学到了一些正确的规律的。但是后面就被噪声带跑偏了。</p><p>不过从某种程度上来说，传统的决策树不是更像是强行记住一些东西么。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;学习一下batch normalization和SELU，顺便看点深度学习的八卦。&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://samaelchen.github.io/categories/DeepLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>李宏毅深度学习作业——language model</title>
    <link href="https://samaelchen.github.io/pytorch_cloze/"/>
    <id>https://samaelchen.github.io/pytorch_cloze/</id>
    <published>2018-08-27T16:00:00.000Z</published>
    <updated>2021-08-31T16:15:25.581Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>之前用LSTM做过情感分析，李宏毅老师17年的课程第一个大作业是做一个完形填空的language model，试着做了一个简单的demo。 <span id="more"></span></p><p>做完型填空其实很直观，就是跟CBOW很像，我们按照上下文猜被挖掉的那个词是什么。</p><p>这次用的还是之前训词向量的语料库，因为那个都是小说原文，所以我们要把数据揉成我们想要的形式，也就是context包含上下文，中间空掉的词是我们的target。</p><p>然后因为要训练LSTM，所以我们会再做一个padding的工作，最后看起来大概会是这样的： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ <span class="number">9405</span>,  <span class="number">1236</span>,  <span class="number">6282</span>,   <span class="number">371</span>,  <span class="number">1968</span>,     <span class="number">0</span>,     <span class="number">0</span>,     <span class="number">0</span>,     <span class="number">0</span>,     <span class="number">0</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">6085</span>, <span class="number">10586</span>,   <span class="number">900</span>,  <span class="number">7561</span>,     <span class="number">0</span>,     <span class="number">0</span>,     <span class="number">0</span>,     <span class="number">0</span>,     <span class="number">0</span>,     <span class="number">0</span>]]])</span><br></pre></td></tr></table></figure></p><p>形式上是<span class="math inline">\(2 \times \text{batch_size} \times \text{seq_len}\)</span>。</p><p>网络的设置非常简单，前半部分过一个LSTM，后半部分过一个LSTM，然后将这两个网络的output拼到一起最后过一个fc。</p><p>这里因为有可能完形填空的时候空的是第一个词或者是最后一个词，所以我们会在句子开头和结尾加上<bos>和<eos>的标志。</p><p>一个示例可以看这个<a href="‘https://github.com/SamaelChen/hexo-practice-code/blob/master/pytorch/langage%20model/LSTM-Full-text-Copy1.ipynb’">notebook</a>。</p><p>这个notebook的脚本没啥通用性，一个是其实没有解决unknown的词的问题，另外是没有解决训练效率的问题。PyTorch没有nce_loss或者是negative sampling这样的loss function，所以后面用softmax做cross entropy的时候复杂度是O(vocab_size)。之前写的negative sampling是针对word2vec写的，所以没什么通用性，看了其他人写的通用性的nce或者negative sampling，总感觉哪里怪怪的。后面还是要考虑自己实现一个。有点烦(╯﹏╰)。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;之前用LSTM做过情感分析，李宏毅老师17年的课程第一个大作业是做一个完形填空的language model，试着做了一个简单的demo。</summary>
    
    
    
    <category term="PyTorch" scheme="https://samaelchen.github.io/categories/PyTorch/"/>
    
    <category term="深度学习" scheme="https://samaelchen.github.io/categories/DeepLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>PyTorch实现LSTM情感分析</title>
    <link href="https://samaelchen.github.io/pytorch_lstm_sentiment/"/>
    <id>https://samaelchen.github.io/pytorch_lstm_sentiment/</id>
    <published>2018-08-14T16:00:00.000Z</published>
    <updated>2021-08-31T16:15:25.581Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>2018.08.16更新一个textCNN。</p><p>尝试使用LSTM做情感分析，这个gluon有非常详细的例子，可以直接参考gluon的<a href="http://zh.gluon.ai/chapter_natural-language-processing/sentiment-analysis.html">官方教程</a>。这里尝试使用PyTorch复现一个。数据用的是IMDB的数据<a href="http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz">http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz</a></p><span id="more"></span><p>首先我们导入相关的package：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.utils.data.dataloader <span class="keyword">as</span> dataloader</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.autograd <span class="keyword">as</span> autograd</span><br><span class="line"><span class="keyword">import</span> torchtext.vocab <span class="keyword">as</span> torchvocab</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> gensim</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> snowballstemmer</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> chain</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br></pre></td></tr></table></figure><p>然后我们定义读数的函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readIMDB</span>(<span class="params">path, seg=<span class="string">&#x27;train&#x27;</span></span>):</span></span><br><span class="line">    pos_or_neg = [<span class="string">&#x27;pos&#x27;</span>, <span class="string">&#x27;neg&#x27;</span>]</span><br><span class="line">    data = []</span><br><span class="line">    <span class="keyword">for</span> label <span class="keyword">in</span> pos_or_neg:</span><br><span class="line">        files = os.listdir(os.path.join(path, seg, label))</span><br><span class="line">        <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(path, seg, label, file), <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf8&#x27;</span>) <span class="keyword">as</span> rf:</span><br><span class="line">                review = rf.read().replace(<span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">                <span class="keyword">if</span> label == <span class="string">&#x27;pos&#x27;</span>:</span><br><span class="line">                    data.append([review, <span class="number">1</span>])</span><br><span class="line">                <span class="keyword">elif</span> label == <span class="string">&#x27;neg&#x27;</span>:</span><br><span class="line">                    data.append([review, <span class="number">0</span>])</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">train_data = readIMDB(<span class="string">&#x27;aclImdb&#x27;</span>)</span><br><span class="line">test_data = readIMDB(<span class="string">&#x27;aclImdb&#x27;</span>, <span class="string">&#x27;test&#x27;</span>)</span><br></pre></td></tr></table></figure><p>接着是分词，这里只做非常简单的分词，也就是按照空格分词。当然按照一些传统的清洗方式效果会更好。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenizer</span>(<span class="params">text</span>):</span></span><br><span class="line">    <span class="keyword">return</span> [tok.lower() <span class="keyword">for</span> tok <span class="keyword">in</span> text.split(<span class="string">&#x27; &#x27;</span>)]</span><br><span class="line"></span><br><span class="line">train_tokenized = []</span><br><span class="line">test_tokenized = []</span><br><span class="line"><span class="keyword">for</span> review, score <span class="keyword">in</span> train_data:</span><br><span class="line">    train_tokenized.append(tokenizer(review))</span><br><span class="line"><span class="keyword">for</span> review, score <span class="keyword">in</span> test_data:</span><br><span class="line">    test_tokenized.append(tokenizer(review))</span><br><span class="line"></span><br><span class="line">vocab = <span class="built_in">set</span>(chain(*train_tokenized))</span><br><span class="line">vocab_size = <span class="built_in">len</span>(vocab)</span><br></pre></td></tr></table></figure><p>因为这个数据集非常小，所以如果我们用这个数据集做word embedding有可能过拟合，而且模型没有通用性，所以我们传入一个已经学好的word embedding。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">wvmodelwvmodel = gensim.models.KeyedVectors.load_word2vec_format(<span class="string">&#x27;test_word.txt&#x27;</span>,</span><br><span class="line">                                                          binary=<span class="literal">False</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br></pre></td></tr></table></figure><p>这里的“test_word.txt”是我将glove的词向量转换后的结果，当时测试gensim的这个功能瞎起的名字，用的是glove的6B，100维的预训练数据。</p><p>然后一样要定义一个word to index的词典：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">word_to_idxword_to  = &#123;word: i+<span class="number">1</span> <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)&#125;</span><br><span class="line">word_to_idx[<span class="string">&#x27;&lt;unk&gt;&#x27;</span>] = <span class="number">0</span></span><br><span class="line">idx_to_word = &#123;i+<span class="number">1</span>: word <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)&#125;</span><br><span class="line">idx_to_word[<span class="number">0</span>] = <span class="string">&#x27;&lt;unk&gt;&#x27;</span></span><br></pre></td></tr></table></figure><p>定义的目的是为了将预训练的weight跟我们的词库拼上。另外我们定义了一个unknown的词，也就是说没有出现在训练集里的词，我们都叫做unknown，词向量就定义为0。</p><p>然后就是编码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode_samples</span>(<span class="params">tokenized_samples, vocab</span>):</span></span><br><span class="line">    features = []</span><br><span class="line">    <span class="keyword">for</span> sample <span class="keyword">in</span> tokenized_samples:</span><br><span class="line">        feature = []</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> sample:</span><br><span class="line">            <span class="keyword">if</span> token <span class="keyword">in</span> word_to_idx:</span><br><span class="line">                feature.append(word_to_idx[token])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                feature.append(<span class="number">0</span>)</span><br><span class="line">        features.append(feature)</span><br><span class="line">    <span class="keyword">return</span> features</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad_samples</span>(<span class="params">features, maxlen=<span class="number">500</span>, PAD=<span class="number">0</span></span>):</span></span><br><span class="line">    padded_features = []</span><br><span class="line">    <span class="keyword">for</span> feature <span class="keyword">in</span> features:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(feature) &gt;= maxlen:</span><br><span class="line">            padded_feature = feature[:maxlen]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            padded_feature = feature</span><br><span class="line">            <span class="keyword">while</span>(<span class="built_in">len</span>(padded_feature) &lt; maxlen):</span><br><span class="line">                padded_feature.append(PAD)</span><br><span class="line">        padded_features.append(padded_feature)</span><br><span class="line">    <span class="keyword">return</span> padded_features</span><br></pre></td></tr></table></figure><p>我们这里为了解决评论长度不一致的问题，将所有的评论都取500个词，超过的就取前500个，不足的补0。</p><p>整理一下训练数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train_features = torch.tensor(pad_samples(encode_samples(train_tokenized, vocab)))</span><br><span class="line">train_labels = torch.tensor([score <span class="keyword">for</span> _, score <span class="keyword">in</span> train_data])</span><br><span class="line">test_features = torch.tensor(pad_samples(encode_samples(test_tokenized, vocab)))</span><br><span class="line">test_labels = torch.tensor([score <span class="keyword">for</span> _, score <span class="keyword">in</span> test_data])</span><br></pre></td></tr></table></figure><p>然后就是定义网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SentimentNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="params"><span class="function">                 bidirectional, weight, labels, use_gpu, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SentimentNet, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens = num_hiddens</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.use_gpu = use_gpu</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(weight)</span><br><span class="line">        self.embedding.weight.requires_grad = <span class="literal">False</span></span><br><span class="line">        self.encoder = nn.LSTM(input_size=embed_size, hidden_size=self.num_hiddens,</span><br><span class="line">                               num_layers=num_layers, bidirectional=self.bidirectional,</span><br><span class="line">                               dropout=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> self.bidirectional:</span><br><span class="line">            self.decoder = nn.Linear(num_hiddens * <span class="number">4</span>, labels)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.decoder = nn.Linear(num_hiddens * <span class="number">2</span>, labels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        embeddings = self.embedding(inputs)</span><br><span class="line">        states, hidden = self.encoder(embeddings.permute([<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]))</span><br><span class="line">        encoding = torch.cat([states[<span class="number">0</span>], states[-<span class="number">1</span>]], dim=<span class="number">1</span>)</span><br><span class="line">        outputs = self.decoder(encoding)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><p>那这里需要注意几个点，第一，LSTM可以不initialize hidden，如果不initialize的话，那么PyTorch会默认初始为0。</p><p>另外就是LSTM这里传进去的数据格式是[seq_len, batch_size, embedded_size]。而我们传进去的数据是[batch_size, seq_len]的样子，那经过embedding之后的结果是[batch_size, seq_len, embedded_size]。所以我们这里要将第二个维度和第一个维度做个调换。而LSTM这边output的dimension和inputs是一致的，如果这里我们不做维度的调换，可以将LSTM的batch_first参数设置为True。然后我们要拿到每个batch的初始状态和最后状态还是一样要去做一个第一第二维度的调换。这里非常的绕，我在这里卡了好久(=<span class="citation" data-cites="__">@__</span>@=)</p><p>第三就是我这里用了最初始的状态和最后的状态拼起来作为分类的输入。</p><p>另外有一点吐槽的就是，MXNet的dense层比较强大啊，不用定义输入的维度，只要定义输出的维度就可以了，操作比较骚啊。</p><p>然后我们把weight导进来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">weight = torch.zeros(vocab_size+<span class="number">1</span>, embed_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(wvmodel.index2word)):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        index = word_to_idx[wvmodel.index2word[i]]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    weight[index, :] = torch.from_numpy(wvmodel.get_vector(</span><br><span class="line">        idx_to_word[word_to_idx[wvmodel.index2word[i]]]))</span><br></pre></td></tr></table></figure><p>这里我们将不在glove里面的词全部填为0，后面想了一下，其实也可以试试这些全部随机试试。</p><p>接着定义参数就可以训练了。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">embed_size = <span class="number">100</span></span><br><span class="line">num_hiddens = <span class="number">100</span></span><br><span class="line">num_layers = <span class="number">2</span></span><br><span class="line">bidirectional = <span class="literal">True</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">labels = <span class="number">2</span></span><br><span class="line">lr = <span class="number">0.8</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">use_gpu = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">net = SentimentNet(vocab_size=(vocab_size+<span class="number">1</span>), embed_size=embed_size,</span><br><span class="line">                   num_hiddens=num_hiddens, num_layers=num_layers,</span><br><span class="line">                   bidirectional=bidirectional, weight=weight,</span><br><span class="line">                   labels=labels, use_gpu=use_gpu)</span><br><span class="line">net.to(device)</span><br><span class="line">loss_function = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=lr)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">train_set = torch.utils.data.TensorDataset(train_features, train_labels)</span><br><span class="line">test_set = torch.utils.data.TensorDataset(test_features, test_labels)</span><br><span class="line"></span><br><span class="line">train_iter = torch.utils.data.DataLoader(train_set, batch_size=batch_size,</span><br><span class="line">                                         shuffle=<span class="literal">True</span>)</span><br><span class="line">test_iter = torch.utils.data.DataLoader(test_set, batch_size=batch_size,</span><br><span class="line">                                        shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><p>这个位置需要注意的是，我们在train加了一个shuffle，如果不加shuffle的话，模型会学到奇奇怪怪的地方去。</p><p>最后训练一下就好了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    start = time.time()</span><br><span class="line">    train_loss, test_losses = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    train_acc, test_acc = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    n, m = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> feature, label <span class="keyword">in</span> train_iter:</span><br><span class="line">        n += <span class="number">1</span></span><br><span class="line">        net.zero_grad()</span><br><span class="line">        feature = Variable(feature.cuda())</span><br><span class="line">        label = Variable(label.cuda())</span><br><span class="line">        score = net(feature)</span><br><span class="line">        loss = loss_function(score, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        train_acc += accuracy_score(torch.argmax(score.cpu().data,</span><br><span class="line">                                                 dim=<span class="number">1</span>), label.cpu())</span><br><span class="line">        train_loss += loss</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> test_feature, test_label <span class="keyword">in</span> test_iter:</span><br><span class="line">            m += <span class="number">1</span></span><br><span class="line">            test_feature = test_feature.cuda()</span><br><span class="line">            test_label = test_label.cuda()</span><br><span class="line">            test_score = net(test_feature)</span><br><span class="line">            test_loss = loss_function(test_score, test_label)</span><br><span class="line">            test_acc += accuracy_score(torch.argmax(test_score.cpu().data,</span><br><span class="line">                                                    dim=<span class="number">1</span>), test_label.cpu())</span><br><span class="line">            test_losses += test_loss</span><br><span class="line">    end = time.time()</span><br><span class="line">    runtime = end - start</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch: %d, train loss: %.4f, train acc: %.2f, test loss: %.4f, test acc: %.2f, time: %.2f&#x27;</span> %</span><br><span class="line">          (epoch, train_loss.data / n, train_acc / n, test_losses.data / m, test_acc / m, runtime))</span><br></pre></td></tr></table></figure><p>也可以直接看我的<a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/pytorch/langage%20model/lstm-sentiment.ipynb">notebook</a></p><p>后面试试textCNN，感觉也挺骚气的。</p><hr /><p>2018.08.16 更新一个textCNN的玩法。</p><p>CNN太熟了，很容易搞，其实只要把网络改一下，其他的动都不用动：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">textCNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embed_size, seq_len, labels, weight, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(textCNN, self).__init__(**kwargs)</span><br><span class="line">        self.labels = labels</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(weight)</span><br><span class="line">        self.embedding.weight.requires_grad = <span class="literal">False</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, (<span class="number">3</span>, embed_size))</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, (<span class="number">4</span>, embed_size))</span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, (<span class="number">5</span>, embed_size))</span><br><span class="line">        self.pool1 = nn.MaxPool2d((seq_len - <span class="number">3</span> + <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.pool2 = nn.MaxPool2d((seq_len - <span class="number">4</span> + <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.pool3 = nn.MaxPool2d((seq_len - <span class="number">5</span> + <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.linear = nn.Linear(<span class="number">3</span>, labels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        inputs = self.embedding(inputs).view(inputs.shape[<span class="number">0</span>], <span class="number">1</span>, inputs.shape[<span class="number">1</span>], -<span class="number">1</span>)</span><br><span class="line">        x1 = F.relu(self.conv1(inputs))</span><br><span class="line">        x2 = F.relu(self.conv2(inputs))</span><br><span class="line">        x3 = F.relu(self.conv3(inputs))</span><br><span class="line"></span><br><span class="line">        x1 = self.pool1(x1)</span><br><span class="line">        x2 = self.pool2(x2)</span><br><span class="line">        x3 = self.pool3(x3)</span><br><span class="line"></span><br><span class="line">        x = torch.cat((x1, x2, x3), -<span class="number">1</span>)</span><br><span class="line">        x = x.view(inputs.shape[<span class="number">0</span>], <span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        x = self.linear(x)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, self.labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span>(x)</span><br></pre></td></tr></table></figure><p>这里的网络设计很简单，就是用三个filter去扫一遍文章，filter的尺寸其实就是我们一次看多少个词。这样扫完以后是三个向量，然后pooling一下得到三个实数。把这三个实数拼成一个向量，然后用fc分类一下就结束了。</p><p>然后初始化网络：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net = textCNN(vocab_size=(vocab_size+<span class="number">1</span>), embed_size=embed_size,</span><br><span class="line">              seq_len=<span class="number">500</span>, labels=labels, weight=weight)</span><br></pre></td></tr></table></figure><p>其他的都没改，就可以直接跑了。速度上CNN比LSTM的参数少，速度快很多，不过只跑几轮的话效果差一点。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;2018.08.16更新一个textCNN。&lt;/p&gt;
&lt;p&gt;尝试使用LSTM做情感分析，这个gluon有非常详细的例子，可以直接参考gluon的&lt;a href=&quot;http://zh.gluon.ai/chapter_natural-language-processing/sentiment-analysis.html&quot;&gt;官方教程&lt;/a&gt;。这里尝试使用PyTorch复现一个。数据用的是IMDB的数据&lt;a href=&quot;http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz&quot;&gt;http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    <category term="PyTorch" scheme="https://samaelchen.github.io/categories/PyTorch/"/>
    
    
  </entry>
  
  <entry>
    <title>word2vec的PyTorch实现</title>
    <link href="https://samaelchen.github.io/word2vec_pytorch/"/>
    <id>https://samaelchen.github.io/word2vec_pytorch/</id>
    <published>2018-08-01T16:00:00.000Z</published>
    <updated>2021-08-31T16:15:25.582Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Word Embedding现在是现在NLP的入门必备，这里简单实现一个CBOW的W2V。</p><p>2018-07-06更新一发用一篇小说来训练模型的脚本。</p><p>2018-08-02更新一发negative sampling版本。</p><span id="more"></span><h1 id="negtive-sampling版本">negtive sampling版本</h1><p><strong>2018-08-02更新基于negative sampling方法的W2V</strong></p><p>翻了之前项亮实现的MXNet版本的NCE，看的不甚理解，感觉他写的那个是NEG的样子，然后还是自己写一个简单的negative sampling来做这个事情。关于NCE和NEG的区别，其实NEG就像是NCE的一个特殊情况，这个可以看<a href="https://arxiv.org/pdf/1410.8251.pdf">Notes on Noise Contrastive Estimation and Negative Sampling</a>，或者是谷歌的一篇<a href="https://www.tensorflow.org/extras/candidate_sampling.pdf">总结</a>。</p><p>关于negative sampling这里简单介绍一下，其实负采样的思路非常的简单，就是原来我们有多少个词，那么softmax就要算多少个词的概率，用负采样的方法就是将原来这样的巨量分类问题变成一个简单的二分类问题。也就是说，原来正确的label依然保留，接着只要sample出一小部分的负样本出来，然后做一个二分类问题就可以了。至于需要sample多少负样本，谷歌的C版本中是用了5个，好像哪里见过说不超过25个就可以了，但是现在忘了是哪篇文章了，可能不准确O__O "…</p><p>具体的公式推导其实很简单，可以看一下gluon关于<a href="http://zh.gluon.ai/chapter_natural-language-processing/word2vec.html">负采样的介绍</a>。</p><p>所以实际上要实现这个负采样非常的容易，只要设计一个抽样分布，然后开始抽样就可以了。在很多词向量的资料里面都说到了，采样分布选用的是： <span class="math display">\[P(w_i) = \frac{f(w_i)^{0.75}}{\sum(f(w_j)^{0.75})}\]</span> 这个其实非常像softmax，就是说用单个词的词频除以全部词频的和，原来的代码中加入了0.75的这个幂指数，完全是炼丹经验。</p><p>然后网上参考了一个开源的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NEGLoss</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, ix_to_word, word_freqs, num_negative_samples=<span class="number">5</span>,</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NEGLoss, self).__init__()</span><br><span class="line">        self.num_negative_samples = num_negative_samples</span><br><span class="line">        self.vocab_size = <span class="built_in">len</span>(word_freqs)</span><br><span class="line">        self.dist = F.normalize(torch.Tensor(</span><br><span class="line">            [word_freqs[ix_to_word[i]] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.vocab_size)]).<span class="built_in">pow</span>(<span class="number">0.75</span>), dim=<span class="number">0</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self, num_samples, positives=[]</span>):</span></span><br><span class="line">        weights = torch.zeros((self.vocab_size, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> positives:</span><br><span class="line">            weights[w] += <span class="number">1.0</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_samples):</span><br><span class="line">            w = torch.multinomial(self.dist, <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">while</span>(w <span class="keyword">in</span> positives):</span><br><span class="line">                w = torch.multinomial(self.dist, <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">            weights[w] += <span class="number">1.0</span></span><br><span class="line">        <span class="keyword">return</span> weights</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>, target</span>):</span></span><br><span class="line">        <span class="keyword">return</span> F.nll_loss(<span class="built_in">input</span>, target,</span><br><span class="line">                          self.sample(self.num_negative_samples,</span><br><span class="line">                                      positives=target.data.numpy()))</span><br></pre></td></tr></table></figure><p>但是有个小问题就是，这里采用的其实是很取巧的一个方法，就是说，我每次会生成一个矩阵告诉pytorch究竟有哪6个sample被我拿到了，然后算negative log likelihood的时候就只算这6个。结果上来说，是实现了负采样，但是从算法效率上来说，其实并没有起到减少计算量的效果。</p><p>所以这里我们实现一个非常简单，类似nagative sampling，但是不是非常严格的采样函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neg_sample</span>(<span class="params">num_samples, positives=[]</span>):</span></span><br><span class="line">    freqs_pow = torch.Tensor([freqs[ix_to_word[i]] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(vocab_size)]).<span class="built_in">pow</span>(<span class="number">0.75</span>)</span><br><span class="line">    dist = freqs_pow / freqs_pow.<span class="built_in">sum</span>()</span><br><span class="line">    w = np.random.choice(<span class="built_in">len</span>(dist), (<span class="built_in">len</span>(positives), num_samples), p=dist.numpy())</span><br><span class="line">    <span class="keyword">if</span> positives.is_cuda:</span><br><span class="line">        <span class="keyword">return</span> torch.tensor(w).to(device)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.tensor(w)</span><br></pre></td></tr></table></figure><p>然后相应的，我们需要将我们的CBOW也变一下，按照 <span class="math display">\[-\text{log} \frac{1}{1+\text{exp}\left(-u_c^\top (v_{o_1} + \ldots + v_{o_{2m}}) /(2m)\right)}  - \sum_{k=1, w_k \sim \mathbb{P}(w)}^K \text{log} \frac{1}{1+\text{exp}\left((u_{i_k}^\top (v_{o_1} + \ldots + v_{o_{2m}}) /(2m)\right)}.\]</span> 这个公式计算最后的loss。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CBOW</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embedding_dim, context_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CBOW, self).__init__()</span><br><span class="line">        self.embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.embeddings.weight.data.uniform_(-<span class="number">0.5</span> / vocab_size, <span class="number">0.5</span> / vocab_size)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs, label</span>):</span></span><br><span class="line">        negs = neg_sample(<span class="number">5</span>, label)</span><br><span class="line">        u_embeds = self.embeddings(label).view(<span class="built_in">len</span>(label), -<span class="number">1</span>)</span><br><span class="line">        v_embeds_pos = self.embeddings(inputs).mean(dim=<span class="number">1</span>)</span><br><span class="line">        v_embeds_neg = self.embeddings(negs).mean(dim=<span class="number">1</span>)</span><br><span class="line">        loss1 = torch.diag(torch.matmul(u_embeds, v_embeds_pos.transpose(<span class="number">0</span>, <span class="number">1</span>)))</span><br><span class="line">        loss2 = torch.diag(torch.matmul(u_embeds, v_embeds_neg.transpose(<span class="number">0</span>, <span class="number">1</span>)))</span><br><span class="line">        loss1 = -torch.log(<span class="number">1</span> / (<span class="number">1</span> + torch.exp(-loss1)))</span><br><span class="line">        loss2 = -torch.log(<span class="number">1</span> / (<span class="number">1</span> + torch.exp(loss2)))</span><br><span class="line">        loss = (loss1.mean() + loss2.mean())</span><br><span class="line">        <span class="keyword">return</span>(loss)</span><br></pre></td></tr></table></figure><p>这里我将embedding层的权重进行了标准化，通过这样的标准化可以避免后面计算loss的时候出现无穷大的情况。然后其他参数不用做什么变化，开始训练看看效果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    total_loss = torch.Tensor([<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> context, target <span class="keyword">in</span> tqdm(data_iter):</span><br><span class="line">        context_ids = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(context[<span class="number">0</span>])):</span><br><span class="line">            context_ids.append(make_context_vector([context[j][i] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(context))], word_to_ix))</span><br><span class="line">        context_ids = torch.stack(context_ids)</span><br><span class="line">        context_ids = context_ids.to(device)</span><br><span class="line">        model.zero_grad()</span><br><span class="line">        label = make_context_vector(target, word_to_ix)</span><br><span class="line">        label = label.to(device)</span><br><span class="line">        loss = model(context_ids, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">    losses.append(total_loss)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch %d loss %.4f&#x27;</span> %(epoch, total_loss))</span><br><span class="line"><span class="built_in">print</span>(losses)</span><br></pre></td></tr></table></figure><p>完整的notebook可以看<a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/pytorch/langage%20model/w2v_ngs.ipynb">这个</a>，效率上有质的提升。batchsize还是1024的时候大概压缩到15分钟左右，放到8192的时候大概一个epoch是10分钟。一本满足。</p><hr /><h1 id="toy-版本">toy 版本</h1><p>首先import必要的模块： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.autograd <span class="keyword">as</span> autograd</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br></pre></td></tr></table></figure></p><p>CBOW的全称是continuous bag of words。和传统的N-gram相比，CBOW会同时左右各看一部分词。也就是说，根据左右两边的词，猜测中间的词是什么。而传统的N-gram是根据前面的词，猜后面的词是什么。在PyTorch的官网上给出了N-gram的实现。因此我们只需要在这个基础上进行简单的修改就可以得到基于CBOW的W2V模型。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">CONTEXT_SIZE = <span class="number">2</span>  <span class="comment"># 2 words to the left, 2 to the right</span></span><br><span class="line">raw_text = <span class="string">&quot;&quot;&quot;We are about to study the idea of a computational process.</span></span><br><span class="line"><span class="string">Computational processes are abstract beings that inhabit computers.</span></span><br><span class="line"><span class="string">As they evolve, processes manipulate other abstract things called data.</span></span><br><span class="line"><span class="string">The evolution of a process is directed by a pattern of rules</span></span><br><span class="line"><span class="string">called a program. People create programs to direct processes. In effect,</span></span><br><span class="line"><span class="string">we conjure the spirits of the computer with our spells.&quot;&quot;&quot;</span>.split()</span><br><span class="line"></span><br><span class="line"><span class="comment"># By deriving a set from `raw_text`, we deduplicate the array</span></span><br><span class="line">vocab = <span class="built_in">set</span>(raw_text)</span><br><span class="line">vocab_size = <span class="built_in">len</span>(vocab)</span><br><span class="line"></span><br><span class="line">word_to_ix = &#123;word: i <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)&#125;</span><br><span class="line">data = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, <span class="built_in">len</span>(raw_text) - <span class="number">2</span>):</span><br><span class="line">    context = [raw_text[i - <span class="number">2</span>], raw_text[i - <span class="number">1</span>],</span><br><span class="line">               raw_text[i + <span class="number">1</span>], raw_text[i + <span class="number">2</span>]]</span><br><span class="line">    target = raw_text[i]</span><br><span class="line">    data.append((context, target))</span><br><span class="line"><span class="built_in">print</span>(data[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure><p>首先定义我们需要的数据。这里的CBOW的Windows是2。因为单词没法直接拿来训练，因此这里我们用id来唯一标识每一个单词。然后我们需要做的一个事情就是将这些id编码成向量。14年谷歌放出来的C那一版我印象中是用的霍夫曼树再降维，现在的PyTorch和gluon都有embedding的类，可以将分类的数据直接编码成向量。所以我们现在用框架实现这个事情就非常简单了。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CBOW</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embedding_dim, context_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CBOW, self).__init__()</span><br><span class="line">        self.embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.linear1 = nn.Linear(context_size * embedding_dim, <span class="number">128</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">128</span>, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        embeds = self.embeddings(inputs).view((<span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line">        out = F.relu(self.linear1(embeds))</span><br><span class="line">        out = self.linear2(out)</span><br><span class="line">        log_probs = F.log_softmax(out, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span>(log_probs)</span><br></pre></td></tr></table></figure><p>这个CBOW的类很简单，继承了PyTorch的Module类，然后第一步我们就做了一个embedding，然后做了一个隐藏层和一个输出层。最后我们做了一个softmax的动作来得到probability。这就是我们需要训练的神经网络。所以一直说W2V是一个单层的神经网络就是这个原因。</p><p>然后我们定义一个简单的函数，将单词转变成id <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_context_vector</span>(<span class="params">context, word_to_ix</span>):</span></span><br><span class="line">    idxs = [word_to_ix[w] <span class="keyword">for</span> w <span class="keyword">in</span> context]</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(idxs, dtype=torch.long)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">make_context_vector(data[<span class="number">0</span>][<span class="number">0</span>], word_to_ix)</span><br></pre></td></tr></table></figure></p><p>接着定义一些需要的参数： <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">losses = []</span><br><span class="line">loss_function = nn.NLLLoss()</span><br><span class="line">model = CBOW(<span class="built_in">len</span>(vocab), embedding_dim=<span class="number">10</span>, context_size=CONTEXT_SIZE*<span class="number">2</span>)</span><br><span class="line">model.to(device)</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure></p><p>这里需要注意一点，context_size需要windows的大小乘2，因为CBOW同时左右都看了这些词，所以我们放进来的词实际上是windows乘2的数量。</p><p>这里我用了GPU来加速计算。如果没有GPU的可以注释掉所有跟device相关的代码，这个数据量不大，体会不到GPU的优势。</p><p>然后就是正式训练 <figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    total_loss = torch.Tensor([<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> context, target <span class="keyword">in</span> data:</span><br><span class="line">        context_ids = make_context_vector(context, word_to_ix)</span><br><span class="line">        context_ids = context_ids.to(device)</span><br><span class="line">        model.zero_grad()</span><br><span class="line">        log_probs = model(context_ids)</span><br><span class="line">        label = torch.tensor([word_to_ix[target]], dtype=torch.long)</span><br><span class="line">        label = label.to(device)</span><br><span class="line">        loss = loss_function(log_probs, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">    losses.append(total_loss)</span><br><span class="line"><span class="built_in">print</span>(losses)</span><br></pre></td></tr></table></figure></p><p>这样就是一个词向量的训练过程。如果我们需要得到embedding之后的结果，只需要将数据过一遍embedding这一层就可以了。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.embeddings(make_context_vector(data[<span class="number">0</span>][<span class="number">0</span>], word_to_ix))</span><br></pre></td></tr></table></figure><p>可以对比一下训练前和训练后向量的差异。</p><hr /><h1 id="softmax低效率版本">softmax低效率版本</h1><p>2018-07-06更新内容：</p><p>之前写的那个是一个非常toy的网络，本质上就是了解一下word2vec是怎么一回事。不过完全不具备实操的能力。下面找了一些开源的语料，稍微修改了一下之前的脚本，还是基于CBOW的模型，这样就可以正常跑日常的数据。语料地址<a href="https://github.com/lxrogers/CS221SAT/tree/master/data/Holmes_Training_Data">https://github.com/lxrogers/CS221SAT/tree/master/data/Holmes_Training_Data</a>。</p><p>先import一些必要的包，这里的tqdm是显示进度的。 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.utils.data.dataloader <span class="keyword">as</span> dataloader</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.autograd <span class="keyword">as</span> autograd</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> gc</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br></pre></td></tr></table></figure></p><p>然后读入语料数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">text = []</span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> os.listdir(<span class="string">&#x27;Holmes_Training_Data/&#x27;</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(<span class="string">&#x27;Holmes_Training_Data&#x27;</span>, file), <span class="string">&#x27;r&#x27;</span>, errors=<span class="string">&#x27;ignore&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        text.extend(f.read().splitlines())</span><br><span class="line"></span><br><span class="line">text = [x.replace(<span class="string">&#x27;*&#x27;</span>, <span class="string">&#x27;&#x27;</span>) <span class="keyword">for</span> x <span class="keyword">in</span> text]</span><br><span class="line">text = [re.sub(<span class="string">&#x27;[^ \fA-Za-z0-9_]&#x27;</span>, <span class="string">&#x27;&#x27;</span>, x) <span class="keyword">for</span> x <span class="keyword">in</span> text]</span><br><span class="line">text = [x <span class="keyword">for</span> x <span class="keyword">in</span> text <span class="keyword">if</span> x != <span class="string">&#x27;&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(text[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure><p>这里我ignore了一些文本读入的错误，然后过滤掉了符号。</p><p>因为语料是英文的，所以这里按照空格分割单词，比中文方便太多。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">raw_text = []</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> text:</span><br><span class="line">    raw_text.extend(x.split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line">raw_text = [x <span class="keyword">for</span> x <span class="keyword">in</span> raw_text <span class="keyword">if</span> x != <span class="string">&#x27;&#x27;</span>]</span><br></pre></td></tr></table></figure><p>分好词以后就可以开始构建词库</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vocab = <span class="built_in">set</span>(raw_text)</span><br><span class="line">vocab_size = <span class="built_in">len</span>(vocab)</span><br></pre></td></tr></table></figure><p>接着跟之前一样，构建一个提供训练数据的函数，并准备好训练数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_context_vector</span>(<span class="params">context, word_to_ix</span>):</span></span><br><span class="line">    idxs = [word_to_ix[w] <span class="keyword">for</span> w <span class="keyword">in</span> context]</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(idxs, dtype=torch.long)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">word_to_ix = &#123;word: i <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)&#125;</span><br><span class="line">data = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, <span class="built_in">len</span>(raw_text) - <span class="number">2</span>):</span><br><span class="line">    context = [raw_text[i - <span class="number">2</span>], raw_text[i - <span class="number">1</span>],</span><br><span class="line">               raw_text[i + <span class="number">1</span>], raw_text[i + <span class="number">2</span>]]</span><br><span class="line">    target = raw_text[i]</span><br><span class="line">    data.append((context, target))</span><br><span class="line"><span class="built_in">print</span>(data[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure><p>定义网络，这里要注意的是，因为数据比较大，我们是分batch喂进来的，因此之前forward的时候，我们把embedding的数据摊开的时候是摊成一行的，这里需要摊成每个batch_size的大小。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CBOW</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embedding_dim, context_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CBOW, self).__init__()</span><br><span class="line">        self.embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.linear1 = nn.Linear(context_size * embedding_dim, <span class="number">128</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">128</span>, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        embeds = self.embeddings(inputs).view(<span class="built_in">len</span>(inputs), -<span class="number">1</span>)</span><br><span class="line">        out = F.relu(self.linear1(embeds))</span><br><span class="line">        out = self.linear2(out)</span><br><span class="line">        log_probs = F.log_softmax(out, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span>(log_probs)</span><br></pre></td></tr></table></figure><p>定义各种参数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">CONTEXT_SIZE = <span class="number">2</span></span><br><span class="line">batch_size = <span class="number">1024</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">losses = []</span><br><span class="line">loss_function = nn.NLLLoss()</span><br><span class="line">model = CBOW(vocab_size, embedding_dim=<span class="number">100</span>,</span><br><span class="line">             context_size=CONTEXT_SIZE*<span class="number">2</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"><span class="comment"># model = torch.nn.DataParallel(model, device_ids=[0, 1]).cuda()</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure><p>我这里本来写了多卡的跑法，但是不知道是不是我写法有问题还是为什么，每次我跑第二块卡的时候，PyTorch都会去第一块卡开一块空间出来，就算我只是在第二块卡跑也会在第一块卡开一些空间。比较神奇，后面再研究一下。</p><p>然后定义一下data iterator。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_iter = torch.utils.data.DataLoader(data, batch_size=batch_size,</span><br><span class="line">                                        shuffle=<span class="literal">False</span>, num_workers=<span class="number">4</span>)</span><br></pre></td></tr></table></figure><p>然后这里要注意的是，shuffle参数会影响每次iter的速度，shuffle会慢很多。另外num_workers越多速度越快，但是很可能会内存爆炸，需要自己调一个合适的。</p><p>然后就可以开始训练了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    total_loss = torch.Tensor([<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> context, target <span class="keyword">in</span> tqdm(data_iter):</span><br><span class="line">        context_ids = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(context[<span class="number">0</span>])):</span><br><span class="line">            context_ids.append(make_context_vector([context[j][i] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(context))], word_to_ix))</span><br><span class="line">        context_ids = torch.stack(context_ids)</span><br><span class="line">        context_ids = context_ids.to(device)</span><br><span class="line"><span class="comment">#         context_ids = torch.autograd.Variable(context_ids.cuda())</span></span><br><span class="line">        model.zero_grad()</span><br><span class="line">        log_probs = model(context_ids)</span><br><span class="line">        label = make_context_vector(target, word_to_ix)</span><br><span class="line">        label = label.to(device)</span><br><span class="line"><span class="comment">#         label = torch.autograd.Variable(label.cuda())</span></span><br><span class="line">        loss = loss_function(log_probs, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">    losses.append(total_loss)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch %d loss %.4f&#x27;</span> %(epoch, total_loss))</span><br><span class="line"><span class="built_in">print</span>(losses)</span><br></pre></td></tr></table></figure><p>如果要多卡可以把to(device)的代码改成注释的代码就可以了。</p><p>然后就是需要<strong>注意</strong>的点了。</p><p><strong>这个网络的确是work的，训练完可以试一下发现queen-woman+man和king的cosine similarity的确比monkey或者其他的单词要高。但是这个网络的效率很低！很低！很低！（你觉得我会告诉你一个epoch需要跑一个半小时么）。</strong></p><p>原因在哪呢？其实很简单因为我这里使用的是softmax，也就是说，这个网络每一次训练都需要预测所有的词，比如我这个训练集里面有接近37万个词，那么每次就需要预测37万个类，效率之低可想而知。那么有什么解决方案呢？最早的时候，也就是谷歌C版本的解决方案是基于霍夫曼树的hierarchical softmax。后来DeepMind有一篇介绍把NCE（Noise-contrastive estimation）用来加速的论文<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="http://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf">[1]</span></a></sup>。再后来又出现了negative sampling的论文<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">[2]</span></a></sup>。不过直观感受上，NCE和negative sampling是很像的，算是殊途同归吧。</p><p>后面过段时间更新对这两种方法的理解和代码。</p><div id="footnotes"><hr><div id="footnotelist"><ol style="list-style: none; padding-left: 0; margin-left: 40px"><li id="fn:1"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">http://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf<a href="#fnref:1" rev="footnote"> ↩︎</a></span></li><li id="fn:2"><span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf<a href="#fnref:2" rev="footnote"> ↩︎</a></span></li></ol></div></div>]]></content>
    
    
    <summary type="html">&lt;p&gt;Word Embedding现在是现在NLP的入门必备，这里简单实现一个CBOW的W2V。&lt;/p&gt;
&lt;p&gt;2018-07-06更新一发用一篇小说来训练模型的脚本。&lt;/p&gt;
&lt;p&gt;2018-08-02更新一发negative sampling版本。&lt;/p&gt;</summary>
    
    
    
    <category term="PyTorch" scheme="https://samaelchen.github.io/categories/PyTorch/"/>
    
    
  </entry>
  
  <entry>
    <title>台大李宏毅深度学习——神经网络一些骚操作</title>
    <link href="https://samaelchen.github.io/deep_learning_step4/"/>
    <id>https://samaelchen.github.io/deep_learning_step4/</id>
    <published>2018-07-22T16:00:00.000Z</published>
    <updated>2021-08-31T16:15:25.577Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>神经网络自2012年躲过世界末日后开始寒武纪大爆发的既视感，从最原始的FC开始出现了各种很风骚的操作。这里介绍三种特殊的结构：spatial transformer layer，highway network &amp; grid LSTM，还有recursive network。</p><span id="more"></span><h1 id="spatial-transformer">Spatial Transformer</h1><p>CNN是这一次深度学习大爆发的导火线，但是CNN有非常明显的缺陷。如果一个图像里的元素发生了旋转、位移、缩放等等变换，那么CNN的filter就不认识同样的元素了。也就是说，对于CNN而言，如果现在训练数字识别，喂进去的数据全是规整的，那么倾斜的数字可能就不认识了。</p><p>其实从某种意义上来说，这个就是过拟合了，每个filter能做的事情是非常固定的。不过换个角度来看，是不是也能理解为数据过分干净了？</p><p>那么为了解决这样的问题，其实有很多解决方案，比如说增加样本量是最简单粗暴的方法，通过image augment就可以得到海量的训练数据。另外一般CNN里面的pooling层也是解决这个问题的，不过受限于pooling filter的size大小，一般来说很难做到全图级别的transform。另外一种做法就是spatial transformer。实际上，spatial transformer layer我感觉上就是嵌入网络的image augment，或者说是有导向性的image augment。这样的做法可以减少无脑augment带来太多的噪音。个人理解不一定对。这种方法是DeepMind提出的，论文就是<a href="https://arxiv.org/pdf/1506.02025.pdf">Spatial Transformer Networks</a>。</p><p>首先看一下如果要对图像进行transform的操作，我们应该怎么做？对于一个图像里的像素而言有两个下标<span class="math inline">\((x,y)\)</span>来表示位置，那么我们就可以将这个看作是一个向量。这样以来，我们只需要通过一个二阶方阵就可以操作图像的缩放和旋转，然后加上一个二维向量就可以控制图片的平移。也就是说 <span class="math display">\[\begin{bmatrix}x&#39; \\ y&#39; \end{bmatrix} = \begin{bmatrix}a &amp; b \\ c &amp; d \end{bmatrix} \begin{bmatrix}x \\ y \end{bmatrix} + \begin{bmatrix}e \\ f \end{bmatrix}\]</span></p><p>当然，简洁一点可以写成： <span class="math display">\[\begin{bmatrix}x&#39; \\ y&#39; \end{bmatrix} = \begin{bmatrix}a &amp; b &amp; c \\ d &amp; e &amp; f \end{bmatrix} \begin{bmatrix}x \\ y \\ 1 \end{bmatrix}\]</span> 两个公式的元素没有严格对应，不过意思一样。</p><p>但是这里需要注意的事情是，比如我们原来输入的图片是<span class="math inline">\(3 \times 3\)</span>的，我们输出的还是一个<span class="math inline">\(3 \times 3\)</span>的图片，而位置变换后超出下标的部分我们就直接丢弃掉，而原来有值，现在没有值的部分就填0。示意图如下：</p><p><img data-src='https://i.imgur.com/CSnxAU6.png'></p><p>然后有一点我一直没理解的点就是，在论文里面上一个等式的左边是source layer，右边的是target layer。直观上从forward的方向上看，数据从上一层到下一层，那么变化就应该是第一层经过变化后变到第二层。</p><p>论文里面没有太解释为什么会是这样的操作，看了一些别人的博客，大部分人也说得不清不楚的。个人的感觉吧，为了这么做是为了保证输出的feature map的维度能够保持不变，论文里面有一个示例图：</p><p align="center"><img data-src='https://i.imgur.com/XIBMatZ.png' width=70%></p><p>从图上面看的话，target和source都是保持不变的，唯一变换的是source上的sampling grid（感觉这么说也不太对，sampling grid的像素点数量其实也没变，就是位置或者说形状变了）。而这个sampling grid就是将target的网格坐标通过上面的公式做仿射变换得到的。那如果反过来，也就是说我们直接用source做放射变换的话，很可能得到的target是不规整的。所以应该说spatial transformer layer做的事情是学习我们正常理解的仿射变换的逆矩阵。比较神奇的是这个用bp居然可以自己学出来。</p><p>那么这里就会有个问题，因为sampling grid的像素点其实是没有变过的，所以这就意味着说仿射变换的结果很可能得到是小数的index。比如说<span class="math inline">\(\begin{bmatrix}1.6 \\ 2.4 \end{bmatrix} = \begin{bmatrix}0 &amp; 0.5 \\ 1 &amp; 0 \end{bmatrix} \begin{bmatrix}2 \\ 2 \end{bmatrix} + \begin{bmatrix}0.6 \\ 0.4 \end{bmatrix}\)</span>，那么这个时候要怎么办呢？如果我们按照就近原则的话，那么这个位置又会被定位到原图的<span class="math inline">\([2, 2]\)</span>这个位置，那么梯度就会变成0。所以这样是不行的，那么为了可以进行bp，论文里面采用了双线性插值的方法。也就是说，用离这个位置最近的四个顶点的像素，按照距离的比例作为权重，然后加权平均来填补这个位置的像素。</p><p>这个算法大概原理如下：</p><p align="center"><img data-src='https://i.imgur.com/b7IprgN.png' width=50%></p><p>我们现在想要求中间绿色点的像素，那么我们先算出<span class="math inline">\(R_1\)</span>和<span class="math inline">\(R_2\)</span>的像素： <span class="math display">\[R_1 = \frac{x_2 - x}{x_2 - x_1}Q_{11} + \frac{x - x_1}{x_2 - x_1}Q_{21} \\R_2 = \frac{x_2 - x}{x_2 - x_1}Q_{12} + \frac{x - x_1}{x_2 - x_1}Q_{22}\]</span> 然后计算<span class="math inline">\(P\)</span>的像素： <span class="math display">\[\boxed{P = \frac{y_2 - y}{y_2 - y_1}R_1 + \frac{y - y_1}{y_2 - y_1}R_2}\]</span></p><p>那么在DeepMind的试验里面，在卷基层里面加入了ST层之后，收敛以后target得到的输出大体上都是不变的。就像下图：</p><p align="center"><img data-src="https://i.imgur.com/x0Za3Tx.gif" /></p><p>另外就是这个变换矩阵，如果我们强行让这个矩阵长成<span class="math inline">\(\begin{bmatrix}1 &amp; 0 &amp; a \\ 0 &amp; 1 &amp; b \end{bmatrix}\)</span>，那么就会变成attention模式，网络自己会去原图上面扫描，这样就会知道模型在训练的时候关注图片的哪个位置。看起来就像下图：</p><p align="center"><img data-src='https://i.imgur.com/IDeic8W.png' width=70%></p><p>上面那一排的网络有两个ST layer，大体上可以看出来，红色的框都是在鸟头的位置，绿色的框都是在鸟身的位置。</p><h1 id="highway-network-grid-lstm">Highway network &amp; Grid LSTM</h1><p>Highway network最早是<a href="https://arxiv.org/pdf/1505.00387.pdf">Highway Networks</a>和<a href="https://arxiv.org/pdf/1507.06228.pdf">Training Very Deep Networks</a>这两篇论文提出的。Highway network实际上受到了LSTM的启发，从结构上来看，深层的前馈网络其实和LSTM非常的像，如下图：</p><p align="center"><img data-src='https://i.imgur.com/L4cqtdk.png' width=80%></p><p>所以二者的差别就在于，在前馈中只有一个input，而LSTM中每一层都要把这一个时刻的x也作为输入。所以很自然的一个想法，在LSTM中有一个forget gate决定要记住以前多久的信息，那么在前馈网络中也可以引入一个gate来决定有哪些之前的信息干脆就不要了，又或者有哪些以前的信息直接在后面拿来用。那最简单LSTM变种是GRU，所以highway network借鉴了GRU的方法，把reset gate拿掉，再把每个阶段的x拿掉。</p><p>所以将GRU简化一下再竖起来，我们就可以得到highway network：</p><p align="center"><img data-src='https://i.imgur.com/SsDSDuy.png' width=70%></p><p>那么模仿GRU的计算方法，我们计算<span class="math inline">\(h&#39; = \sigma(Wa^{t-1})\)</span>，<span class="math inline">\(z = \sigma(W^z a^{t-1})\)</span>，所以<span class="math inline">\(a^t = z \odot a^{t-1} + (1-z) \odot h&#39;\)</span>。</p><p>而后面微软的<a href="https://arxiv.org/pdf/1512.03385.pdf">ResNet</a>其实就是一个highway network的特别版本：</p><p align="center"><img data-src='https://i.imgur.com/hDTBRrE.png' width=60%></p><p>当然感觉也可以将ResNet看做是竖起来的LSTM。那ResNet里面的变换可以是很多层的，所以在现在的实现中，很常见的一个情况是将这个东西叫做一个residual block。</p><p>所以利用highway network有一个非常明显的好处就是可以避免前馈网络太深的时候会导致梯度消失的问题。另外有一个好处就是通过highway network可以让网络自己去学习到底哪个layer是有用的。</p><p>那既然可以将深度的记忆传递下去，那么这样的操作也可以用到LSTM里面，也就是grid LSTM。一般的LSTM是通过forget gate将时间方向上的信息传递下去的，但是并没有将layer之间的信息传递下去。因此grid LSTM就是加一个参数纵向传递，从而将layer的信息传递下去，直观上来说，就是在<span class="math inline">\(y\)</span>后面再拼一个vector，然后这个vector的作用跟<span class="math inline">\(c\)</span>一样。具体的可以看一下DeepMind的这篇论文，<a href="https://arxiv.org/pdf/1507.01526v1.pdf">Grid LSTM</a>。粗略来说，结构上像这样：</p><p align="center"><img data-src='https://i.imgur.com/BUWr2kn.png' width=60%></p><p>那有2D的grid LSTM很自然就会有3D的grid LSTM，套路都是差不多的。不过我还没想到的是，3D的grid LSTM要用在什么场景当中，多个output？！</p><h1 id="recursive-structure">Recursive Structure</h1><p>遥想当年刚接触RNN的时候根本分不清recursive network和recurrent network，一个是递归神经网络，一个是循环神经网络，傻傻分不清。但是实际上，recurrent network可以看作是recursive network的特殊结构。Recursive network本身是需要事先定义好结构的，比如：</p><p align="center"><img data-src='https://i.imgur.com/SgEEBbw.png' width=60%></p><p>那常见的recurrent network其实也可以看做是这样的一个树结构的recursive network。Recursive network感觉上好像也没什么特别有意思的东西，比较有趣的就是这边<span class="math inline">\(f\)</span>的设计。比如说现在想要让机器学会做句子的情感分析，那么很简单的一个想法就是把每一个词embedding，然后放到网络里面训练，那么我们可以用这样的一个结构：</p><p align="center"><img data-src='https://i.imgur.com/YFhqVos.png' width=60%></p><p>因为在自然语言里面会有一些类似否定之否定的语法，所以我们希望说very出现的时候是加强语气，但是not出现的时候就是否定之前的。如果用数学的语言来表达，这不就是乘以一个系数嘛。所以在这样的情况下，如果我们只是简单的加减操作，那么就没有办法实现这种“乘法”操作。所以这个时候，我们的<span class="math inline">\(f\)</span>设计就会有点技巧：</p><p align="center"><img data-src='https://i.imgur.com/E4Gt3UC.png' width=60%></p><p>那么看一下这个设计。如果我们直接采用最传统的做法，就是将<span class="math inline">\(a\)</span>和<span class="math inline">\(b\)</span>直接concat起来，然后乘以一个矩阵<span class="math inline">\(W\)</span>，再经过一个激活函数变换，这样的操作其实只能做到线性的关系，个人感觉，实际上这样的设计会将<span class="math inline">\(a\)</span>和<span class="math inline">\(b\)</span>的一些交互特性变成隐藏特征保存在<span class="math inline">\(W\)</span>当中，但是一旦输入变化了，这些隐藏的特征却不能被传递出来，所以效果不好。</p><p>因此下面的一种设计就比较骚气，后面还是传统的做法，但是前面加上了一个vector。这个vector的元素就是来学习这些词之间的interaction，然后将这些interaction变成bias，因为recursive network的function都是不变的，因此这些bias就这样被传递下去了。那么这里有一个需要注意的就是，我们这里有几个词，那我们就需要多少个bias，而且每个bias中间的这个矩阵<span class="math inline">\(W\)</span>都是不一样的。</p><p>这是三个比较骚气的网络结构变换，感觉看了这么多，好多网络之间都是殊途同归啊，会不会最后有一个非常general的网络结构出现，使得现在的每一种网络都是其一种特殊情况呢？</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;神经网络自2012年躲过世界末日后开始寒武纪大爆发的既视感，从最原始的FC开始出现了各种很风骚的操作。这里介绍三种特殊的结构：spatial transformer layer，highway network &amp;amp; grid LSTM，还有recursive network。&lt;/p&gt;</summary>
    
    
    
    <category term="深度学习" scheme="https://samaelchen.github.io/categories/DeepLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>经济思想史读书笔记——亚当·斯密</title>
    <link href="https://samaelchen.github.io/economics_thoughts_notes4/"/>
    <id>https://samaelchen.github.io/economics_thoughts_notes4/</id>
    <published>2018-07-17T16:00:00.000Z</published>
    <updated>2021-08-31T16:15:25.578Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>“亚当·斯密在政治经济学的历史中占据如此中心的位置，以至于这个谨慎的水手不愿在这样广阔的海洋上开始着手航行。”——亚历山大·格雷</p><span id="more"></span><p>亚当·斯密是经济思想史上一个非常重要的人物，他的著作是经济思想发展中的一道分水岭。</p><h1 id="博学多才的经济学家">博学多才的经济学家</h1><p>斯密的思想涉及到很多领域，包括经济、政治还有哲学。在斯密所处的时代，从事研究的知识精英被要求掌握最宽广的人类知识，而不是仅仅专攻某一个领域的知识。这种多学科方法的一个后果是，像亚当·斯密探求如今称为社会科学的人认为，牛顿在物理学中建立的科学严密性，他们也可以做到。</p><p>斯密常被称为经济学之父，尽管很多先贤看到了经济学很多方面，但是没有人能够将决定国民财富的力量、培育经济增长与发展的适宜的政策以及通过市场力量有效协调大量经济决策的方式，整合成一个全面的观点。</p><p>斯密对经济学范围的认识继承了英国重商主义的观点。他对解释国民财富的性质与原因感兴趣。但是斯密考察的范围比现代经济学家研究的要宽泛，他用政治的、社会的、历史的材料来填充其经济模型。</p><p>虽然斯密在经济学史上的地位非常重要，但是斯密的理论模型缺少优雅与严密。</p><h1 id="斯密的市场分析与政策结论">斯密的市场分析与政策结论</h1><p>斯密在经济思想史的重要性基于（1）他对经济体相互关联性的广泛了解和（2）他对经济政策的影响。斯密作为一位经济学家的强大实力在于，他洞察了：（1）经济体组成部分的相互依赖；（2）用来推动已过财富的政策。他不仅是一个经济学家，而且是一位指出了经济发展与富足方法的哲学家。</p><h2 id="前后关联的经济政策">前后关联的经济政策</h2><p>斯密的方法论塑造了他对经济体的分析，以及他关注政府政策的决心。斯密对自由放任的主张部分依赖于市场如何产生某些结果的理论模型，还来自于他对现有历史与制度环境的观察。所以，斯密的经济政策是前后关联的。斯密对自由市场的拥护并不是因为他认为市场是完美的，而是因为他所处的时代，英国的历史与制度结构导致，市场通常比政府干预产生更好的结果。</p><p>在之前讨论过，经济学科学处理经济变量之间实证的、事务性的关系，即探讨“是什么”。规范经济学涉及的是“应当怎样”的问题。经济学艺术是以政策为导向的。亚当·斯密是一个超群的经济学艺术大师。因而，前后关联的经济政策，就是另一种表达经济学艺术观点的方式。</p><h2 id="自然秩序和谐和自由放任">自然秩序、和谐和自由放任</h2><p>斯密的经济学与重商主义有很多相同的基本因素。他们认为通过科学的调查能够揭示事务性的因果关系。斯密也像重商主义一样，提出人类本性的基本假设：人类是理性的，是有私心的，很大程度上受经济利己主义的驱使。也就是古典经济学里面的一个基本假设——理性人假设。</p><p>但是斯密体系和重商主义有一个很大的不同点在于，他假设竞争性市场在极大程度上是存在的，在这些市场内部，生产要素自由流动，从而提升了它们的经济优势。第二个区别是，经济体的自然运作，能够比人类做出的任何安排更有效地解决冲突。这也是斯密体系里面一个非常重要的假设基础，就是存在完全竞争市场。实际上，斯密的自由放任政策只有在完全竞争市场下才能成立。</p><p>斯密的推论过程非常简单。人类是理性的，是有私心的，受利己主义驱使。如果放任不管，每个个体都会追求自己的私利，在促进私利的同时也促进了社会利益。政府不应当干预这一过程，而应当遵循自由放任的政策。在斯密体系内，私利与公共利益是和谐的。也就是说，在斯密看来，在没有政府干预的竞争性市场上会出现资源的最佳配置。</p><h2 id="竞争性市场的运作">竞争性市场的运作</h2><p>斯密对经济理论最重要的贡献是他对竞争性市场运作的分析。他能比以前的经济学家更详细说明，源于竞争的价格在长期中为什么等于生产成本这一道理。他对价格形成与资源配置的分析中，他将短期价格称为“市场价格”， 将长期价格称为“自然价格”。他认为，竞争从根本上要求有大量的销售者；数值经济体中利润、工资、租金的一群资源拥有者；资源在行业间自由流动。给定这些条件，资源拥有者的私利将会形成长期自然价格，该价格将经济体不同部门之间的利润率、工资、租金均等化。</p><p>确立了竞争性市场的优越性后，斯密毫无困难地构建起他反对垄断与政府干预的论据。但是斯密的自由放任是基于竞争市场存在的假设的。</p><p>斯密认为，尽管重商主义者关于政府干预的很多主张都声称是促进了社会利益，但是其实是增进了个人私利，这种管制不是有利于国家，而是利于商人。但是斯密也并不主张完全的自由放任，他认为在他所处时代历史的、政治的、制度的背景下，他认为，保护幼稚产业的关税是有必要的；国防的贸易管制也是有必要的；政府还应该提供具有极大社会收益但是私人市场因为没有充足利润而不去提供的产品，以此来限定他对自由放任的主张。</p><h2 id="资本与资本家">资本与资本家</h2><p>斯密提出了关于资本在财富生产过程中和在经济发展中的作用的一些重要概念。首先，他指出，一国的现有财富取决于资本积累，原因在于资本积累决定了劳动分工和参加生产性劳动的人口比例。其次，斯密断定资本积累也会导致经济发展。再次，与资本积累相结合的个人私利导致资本在各产业之间的最佳配置。</p><p>在斯密体系中，资本家在经济体运行中扮演主要的角色。资本家对财富与利润的追逐，引导经济体实现资源的有效配置和经济增长。在私有财产经济体中，资本的来源是个人储蓄。斯密认为，劳动并不能够积累资本，原因在于工资水平仅仅能够满足直接的消费欲望。</p><p>斯密断定，恰恰是一部分正在星期的产业接济是对社会有益的人，他们为了利润而奋斗，努力积累资本，通过储蓄和投资来增加他们的财富。因此有利于资本家的收入不平等分配具有巨大的社会重要性。没有收入的不平等分配，就不可能有经济增长，因为全部的年产出都会被消费掉。</p><h2 id="斯密对政策的影响">斯密对政策的影响</h2><p>斯密的重要贡献在于他对市场经济在多种用途之间配置稀缺资源方式的广泛看法。他的主要政策结论是政府应当接受自由放任的政策。</p><h1 id="国民财富的性质与原因">国民财富的性质与原因</h1><p>斯密不认同重商主义的一国的贵金属就等同于一国财富的观点。他认为，财富是产品与服务的年流量，而不是贵金属的累计储备量。他也解释了出口与进口之间的关系，认识到出口的基本作用是支付进口。此外他暗示，经济活动的最终目的是消费。这将斯密的经济学与重商主义的经济学加以区分，后者将生产视为目的。而在国家财富源泉的认识上，斯密与重农主义也不一样，后者强调的是消费。</p><p>斯密继而建议，国家的财富应当按照人均指标来衡量，这就是现在经常被提及的人均GDP。</p><h2 id="国民财富的原因">国民财富的原因</h2><p>斯密主张，一个国家的财富，也就是国家的收入取决于：（1）劳动生产力；（2）有效使用或者生产性使用的劳动者的比例。因为在他的完全竞争市场下，经济体将自动实现资源的充分利用，因此他只需要考察那些决定一国产品与服务生产能力的因素。</p><h3 id="劳动生产力">劳动生产力</h3><p>斯密认为，劳动生产力取决于劳动分工。专业化与劳动分工提高了劳动生产力。尽管斯密认为专业化与劳动分工的经济利益，但是他也察觉到一些严重的社会成本。劳动分工的一个社会缺点是公认被赋予重复性的任务，这些任务很快变得单调乏味。这一点其实与管理学里面的科学管理法非常相似，另外可以看一下霍桑试验。但是我们不得不承认，劳动分工增加了人类福利。</p><p>劳动分工一次取决于斯密所谓的市场的范围与资本的积累。市场越大，可销售的数量越多，劳动分工的机会就越多。另一方面，有限的市场仅允许有限的劳动分工。劳动分工收到资本积累的限制，原因在于生产过程是耗时的：在生产开始与成品的最终销售之间存在时间间隔。</p><p>在一个简单的经济体中，劳动分工是微小的，只需要很少的资本来维持劳动力。但是随着劳动分工的增加，劳动者不再为其自身的消费生产产品，在耗时的生产过程期间，必须保持一定的消费品储备来维持劳动者。这一定数量的产品来自储蓄，也就是斯密所说的资本。资本家的一个主要功能是缩短生产开始与最终产品销售之间的时间间隔而提供手段。因此可以使用劳动分工的生产过程的范围，收到可以利用的资本积累数量的限制。换而言之，斯密认为，劳动分工随着越来越多的储蓄而越来越细分。如果将这一个观点放到小一点的经济体上，比如公司，我们就会很直观理解，创业公司，一个人当好几个人用，当公司庞大到商业帝国的程度，几个人当一个人用。</p><h3 id="生产性与非生产性劳动">生产性与非生产性劳动</h3><p>按照斯密的观点，资本积累也决定了生产性：劳动者与非生产性劳动者的比例。他主张，生产可销售商品所使用的劳动是生产性劳动，而生产服务所使用的劳动则是非生产性劳动。斯密的观点其实非常的朴素，资本应当用于再生产。他的观点，如果一个做法对个体是正确的，那么对国家也是正确的，因此，国家的资本越多，就更应该用于支持生产性劳动。在斯密的时代，他并没有意识到第三产业的作用。</p><p>斯密强调，将大量收入分配给进行储蓄和投资的资本家，将少量收入分配给地主，可以获得最高的经济增长率。某种程度上，是不是也可以看作是需要大力扶持实业。此外，因为经济增长收到政府非生产性劳动支出的约束，例如军队，所以拥有较小的政府，就可以对资本家征收较少的税，以便他们可以积累更多的资本。这里有一点点小国寡民的意味。</p><h2 id="对国民财富的总结">对国民财富的总结</h2><p>其实在斯密的观点中，这一点一句话就可以概括，资本是国家财富的主要决定因素。</p><p>对斯密而言，资本积累毫无疑问要求一个自由市场与私人财产的制度框架。在自由市场体制中，既定的投资支出水平，在没有政府指导的运转中被加以分配，以确保最高的经济增长率。在私人财产体制中，对高资本积累率的进一步要求就是不平等的收入分配。</p><h1 id="国际贸易">国际贸易</h1><p>在斯密的观点中，只有错误地认为一个国家的财富取决于它所持有的贵金属和借据，贸易顺差才是有利的。而斯密的主张是非规制的对外贸易，理由是如果英国能够以低于法国的成本生产一种产品，例如羊毛，并且法国可以以低于英国的成本生产另一种产品，例如葡萄酒，那么两国各自用生产成本较低的产品，去交换生产成本较高的产品，这种交换对双方来说都是有利的。也就是现在经济学中的对外贸易的绝对成本学说。实际上这种学说不局限于国际贸易，还适用于一国的内部贸易。</p><p>在现代经济学的观点中，随着劳动越来越专业化，存在收益递增（成本递减）的情况。斯密的对外贸易优势的部分观点，明显基于收益递增这一动态概念。但是这种观点是有非常明显的缺陷的，事实上，任何一国国家都不可能长期保持一种生产方式不变。</p><p>在这个方面，古典经济学和重商主义者有一个重大的区别。重商主义者认为国际贸易是一种零和博弈，而古典经济学认为不是。但是斯密只是认识到贸易对各国都有利，而没有认识到贸易过程中的价格机制。这一点在之后的李嘉图等人的理论中得到解释。</p><h1 id="价值理论">价值理论</h1><p>区分价值与价格困惑了早期经济学家。这主要集中于三个问题：（1）什么决定了产品的价格？也就是什么决定了相对价格？（2）什么决定了价格总水平？（3）什么是福利的最佳度量标准。实际上，斯密也没有非常明确给出答案。</p><h2 id="相对价格">相对价格</h2><p>按照现代的经济学术语，相对价格指的是商品间的价格比例关系。实际上相对价格是由李嘉图提出来的。另外一个概念是绝对价格，绝对价格其实就是用货币单位表示的价格水平。</p><p>斯密认为，市场价格或者短期价格是由供需双方决定的。自然价格或者长期均衡价格通常取决与生产成本。在现代经济学里面，我们认为自然价格是达到供需平衡时候的价格。</p><p>斯密对他所处时代经济体中相对价格形成的分析，区分为两个时间段和经济体的两个宽泛的部门，分别是短期与长期、农业与制造业。在短期或者市场阶段，斯密在制造业与农业中都发现了乡下倾斜的需求曲线与向上倾斜的供给曲线；因此市场价格取决于需求与供给。斯密对长期中发生的更为复杂的“自然价格”的分析，包含着一些矛盾。对于农业部门而言，自然价格取决于供给与需求，原因在于长期供给曲线向上倾斜，表明成本递增。但是对制造业部门来说，长期供给曲线有时假定为完全富有弹性（水平的），表明成本不变；在分析的另一些地方又向下倾斜，表明成本递减。在制造业中，当长期供给曲线完全富有弹性时，价格就完全取决于生产成本；但是，当长期供给曲线向下倾斜时，自然价格就取决于需求与供给双方。不过，无视长期供给曲线在制造业中的形状，主要强调生产成本对自然价格的决定，这是斯密以及后来的古典经济学家的特点。</p><p>经院哲学对相对价格问题感兴趣，因为他们关注交换过程中的道德问题；重商主义者则是认为财富在交换过程中产生。斯密认为，一旦经济体实行专业化和劳动分工，交换就变成必需。如果交换发生在斯密时代的市场中，就会出现一些显而易见的问题。第一，如果交换处于高于物物交换水平的状况下，就会存在交换媒介的问题。第二，存在价值或者相对价格的问题。</p><h2 id="价值的含义">价值的含义</h2><p>斯密认为价值一词有两种不同的含义，它有时表示一些特定物品的效用，有时又表示因占有物品而取得的对其他产品的购买力。前者可以称为“使用价值”，后者被称为“交换价值”。使用价值很大的东西，其交换价值往往很小，甚至没有；相反，交换价值很大的东西，其使用价值往往极小。就像水的使用价值非常高，但是交换价值很低；钻石的交换价值很高，但是使用价值非常低。</p><p>这里的交换价值是指一种商品购买其他产品的能力。这是市场所表达的一种客观度量。他关于使用价值的概念是含糊的。一方面，使用价值有道德内涵，这是对经院哲学的回归。另一方面，使用价值是一件商品满足需要的能力，是因持有或消费一件产品而获得的效用。当一件商品被消费时，可以获得几种效用：（1）它的总效用，（2）它的平均效用，（3）它的边际效用。斯密的关注点是总效用，这就模糊了他对需求如何在价格决定中发挥作用的理解。显然，水的总效用超过了钻石的总效用。然而，因为商品的边际效用经常随着其消费得更多而递减，所以水的边际效用比钻石的边际效用低。</p><p>我们愿意为一件商品所支付的价格——我们对获得又一单位商品所寄予的价值——不仅取决于商品的总效用，而且取决于其边际效用。因为斯密没有意识到这一点，因此他既不能为“钻石-水悖论”找到满意的解决办法，也不能了解其使用价值与交换价值之间的关系。某种程度上是不是认为这是供给量上的区别？大多数情况下水比钻石多，所以边际效用就低。</p><h2 id="斯密关于相对价格">斯密关于相对价格</h2><p>因为斯密对相对价格的决定因素有些困惑，所以，他发展了与这些因素相关的三个独立的理论：（1）劳动成本价值理论，（2）劳动支配价值理论，（3）生产成本价值理论。</p><p>他假设了经济体两种截然不同的状态：初期野蛮状态或者原始社会，它被界定为这样的一个经济体，其中资本还没有被积累起来，土地未被使用；发达经济体，其中资本与土地不再是资源充足的产品（它们具有超过零以上的价格）。</p><h2 id="原始社会中的劳动成本理论">原始社会中的劳动成本理论</h2><p>如果在一个狩猎国家，杀死一头海狸所需的劳动通常两倍于杀死一头野鹿所需要的劳动，那么，一头海狸自然就可以换来或者值两头野鹿。按斯密的劳动成本理论，在还不存在土地与资本的经济体中，或者土地与资本还是无限充足的自然资源的经济体中，一件产品的交换价值或者价格，由生产产品所需的劳动量决定。</p><p>这让我们认识到劳动成本价值理论的第一个难点。我们应当如何度量生产一件商品所需要的劳动量。斯密认识到生产一件产品所需的劳动量不能简单地用时钟表示的时间数量来度量，原因在于，除了时间之外，也必须考虑有关的精巧或者技能，以及任务的艰难与困苦。</p><p>在这一点上，斯密遇到了所有的劳动成本价值理论都遇到的，扔未被后来的经济学家成功地予以解决的一个难题。如果劳动量是一个以上变量的函数，那么，我们必须找到一种方法来说明所有变量的相对重要性。斯密主张时间、艰难程度以及精巧程度上的差异都反映在支付给劳动者的工资中。</p><p>但是斯密的观点只是重申了问题，他是在表明一件产品是依照支付给劳动者的工资，而不是依照包含在产品中的劳动量拥有价值。这是一个循环推论。斯密利用一套价格解释另一套价格。</p><h3 id="原始社会中的劳动支配">原始社会中的劳动支配</h3><p>按照斯密的观点，一件产品的价值“对于那些拥有产品的人以及那些想用它去交换一些新产品的人来说，正好等于产品能够使他们购买或支配的劳动量”。也就是说，如果捕获一头海狸或者两头野鹿需要两小时，那么两头野鹿就等于一头海狸。</p><h3 id="发达经济体中的劳动理论">发达经济体中的劳动理论</h3><p>因为资本已经被积累，土地也已经被利用，并且一件产品的最终价格也必须包括当做利润的资本家的收益以及当做地租的地主的收益。最终价格形成了由工资、利润、地租这些要素报酬构成的收入。</p><h3 id="相对价格的生产成本理论">相对价格的生产成本理论</h3><p>斯密最终放弃了任何劳动价值理论都适用于他所处时代一样发达的经济体。斯密似乎发现，一旦资本被积累起来，土地被加以利用，并且一旦必须支付利润、地租，还有劳动，能唯一适当解释价格的就是生产成本理论。</p><p>在成本理论中，一件商品的价值取决于对所有生产要素的支付：除了劳动之外还有土地和资本。在斯密的体系中，利润这一术语既包括今天的利润，也包括利息。在斯密假设平均成本不随着产量的增加而增加的地方，无论使用总成本还是使用平均成本，通过加总工资、利润和地租，这样的相对价格都是不变的。在平均成本随着产量变化的地方，价格就取决于需求与供给双方。然而，在分析长期自然价格的决定时，即使当供给曲线不被假定为完全富有弹性时，斯密也强调供给与生产成本。斯密主张，竞争占优势的地方，商人、劳动者、地主的私利将导致与生产成本相等的自然价格。</p><h1 id="分配理论">分配理论</h1><p>收入的个人分配取决于个人所出售的生产要素的价格与数量。劳动是大部分家庭拥有的唯一生产要素，因此家庭的收入一般取决于工资率与工作时间的长度。拥有财产的那些家庭所获得的财产收入量，取决于家庭所拥有的资本与土地的数量以及这些要素的价格。因为工资、利润、地租都是经济体中的价格，所以它们的相对价格——连同个人出售的劳动、资本、土地数量一起——决定了收入的分配。</p><h2 id="工资">工资</h2><p>斯密认为，在对工资的讨价还价过程中，劳动处于劣势。因为劳动市场是买方市场，雇主少，可以容易联合起来巩固他们的地位。即使罢工，雇主也有足够资源维持他们的生活，但是没有工作，工人生存困难。在这一部分，斯密削弱了市场力量的有益运作过程，并似乎已经意识到其完全竞争市场的假设受到了限制。</p><h2 id="工资基金">工资基金</h2><p>因为生产过程是耗时的，所以从生产过程开始到最终销售，需要一部分的产品库存或资本来维持劳动者的衣食住行，这部分被称为工资基金，来源于资本家的储蓄或者消费中断。给定劳动力和工资基金的规模，工资率=工资基金/劳动力。</p><h2 id="利润">利润</h2><p>斯密很自然接受了利润是因资本家执行了对社会有用的功能而对他的一种支付，这种功能就是在耗时的生产过程期间，为劳动者提供生活必需品，提供工作所用的原料和机器。按照斯密的观点，劳动者允许从其产量中进行利润的扣除，原因在于，劳动者并不拥有工作所用的原料和独立的支持手段。于是利润分为两部分：纯利息收入和风险收入。在原始经济体中，劳动者获得了全部的产品，但是在发达经济体中，劳动者却需要被扣除利润和地租，这一点斯密并没有做出解释。在深信资本主义制度基本和谐的理论家里，这一点是非常自然无需质疑的。</p><h2 id="地租">地租</h2><p>斯密提出了四种地租理论：（1）地主的需求；（2）垄断；（3）差异化的优势；（4）大自然的施舍。在《国富论》前面的部分，地租被视为决定价格的因素，而后面则视为价格被决定的因素。</p><h2 id="随时间变化的利润率">随时间变化的利润率</h2><p>斯密认为一个国家的经济增长取决于资本积累。他预测，随着时间的推移，利润率会下降，原因有三。（1）劳动市场的竞争。资本家的竞争导致工资上升利润下降。（2）商品市场的竞争。生产者竞争家具，商品价格下降，利润减少。（3）投资市场的竞争，因为投资机会有限，所以资本的积累增加会导致利润下降。</p><h1 id="福利与价格总水平">福利与价格总水平</h1><p>斯密努力去发现：第一，决定价格总水平的因素；第二，不同时期福利变化的最佳度量。其实这个问题还是比较复杂的，对于一个生产两种或者更多产品的经济体来说，有没有可能界定和度量其福利的变化。</p><p>如果用总消费或者产量来界定福利，那么需要解决的问题就是，寻找一种度量总消费或者总产品数量的方式。通常而言，这种度量方式就是国家的货币单位。现在的社会，我们通常会用GDP来衡量。</p><p>但是这里会有另外一个问题，那就是货币的本质是一般等价物。这就意味着，货币也是有价格的，它本身也会变化。所以斯密转向劳动，却发现劳动价格也会变化。最后，他采用劳动的复效用作为衡量标准。也就是说，如果我们能够使用较少的劳动生产相同的产量，那么我们就拥有更多的线下，经济状况就会更好。</p><p>事实上这种度量方式比斯密想象的要复杂的多，现在的经济学家在这一方面有了更长远的度量方式，例如度量“生活质量”。</p><p>斯密在经济学史上如此之重要，他对自由放任体系的推崇引导了西方市场经济数百年的发展。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;“亚当·斯密在政治经济学的历史中占据如此中心的位置，以至于这个谨慎的水手不愿在这样广阔的海洋上开始着手航行。”——亚历山大·格雷&lt;/p&gt;</summary>
    
    
    
    <category term="读书笔记" scheme="https://samaelchen.github.io/categories/ReadingNotes/"/>
    
    
  </entry>
  
  <entry>
    <title>线性代数 13</title>
    <link href="https://samaelchen.github.io/linear_algebra_step13/"/>
    <id>https://samaelchen.github.io/linear_algebra_step13/</id>
    <published>2018-07-16T16:00:00.000Z</published>
    <updated>2021-08-31T16:15:25.578Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>两个线性代数里面在机器学习领域大放异彩的算法，SVD和PageRank。</p><span id="more"></span><p>之前对方阵有对角化分解，而且不是所有的方阵都可以对角化，但是SVD是所有矩阵都可以进行分解的。SVD分解的过程如下：</p><p><img data-src='https://i.imgur.com/ZMo1wcb.png'></p><p>这个公式会有什么特性呢？我们假设<span class="math inline">\(U = \{u_1, u_2, \cdots, u_m\}\)</span>，<span class="math inline">\(V = \{v_1, v_2, \cdots, v_n \}\)</span>，<span class="math inline">\(\Sigma\)</span>是常数<span class="math inline">\(\{\sigma_1, \sigma_2, \cdots, \sigma_k\}\)</span>的对角矩阵。这里有一个点要注意的就是<span class="math inline">\(\Sigma\)</span>的样子大体上会是左上角一个对角矩阵，其余部分都是零的<span class="math inline">\(m \times n\)</span>的矩阵。这些<span class="math inline">\(\sigma\)</span>称为奇异值，这些奇异值会等于<span class="math inline">\(A^{\top}A\)</span>的特征根的平方根。</p><p>那么如果矩阵经过SVD分解以后，一定会得到<span class="math inline">\(Av_i = \begin{cases}\sigma_i u_i &amp; \text{if } 1 \le i \le k \\ 0 &amp; \text{if } i &gt; k \end{cases}\)</span>，<span class="math inline">\(A^{\top}u_i = \begin{cases}\sigma_i v_i &amp; \text{if } 1 \le i \le k \\ 0 &amp; \text{if } i &gt; k \end{cases}\)</span>。</p><p>现在问题来了，如果给一个矩阵，要怎么计算奇异值？假设有一个矩阵<span class="math inline">\(A = \begin{bmatrix} 0 &amp; 1 &amp; 2 \\ 1 &amp; 0 &amp; 1 \end{bmatrix}\)</span>，可以直观看到，这个矩阵是<span class="math inline">\(3 \times 2\)</span>的矩阵，因此需要在<span class="math inline">\(\mathbb{R}^3\)</span>和<span class="math inline">\(\mathbb{R}^2\)</span>都要有orthogonal matrix。所以先构建矩阵<span class="math inline">\(A^{\top}A = \begin{bmatrix}1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 2 \\ 1 &amp; 2 &amp; 5 \end{bmatrix}\)</span>。那么做一个<span class="math inline">\(\mathbb{R}^3\)</span>上面的orthogonal matrix，按照这个<a href="https://samaelchen.github.io/linear_algebra_step11/">博客</a>最后的正交化方法，我们可以将矩阵正交化为<span class="math inline">\(v_1 = \frac{1}{\sqrt{30}} \begin{bmatrix} 1 \\ 2 \\ 5 \end{bmatrix}\)</span>，<span class="math inline">\(v_2 = \frac{1}{\sqrt{5}} \begin{bmatrix} 2 \\ -1 \\ 0 \end{bmatrix}\)</span>，<span class="math inline">\(v_3 = \frac{1}{\sqrt{6}} \begin{bmatrix} 1 \\ 2 \\ -1 \end{bmatrix}\)</span>。然后求一下<span class="math inline">\(A^{\top}A\)</span>的特征根分别是6，1和0。所以奇异值就是<span class="math inline">\(\sqrt{6}\)</span>和1。然后就可以按照上面的公式算出<span class="math inline">\(u_1 = \frac{1}{\sqrt{5}} \begin{bmatrix} 2 \\ 1 \end{bmatrix}\)</span>，<span class="math inline">\(u_1 = \frac{1}{\sqrt{5}} \begin{bmatrix} -1 \\ 2 \end{bmatrix}\)</span>。</p><p>那么事实上算到这里，只要将上面的向量集合和奇异值排列好，就完成了矩阵<span class="math inline">\(A\)</span>的SVD分解。也就是<span class="math display">\[A = \begin{bmatrix} \frac{2}{\sqrt{5}} &amp; \frac{-1}{\sqrt{5}} \\ \frac{1}{\sqrt{5}} &amp; \frac{2}{\sqrt{5}} \end{bmatrix} \begin{bmatrix} \sqrt{6} &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \begin{bmatrix} \frac{1}{\sqrt{30}} &amp; \frac{2}{\sqrt{5}} &amp; \frac{1}{\sqrt{6}} \\ \frac{2}{\sqrt{30}} &amp; \frac{-1}{\sqrt{5}} &amp; \frac{2}{\sqrt{6}} \\ \frac{5}{\sqrt{30}} &amp; 0 &amp; \frac{-1}{\sqrt{6}} \end{bmatrix}^{\top}\]</span></p><p>那么SVD跟PCA是有非常多相似的地方的，如果我们使用了全部的奇异值，那么我们就可以还原原来的矩阵，但是如果我们只取了前面的一部分奇异值，我们得到的就是一个损失了一部分信息的矩阵。SVD在机器学习的领域有非常多的应用，最常用的一个地方就是用在推荐算法里面，另外就是降维。此外，还有一个矩阵分解方法是NMF，解释性会更强一些。这个在之前机器学习的博客里面也有提到。</p><p>然后是PageRank。这个算法缔造了今天的谷歌，也被称作是最贵的eigen value。PageRank实际上是一个蛮复杂的模型，这里讲一个最简单的情况，后面找机会再认真学习一下。所以这里有一些矩阵分析里面的定理（虽然我也不是很懂）就直接记结论，证明过程以后再学吧。</p><p>首先我们假设这个世界上只有四个网页，他们的关系如下：</p><p><img data-src='https://i.imgur.com/kpsOeB5.png'></p><p>现在假设有一个人随机浏览网页，他到每一个网站的可能性都是一样的，那么根据上图的结果我们可以得到： <span class="math display">\[\begin{align}x_1 &amp; = x_3 + \frac{1}{2} x_4 \\x_2 &amp; = \frac{1}{3} x_1 \\x_3 &amp; = \frac{1}{3} x_1 + \frac{1}{2} x_2 + \frac{1}{2} x_4 \\x_4 &amp; = \frac{1}{3} x_1 + \frac{1}{2} x_2\end{align}\]</span></p><p>所以我们就可以很简单得到这样一个矩阵<span class="math inline">\(A = \begin{bmatrix}0 &amp; 0 &amp; 1 &amp; \frac{1}{2} \\ \frac{1}{3} &amp; 0 &amp; 0 &amp; 0 \\ \frac{1}{3} &amp; \frac{1}{2} &amp; 0 &amp; \frac{1}{2} \\ \frac{1}{3} &amp; \frac{1}{2} &amp; 0 &amp; 0\end{bmatrix}\)</span>。这样的矩阵我们叫做马尔科夫矩阵，或者叫转移矩阵。这种矩阵的特点是每一个行的和为1，或者每一个列的和为1。根据<a href="https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem">Perron-Frobenius theorem</a>这样的矩阵必然有一个特征值为1。</p><p>所以PageRank实际上在做的事情就是计算<span class="math inline">\(A\)</span>的特征根为1时候的特征向量。这个特征向量最后就是我们的网页排名。</p><p>如果想要对PageRank有多一点的了解可以上Wikipedia看一下<a href="https://en.wikipedia.org/wiki/PageRank">PageRank的页面</a>，也可以直接看原来的<a href="http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf">论文</a>。另外有中文的这篇博客<a href="http://blog.codinglabs.org/articles/intro-to-pagerank.html">http://blog.codinglabs.org/articles/intro-to-pagerank.html</a>，介绍比较全面，不过基本上没有数学证明过程。看看以后有没有空自己推导一遍，顺便Python实现一下。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;两个线性代数里面在机器学习领域大放异彩的算法，SVD和PageRank。&lt;/p&gt;</summary>
    
    
    
    <category term="统计学习" scheme="https://samaelchen.github.io/categories/StatisticalLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>线性代数 12</title>
    <link href="https://samaelchen.github.io/linear_algebra_step12/"/>
    <id>https://samaelchen.github.io/linear_algebra_step12/</id>
    <published>2018-07-15T16:00:00.000Z</published>
    <updated>2021-08-31T16:15:25.578Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>稍微介绍一下两种特殊的矩阵，orthogonal matrix和symmetric matrix。</p><span id="more"></span><p>orthogonal matrix其实就是矩阵里面每个向量相互独立的矩阵，如果是orthonormal的话，这些矩阵里的向量都是单位向量。比如说<span class="math inline">\(\begin{bmatrix}\frac{1}{\sqrt{2}} &amp; 0 &amp; \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} &amp; 0 &amp; \frac{-1}{\sqrt{2}} \\ 0 &amp; 1 &amp; 0 \end{bmatrix}\)</span>。</p><p>这样的矩阵有一些特性，首先，orthogonal matrix <span class="math inline">\(Q\)</span>的transpose和inverse相等。也就是<span class="math inline">\(Q^{\top} = Q^{-1}，且这两个矩阵都是orthogonal的\)</span>，另外，<span class="math inline">\(\det(Q) = \pm 1\)</span>。最后，orthogonal matrix和orthogonal matrix叉乘之后还是orthogonal matrix。</p><p>orthogonal matrix还有一个很特殊的特性，就是向量和orthogonal matrix相乘以后，向量的norm不变。</p><p>另一种特殊矩阵是symmetric matrix，也就是类似<span class="math inline">\(\begin{bmatrix}a &amp; b \\ b &amp; c \end{bmatrix}\)</span>。首先，symmetric matrix一定有实特征根。其次，symmetric matrix一定有orthogonal eigenvectors。最后，symmetric matrix一定是diagonalizable的。这里存在一个等价关系<span class="math inline">\(A \text{ is symmetric等价于} P^{\top}AP = D 或 A = PDP^{\top}\)</span>。而这的<span class="math inline">\(P\)</span>包含<span class="math inline">\(A\)</span>的特征向量，<span class="math inline">\(D\)</span>是<span class="math inline">\(A\)</span>的特征根组成的对角矩阵。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;稍微介绍一下两种特殊的矩阵，orthogonal matrix和symmetric matrix。&lt;/p&gt;</summary>
    
    
    
    <category term="统计学习" scheme="https://samaelchen.github.io/categories/StatisticalLearning/"/>
    
    
  </entry>
  
  <entry>
    <title>经济思想史读书笔记——重商主义、重农主义及其他先驱</title>
    <link href="https://samaelchen.github.io/economics_thoughts_notes3/"/>
    <id>https://samaelchen.github.io/economics_thoughts_notes3/</id>
    <published>2018-07-11T16:00:00.000Z</published>
    <updated>2021-08-31T16:15:25.577Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>值得注意的是，发明家们并不具备被称为“科学态度”的客观性。——威廉·莱特温</p><span id="more"></span><p>1600年至1750年的150年间，经济活动极大增加，文艺复兴完成了资本主义的萌芽，教会的权威极大被削弱，这些为后来的工业革命的爆发奠定了基础。</p><p>这一时期，经济思想从简单的个人、家庭、生产者的观点，向更复杂的将经济体视为有其自身规律与相互关系的一个系统的观点演进。</p><h1 id="重商主义">重商主义</h1><p>重商主义的主要贡献出自英国人和法国人之手。经院哲学主要来自中世纪牧师，而重商主义的经济理论主要来自商人。重商主义的理论更像是个人的智力反应，而不是一个成体系的理论框架。</p><p>重商主义时期，由于领地减少，名族国家增多，重商主义理论试图确定能够推动国家权利与财富增加的最佳政策。</p><p>重商主义者的理论是基于世界总财富不变的假设下入手的。经院哲学在这个假设下论证了个人的财富获取，必定伴随另一个人的失去。重商主义将这个观点推广到了国家之间。因此，在他们看来，一个国家的财富是依靠很多国家的贫困来支撑的。重商主义者强调，国际贸易是增加一国财富和权力的一种手段，并且特别集中研究国家之间的贸易平衡问题。</p><p>大多数重商主义者认为，经济活动的目的是生产（古典经济学认为是消费）。因此，他们为了在国际贸易中保持优势，提倡低工资、低消费。乍一听，这不就是拥有劳动力优势的发展中国家的情况么。在重商主义的理论下，国民的贫困将使国家受益。所以在重商主义的思想下，为了实现贸易顺差，一国应该通过关税、配额、津贴和税收等手段来鼓励出口、限制进口；应当通过政府干预国内经济，以及通过对外贸易规则来刺激生产；应当对从国外进口的制造业产品征收保护性关税；应当鼓励进口用于制造出口产品的廉价原材料。诶，是不是很像某国现在在搞的贸易战的样子。</p><p>重商主义的货币观点是，一国的财富等同于国内持有的贵金属存量。他们深信，货币因素而非实际因素是经济活动与经济增长的首要决定因素。后面看到古典经济学的时候再做一个具体的比较。</p><p>从现在的角度来看，重商主义者被利益驱动，利用政府为他们自身获取经济特权。他们通常是商人，支持政府允许垄断，使得商人垄断者可以索要比没有垄断时更高的价格。</p><p>重商主义者最重要的成就或许是认可了对经济体进行分析的可能性。他们意识到了经济体中非常机械的因果关系，并且相信如果一个人弄懂了这些关系的规则就能控制经济体。他们认为政府干预可以达到既定的目的，但是不能随便干预使得一些基本的经济原理变得复杂。后期的重商主义意识到早期重商主义的很多理论不足，例如硬币不能代表一国的财富；对于所有国家不可能存在一种贸易顺差等。后期重商主义出现了早期古典自由主义的观点。但是古典主义和重商主义还是存在一个重要的差别，那就是重商主义认为私人利益和公关福利之间存在根本的冲突，而古典主义经济学家认为公共利益是个人追求自我利益自然而然的结果。</p><h1 id="重农主义">重农主义</h1><p>重农主义主要在18世纪的法国兴起。重农主义的著作与重商主义不同的是，重农主义有显著的一致性。重农主义的发展是短时间在法国集中出现的，而且有共同的知识领袖——弗朗索瓦·魁奈。</p><p>重农主义认为存在自然法则支配着经济体的运作，这些法则独立于人类意志，而人类可以客观发现它们。他们觉得，有必要通过分离主要经济变量来构建理论模型。重农主义并不集中研究货币，而是重点研究导致经济发展的实际力量上。相对于重商主义认为的财富源于交换过程的观点，重农主义推断财富起源于农业或者自然。</p><p>因为重农主义发展的时代，生产的产品用于支付实际生产成本之后，产生了剩余。对这种剩余的探索让他们形成了净产品的概念。根据他们的观点，劳动只能生产出支付劳动成本的产品，只有土地例外。因此土地的生产产生了净产品，而其他的非农业活动不能产生净产品。所以重农主义者集中注意力于物质生产力而不是价值生产力。</p><p>重农主义的理论巅峰是魁奈的《经济表》。事实上，这个表格的价值流动，看上去很像“投入——产出表”。经济表证明了经济体不同部门之间相互依赖的存在。</p><p>在经济政策上，重农主义者认为存在一种比人类设计的秩序更优的自然秩序，所以政府的任务是实行自由放任的政策。他们推断自由竞争将会导致最优价格；如果每个个体都追求自己的私利，那么社会将从中获利。</p><p>事实上，重商主义者发现净产品的源泉是交换，尤其是国家贸易形式的交换，因此他们提倡贸易顺差。而重农主义者则认为净产品源自农业，因此主张放任自由会引发农业生产的增加，最终引起更大的经济增长。</p><h1 id="其他思想先驱">其他思想先驱</h1><h2 id="托马斯孟">托马斯·孟</h2><p>托马斯·孟是一个主要的重商主义者，但是他的观点跟原始的重商主义不一样。作为一个英国人，他指出，尽管与所有其他国家实现了贸易顺差是合意的，贵金属流出到其他国家是不合意的。但是在与印度的贸易逆差以及出口贵金属到印度却有利于英国，因为这些实践扩大了英国与所有国家的贸易平衡，增加了金银的流入。虽然托马斯·孟是一个重商主义者，但是他以及看到了早期重商主义范例的严重错误。</p><h2 id="威廉配第">威廉·配第</h2><p>配第是第一个提倡测量经济变量的经济学家。配第最早明确提倡用我们所谓的统计方法来度量社会现象。他设法度量一国的人口、国民收入、出口、进口、资本量。尽管配第对统计学的早期应用显得有些原始，但是，他所代表的方法论立场却具有一种世系，这种世系始于他所处时代的经验归纳，止于当代经济学期刊上盛行的计量经济学线代应用。</p><h2 id="伯纳德曼德维尔">伯纳德·曼德维尔</h2><p>曼德维尔发现世界是邪恶的，但主张“在一个熟练政治家的灵巧管理下，私人恶习有可能变成公众利益”。重商主义的信仰具体表现为对产品的恐惧，对生产过剩与消费不足的关注。个人储蓄并不受欢迎，因为它会引发更低的消费、更低的产量，以及更低的就业。曼德维尔是一个纯粹的重商主义者，他坚决主张政府管制对外贸易，从而保证出口总是超过进口。因为社会的目标是生产，所以曼德维尔甚至主张大量拥有人口和童工，并谴责懒惰。他注意到了一条向下倾斜的劳动力供给曲线。根据曼德维尔的观点，较高的工资将减少劳动供给。他的主要观点就是应当接受满身恶习的人类，并通过规则和制度将其引导到社会利益上来。</p><h2 id="大卫休谟">大卫·休谟</h2><p>休谟被称作自由的重商主义者。他认为一个经济体的经济活动水平取决于货币数量及其周转速度，并对一国的贸易平衡、货币数量以及价格总水平之间的关系做出了相当完整的描述。<strong>黄金流动价格机制</strong>被认为是休谟在国际贸易理论中的重要贡献。</p><p>休谟不是一个纯粹的重商主义者在于，他指出，一个经济体不可能持续保持贸易顺差。贸易顺差将导致经济体内金银的增加。货币增加将使得具有贸易顺差的经济体价格上升。那么具有贸易逆差的经济体就会货币减少，价格下降。那么在这样的情况下，原来具有贸易顺差的经济体出口就会减少，进口就会增加。逆差的经济体则相反。这一过程最终将会导致贸易平衡的自动调整。</p><p>休谟认为，尽管一国的货币绝对量不能影响实际产量，但是货币供给的逐渐增加将会引起产量的增加。这一点，休谟并没有跳出重商主义的框架。</p><p>最后休谟主张经济自由，他认为经济自由和政治自由的增长是结合在一起的。</p><h2 id="理查德坎蒂隆">理查德·坎蒂隆</h2><p>理查德·坎蒂隆是部分重商主义，部分重农主义，以及部分的重农主义-古典学派。坎蒂隆通过推理的过程来建立经济学基本原理，并试图收集数据，并在检验原理的过程中加以使用。他最具有影响力的见解是关于市场体制的，该体制通过个人私利这一媒介来协调生产者和消费者的活动。直观感受上，非常接近完全竞争市场。</p><p>他总是倾向于将任何经济成分当做是一个完整结构的一部分。例如，在他的体制中人口变化是内生的，而不是外生的。他区分了由短期因素决定的市场价格与他所谓的内在价值即长期均衡价格。他最熟练的技术分析主要在宏观经济学中，即货币供给变化对价格和生产的影响。他将经济体划分为部门，分析收入在部门之间的流动。他注意到，新的资金进入经济体，价格总水平可能改变，但是相对价格也可能改变，并对经济体的不同部门产生影响。</p><h1 id="西班牙思想">西班牙思想</h1><p>因为地理大发现带来了大量的金银，黄金大量流入西班牙，西班牙国内的价格水平上升，西班牙的知识分子开始评价这些迅速变换的经济现象。在西班牙思想中，货币数量理论表明，货币价值即货币购买力是由流通中的货币数量决定的。其中，路易斯·摩里纳对于市场机制的描述就是我们今天的需求与供给定理，以及货币数量理论。</p><blockquote><p>……产品短缺，促使公平价格上升……丰裕使得公平价格下降。进入市场的购买者的数量在一些时候比另一些时候多一些，他们热切的购买愿望引起价格上升……一个地方缺少货币，会导致其他物品价格下降，货币充裕则会使价格上升……</p></blockquote><h1 id="小结">小结</h1><p>重商主义者和重农主义者都认为经济体可以被正式地加以研究，并且发展了一种抽象方法来发觉能够调节经济体的法则。他们第一次让经济理论立足于抽象的模型构建过程。</p><p>重商主义者就货币在确定价格总水平中的作用，以及对外贸易平衡对国内经济活动的影响方面取得了最初的尝试性见解。重农主义者则是提出了经济体不同部门相关性的概念。在面对经济体的基本冲突上，重商主义者和经院哲学都提倡，要么借助政府，要么借助教会，对经济体施加干预。但是重农主义者则认识到利益冲突的结果基本上是协调的，是相对稀缺性所固有的，他们不提倡政府干预，而是提倡自由放任。</p><p>这一时期的一些英国经济学家既不完全符合重商主义，也不完全符合古典阵营。他们否决了交换中的固有冲突这一较为原始的重商主义观点；反驳了永远保持有利贸易平衡的必要性；也正是他们了解了市场是如何运作来调整个别经济活动的。</p><p>这些重要的思想者没能完成理论的大一统，这就是未来亚当·斯密的任务。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;值得注意的是，发明家们并不具备被称为“科学态度”的客观性。——威廉·莱特温&lt;/p&gt;</summary>
    
    
    
    <category term="读书笔记" scheme="https://samaelchen.github.io/categories/ReadingNotes/"/>
    
    
  </entry>
  
</feed>
