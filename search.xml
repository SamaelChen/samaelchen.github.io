<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>LSTM的PyTorch实现</title>
    <url>/LSTM-pytorch/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>基于PyTorch的LSTM实现。</p>
<span id="more"></span>
<p>PyTorch封装了很多常用的神经网络，要实现LSTM非常的容易。这里用官网的实例修改实现练习里面的character level LSTM。</p>
<p>首先还是老样子，import需要的module： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">1</span>)</span><br></pre></td></tr></table></figure></p>
<p>然后为了将数据放到网络里面，我们需要做一个编码单词的函数： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_char_sequence</span>(<span class="params">word, to_ix</span>):</span></span><br><span class="line">    idxs = [to_ix[char] <span class="keyword">for</span> char <span class="keyword">in</span> word]</span><br><span class="line">    <span class="keyword">return</span>(torch.tensor(idxs, dtype=torch.long))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_sequence</span>(<span class="params">seq, to_ix</span>):</span></span><br><span class="line">    idxs = [to_ix[w] <span class="keyword">for</span> w <span class="keyword">in</span> seq]</span><br><span class="line">    char_idxs = [prepare_char_sequence(w, char_to_ix) <span class="keyword">for</span> w <span class="keyword">in</span> seq]</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(idxs, dtype=torch.long), char_idxs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">training_data = [</span><br><span class="line">    (<span class="string">&quot;The dog ate the apple&quot;</span>.split(), [<span class="string">&quot;DET&quot;</span>, <span class="string">&quot;NN&quot;</span>, <span class="string">&quot;V&quot;</span>, <span class="string">&quot;DET&quot;</span>, <span class="string">&quot;NN&quot;</span>]),</span><br><span class="line">    (<span class="string">&quot;Everybody read that book&quot;</span>.split(), [<span class="string">&quot;NN&quot;</span>, <span class="string">&quot;V&quot;</span>, <span class="string">&quot;DET&quot;</span>, <span class="string">&quot;NN&quot;</span>])</span><br><span class="line">]</span><br><span class="line">word_to_ix = &#123;&#125;</span><br><span class="line">char_to_ix = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> sent, tags <span class="keyword">in</span> training_data:</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sent:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">not</span> <span class="keyword">in</span> word_to_ix:</span><br><span class="line">            word_to_ix[word] = <span class="built_in">len</span>(word_to_ix)</span><br><span class="line">        <span class="keyword">for</span> char <span class="keyword">in</span> word:</span><br><span class="line">            <span class="keyword">if</span> char <span class="keyword">not</span> <span class="keyword">in</span> char_to_ix:</span><br><span class="line">                char_to_ix[char] = <span class="built_in">len</span>(char_to_ix)</span><br><span class="line"><span class="built_in">print</span>(word_to_ix)</span><br><span class="line"><span class="built_in">print</span>(char_to_ix)</span><br><span class="line"></span><br><span class="line">tag_to_ix = &#123;<span class="string">&quot;DET&quot;</span>: <span class="number">0</span>, <span class="string">&quot;NN&quot;</span>: <span class="number">1</span>, <span class="string">&quot;V&quot;</span>: <span class="number">2</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># These will usually be more like 32 or 64 dimensional.</span></span><br><span class="line"><span class="comment"># We will keep them small, so we can see how the weights change as we train.</span></span><br><span class="line">WORD_EMBEDDING_DIM = <span class="number">5</span></span><br><span class="line">CHAR_EMBEDDING_DIM = <span class="number">5</span></span><br><span class="line">CHAR_HIDDEN_DIM = <span class="number">3</span></span><br><span class="line">WORD_HIDDEN_DIM = <span class="number">6</span></span><br></pre></td></tr></table></figure></p>
<p>其实这里想想，如果是全文本，我们的character level编码也就26个字母表那么多。</p>
<p>然后我们定义一个character level的网络： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTMTagger</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, char_embedding_dim, word_embedding_dim, char_hidden_dim, word_hidden_dim, char_size, vocab_size, tagset_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LSTMTagger, self).__init__()</span><br><span class="line">        self.char_embedding_dim = char_embedding_dim</span><br><span class="line">        self.word_embedding_dim = word_embedding_dim</span><br><span class="line">        self.char_hidden_dim = char_hidden_dim</span><br><span class="line">        self.word_hidden_dim = word_hidden_dim</span><br><span class="line">        self.char_embeddings = nn.Embedding(char_size, char_embedding_dim)</span><br><span class="line">        self.word_embeddings = nn.Embedding(vocab_size, word_embedding_dim)</span><br><span class="line"></span><br><span class="line">        self.char_lstm = nn.LSTM(char_embedding_dim, char_hidden_dim)</span><br><span class="line">        self.word_lstm = nn.LSTM((word_embedding_dim + char_hidden_dim), word_hidden_dim)</span><br><span class="line"></span><br><span class="line">        self.hidden2tag = nn.Linear(word_hidden_dim, tagset_size)</span><br><span class="line">        self.char_hidden = self.init_hidden(self.char_hidden_dim)</span><br><span class="line">        self.word_hidden = self.init_hidden(self.word_hidden_dim)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span>(<span class="params">self, hidden_dim</span>):</span></span><br><span class="line">        <span class="keyword">return</span>(torch.zeros(<span class="number">1</span>, <span class="number">1</span>, hidden_dim),</span><br><span class="line">               torch.zeros(<span class="number">1</span>, <span class="number">1</span>, hidden_dim))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, sentence</span>):</span></span><br><span class="line">        char_lstm_result = []</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> sentence[<span class="number">1</span>]:</span><br><span class="line">            self.char_hidden = self.init_hidden(self.char_hidden_dim)</span><br><span class="line">            char_embeds = self.char_embeddings(word)</span><br><span class="line">            lstm_char_out, self.char_hidden = self.char_lstm(char_embeds.view(<span class="built_in">len</span>(word), <span class="number">1</span>, -<span class="number">1</span>), self.char_hidden)</span><br><span class="line">            char_lstm_result.append(lstm_char_out[-<span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">        word_embeds = self.word_embeddings(sentence[<span class="number">0</span>])</span><br><span class="line">        char_lstm_result = torch.stack(char_lstm_result)</span><br><span class="line">        lstm_in = torch.cat((word_embeds.view(<span class="built_in">len</span>(sentence[<span class="number">0</span>]), <span class="number">1</span>, -<span class="number">1</span>), char_lstm_result), <span class="number">2</span>)</span><br><span class="line">        lstm_out, self.hidden = self.word_lstm(lstm_in, self.word_hidden)</span><br><span class="line">        tag_space = self.hidden2tag(lstm_out.view(<span class="built_in">len</span>(sentence[<span class="number">0</span>]), -<span class="number">1</span>))</span><br><span class="line">        tag_scores = F.log_softmax(tag_space, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> tag_scores</span><br></pre></td></tr></table></figure></p>
<p>在forward部分可以看到，这里有两个LSTM。第一个LSTM做的事情是将character拼成word，相当于是返回了一个character level的word embedding。然后用这个embedding和直接embedding的word vector拼到一起，放到第二个LSTM里面训练词性标注。另外要注意的是，这里虽然有两个LSTM模型，但是我们并没有定义第一个LSTM的loss function。因为我们要让这个网络按照最后词性标注的效果来训练，因此我们不需要定义这个网络的loss function。</p>
<p>定义一下相关的参数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = LSTMTagger(CHAR_EMBEDDING_DIM, WORD_EMBEDDING_DIM, CHAR_HIDDEN_DIM, WORD_HIDDEN_DIM, <span class="built_in">len</span>(char_to_ix), <span class="built_in">len</span>(word_to_ix), <span class="built_in">len</span>(tag_to_ix))</span><br><span class="line">loss_function = nn.NLLLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<p>看一下训练前的输出结果是什么： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    inputs = prepare_sequence(training_data[<span class="number">0</span>][<span class="number">0</span>], word_to_ix)</span><br><span class="line">    tag_scores = model(inputs)</span><br><span class="line">    <span class="built_in">print</span>(tag_scores)</span><br></pre></td></tr></table></figure></p>
<p>再看一下训练300轮之后的结果： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">300</span>):</span><br><span class="line">    <span class="keyword">for</span> sentence, tags <span class="keyword">in</span> training_data:</span><br><span class="line">        model.zero_grad()</span><br><span class="line">        model.hidden = model.init_hidden(WORD_EMBEDDING_DIM)</span><br><span class="line">        sentence_in = prepare_sequence(sentence, word_to_ix)</span><br><span class="line">        targets = prepare_char_sequence(tags, tag_to_ix)</span><br><span class="line">        tag_scores = model(sentence_in)</span><br><span class="line"></span><br><span class="line">        loss = loss_function(tag_scores, targets)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    inputs = prepare_sequence(training_data[<span class="number">0</span>][<span class="number">0</span>], word_to_ix)</span><br><span class="line">    tag_scores = model(inputs)</span><br><span class="line">    <span class="built_in">print</span>(tag_scores)</span><br></pre></td></tr></table></figure></p>
<p>一般而言，character level的LSTM会比word level的更有效果。这里因为是一个toy级别的，看不出太显著的差别来。如果是海量数据，一般而言会有比较明显的效果。</p>
<p>另外，原来的example是单向的LSTM，这里顺便做一个双向的。其实双向的LSTM就是正向一个，反向再一个，所以hidden的部分是两倍。所以要修改的地方就是网络的定义，将单向LSTM的hidden乘以2就好了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LSTMTagger</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, embedding_dim, hidden_dim, vocab_size, tagset_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(LSTMTagger, self).__init__()</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line"></span><br><span class="line">        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        self.hidden2tag = nn.Linear(hidden_dim * <span class="number">2</span>, tagset_size)</span><br><span class="line">        self.hidden = self.init_hidden()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span>(torch.zeros(<span class="number">1</span> * <span class="number">2</span>, <span class="number">1</span>, self.hidden_dim),</span><br><span class="line">               torch.zeros(<span class="number">1</span> * <span class="number">2</span>, <span class="number">1</span>, self.hidden_dim))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, sentence</span>):</span></span><br><span class="line">        embeds = self.word_embeddings(sentence)</span><br><span class="line">        lstm_out, self.hidden = self.lstm(</span><br><span class="line">            embeds.view(<span class="built_in">len</span>(sentence), <span class="number">1</span>, -<span class="number">1</span>), self.hidden)</span><br><span class="line">        tag_space = self.hidden2tag(lstm_out.view(<span class="built_in">len</span>(sentence), -<span class="number">1</span>))</span><br><span class="line">        tag_scores = F.log_softmax(tag_space, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> tag_scores</span><br></pre></td></tr></table></figure>
<p>这样就简单实现了一个toy级别的双向LSTM和character level的单向LSTM。</p>
]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
  </entry>
  <entry>
    <title>DBSCAN &amp; OPTICS</title>
    <url>/dbscan_and_optics/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>我如期来更新啦！！！聚类算法是很常用的一种算法，不过最常见的就是KMeans了，虽然很多人都会用，不过讲道理，如果是调用现成机器学习库里面的KMeans的话，我敢保证90%的人答不上来具体的是什么算法。相信我，库里的KMeans跟教科书讲的那个随机取初始点的KMeans不是一个算法哟～</p>
<p>因为KMeans依赖K，但是我怎么知道K要用多少呢？另外，KMeans受限于算法本身，对于球状的数据效果较好，但是不规则形状的就不行了。这种情况下，相对而言，基于密度的聚类算法就比较好用了。sklearn里面现在是放了一个DBSCAN，下一版会更新OPTICS。刚好最近都用了，这里把DBSCAN跟OPTICS算法复现一遍。</p>
<span id="more"></span>
<h1 id="dbscan">DBSCAN</h1>
<p>DBSCAN算法的原始论文是96年的这篇<a href="https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf">《A Density-Based Algorithm for Discovering Clusters in Large Spatial Database with Noise》</a>。</p>
<p>DBSCAN是一种基于密度的聚类算法，也就是说，密度高的区域自动聚成一类。这样一来，我们就避免了人为去设定群组数量的问题，算法可以自动发现群组数量。另外用这种方法，如果一个sample不在高密度区域，就有可能被判定为异常值，那么也可以拿来作为异常值检验的方法。</p>
<p>DBSCAN的思路非常简单，有两个参数，一个是<span class="math inline">\(\varepsilon\)</span>，另一个是minimum points。这里首先定义DBSCAN的几个核心概念，一个是<span class="math inline">\(\varepsilon\)</span>-neighborhood，另一个是core object，还有就是reachable。</p>
<p>首先是<span class="math inline">\(\varepsilon\)</span>-neighborhood。DBSCAN开始的时候会随机选取一个初始点，然后按照<span class="math inline">\(\varepsilon\)</span>的距离寻找临近点，这些点的集合叫做<span class="math inline">\(\varepsilon\)</span>-neighborhood。参数里面的minimum points就是限定这个点的集合的。minimum points限定了这个集合最小需要包含多少样本点，<span class="math inline">\(\varepsilon\)</span>则是限定了要用多大的范围去框定这些样本点。这里有个小细节要注意，那就是，算neighborhood的时候，中心点自己是算进来的。</p>
<p>那么这样圈完一波neighborhood后，我们会将符合样本数量大于等于minpts的中心点叫做core object。而core object跟这些neighbor就是reachable的。</p>
<p>然后为了让算法运作起来，只要neighborhood这个集合里面有点，我们就不断重复这样圈地的动作，然后把中心点从集合中拿掉，直到neighborhood为空。</p>
<p>那我们很自然就会想到，一定会圈到一些点，它们不是core object，但是也在集合里面。这些点我们叫做border，也就是说，这些点是这个类的边界了。那么我们就很自然会想到，也有一些点压根不在neighborhood里面，也不是core的点，这些点就是noise。那既然样本点多了两个状态，reachable的情况也就变得多了，如果是直接可以在neighborhood里面找到的，我们叫做directly-reachable；如果通过neighborhood一层层找，最后找到的，我们叫density-reachable。</p>
<p>可以看图说话：</p>
<p align="center">
<img data-src='https://upload.wikimedia.org/wikipedia/commons/thumb/a/af/DBSCAN-Illustration.svg/1280px-DBSCAN-Illustration.svg.png' width=50%>
</p>
<p>这个图里面，A是core，B、C是border，N是noise。A跟B、C都是density-reachable。但是有没有发现，B是没法返回去找到A的。所以这种reachable是有方向的。</p>
<p>如果对SNA有点了解的朋友就知道，就是一度人脉和N度人脉，但是是一个有向图。</p>
<p>那么算法思路理清了，代码就好写了，这里我就用最常用的欧氏距离了。有人可能会想，如果还是欧氏距离，那跟KMeans还有什么分别，都是画圈圈嘛！请回想一个微积分，只要圈画的够小，就能做出各种形状来。下面是核心部分的代码，详细的可以去看我的<a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/fun/DBSCAN.ipynb">notebook</a>。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_euclidean_dist</span>(<span class="params">p, q</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.sqrt(np.power(p - q, <span class="number">2</span>).<span class="built_in">sum</span>())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_eps_reachable</span>(<span class="params">p, q, eps</span>):</span></span><br><span class="line">    <span class="keyword">return</span> _euclidean_dist(p, q) &lt; eps</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_eps_neighborhood</span>(<span class="params">data, point_id, eps</span>):</span></span><br><span class="line">    neighborhood = []</span><br><span class="line">    point = data[point_id]</span><br><span class="line">    <span class="keyword">for</span> q <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(data)):</span><br><span class="line">        <span class="keyword">if</span> _eps_reachable(point, data[q], eps):</span><br><span class="line">            neighborhood.append(q)</span><br><span class="line">    <span class="keyword">return</span> neighborhood</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_is_core_object</span>(<span class="params">neighborhood, minpts</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(neighborhood) &gt;= minpts:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dbscan</span>(<span class="params">data, eps, minpts</span>):</span></span><br><span class="line">    class_id = <span class="number">0</span></span><br><span class="line">    class_label = np.full(<span class="built_in">len</span>(data), -<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> p_id <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(data)):</span><br><span class="line">        <span class="keyword">if</span> class_label[p_id] == -<span class="number">1</span>:</span><br><span class="line">            neighborhood = _eps_neighborhood(data, p_id, eps)</span><br><span class="line">        <span class="keyword">if</span> _is_core_object(neighborhood, minpts):</span><br><span class="line">            class_label[neighborhood] = class_id</span><br><span class="line">            neighborhood.remove(p_id)</span><br><span class="line">            <span class="keyword">while</span> <span class="built_in">len</span>(neighborhood) &gt; <span class="number">0</span>:</span><br><span class="line">                current_p = neighborhood[<span class="number">0</span>]</span><br><span class="line">                current_neighborhood = _eps_neighborhood(data, current_p, eps)</span><br><span class="line">                <span class="keyword">for</span> n <span class="keyword">in</span> current_neighborhood:</span><br><span class="line">                    <span class="keyword">if</span> class_label[n] == -<span class="number">1</span>:</span><br><span class="line">                        class_label[n] = class_id</span><br><span class="line">                        neighborhood.append(n)</span><br><span class="line">                neighborhood = neighborhood[<span class="number">1</span>:]</span><br><span class="line">            class_id += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> class_label</span><br></pre></td></tr></table></figure>
<h1 id="optics">OPTICS</h1>
<p>那么DBSCAN本身是一个非常牛逼的算法，它解决了我们找K的问题，这样在海量群组的时候，我们不用像KMeans一样去到处尝试K的大小。但是DBSCAN有个问题，那就是这个算法只能检测一个密度。换句话说，如果现在存在一个数据集有两个类，一个类是方差小的，一个类是方差大的。且这两个群组离得不算太远。如果我们为了照顾方差大的群组将eps设得很大，minpts设得很小，那么可能把两个类聚在一起。反过来，我们就可能找不到方差大的类。</p>
<p>那么问题来了，有没有办法量化这个距离呢？三年后，同一组作者在DBSCAN的基础上进化出了<a href="http://www.dbs.ifi.lmu.de/Publikationen/Papers/OPTICS.pdf">OPTICS算法</a>。</p>
<p>既然是DBSCAN的进化版，所以很多概念上都是互通的，只是OPTICS算法多了几个概念，一个是core-distance，一个是reachability-distance。</p>
<p>我们知道，DBSCAN是不断跑马圈地的一个过程，但是我们很直观想就知道，有些密度大的地方，可能不需要<span class="math inline">\(\varepsilon\)</span>那么大的范围就可以圈到minpts个样本，所以在OPTICS算法里面，我们将满足minpts这么多样本点的<span class="math inline">\(\varepsilon&#39;\)</span>叫做core-distance。而reachability-distance就是中心点与临近点的距离，但是，如果临近点落在<span class="math inline">\(\varepsilon&#39;\)</span>内，reachability-distance就用core-distance来替代。如下图：</p>
<p align="center">
<img data-src='https://github.com/SamaelChen/samaelchen.github.io/raw/hexo/images/blog/optics_001.png' width=50%>
</p>
<p>那么用OPTICS的时候我们就需要定义两个列表，一个是seeds，一个是ordered result。seeds就是我们每一轮迭代时候的候选列表，而ordered result就是最终的结果。</p>
<p>具体的过程是这样的。我们先找到一个点，然后一样跑马圈地，接着计算reachability-distance，然后放到seeds里面从小到大排序。每次取第一个seed出来继续圈地，把被取出来的点以及这个点的reachability distance存在ordered result里面。接着就跟DBSCAN一样，不断重复，直到neighborhood为空。这样做的好处就是，我们可以量化评估每个群的密度大小。效果如下图：</p>
<p align="center">
<img data-src='https://github.com/SamaelChen/samaelchen.github.io/raw/hexo/images/blog/optics_002.jpg' width=50%>
</p>
<p>那么我们又会想到，有些seeds里面的点可能随着核心点的移动，reachability distance会不断变小。因为A的core distance里可能是B，而C不在A的core distance里面，但是C在B的core distance里面。如果第一个处理的点是A，第二个处理的点是B，那C其实还是很核心的一个点。那这种时候我们就要跟一它开始的reachability distance做比较，如果新的reachability distance比原来的小，就把原来的值替换掉。</p>
<p>那么废话不多说，上代码，很多地方跟DBSCAN是可以复用的，我就放了一些OPTICS的核心部分，老规矩，详细见<a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/fun/OPTICS.ipynb">notebook</a>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_eps_neighborhood</span>(<span class="params">data, point_id, eps</span>):</span></span><br><span class="line">    neighborhood = []</span><br><span class="line">    rdist = []</span><br><span class="line">    point = data[point_id]</span><br><span class="line">    <span class="keyword">for</span> q <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(data)):</span><br><span class="line">        dist = _euclidean_dist(point, data[q])</span><br><span class="line">        <span class="keyword">if</span> dist &lt; eps:</span><br><span class="line">            neighborhood.append((q, dist))</span><br><span class="line">    neighborhood = np.array(neighborhood, dtype=[(<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;i&#x27;</span>), (<span class="string">&#x27;dist&#x27;</span>, <span class="string">&#x27;f&#x27;</span>)])</span><br><span class="line">    neighborhood = np.delete(neighborhood, np.where(neighborhood[<span class="string">&#x27;id&#x27;</span>] == point_id))</span><br><span class="line">    <span class="keyword">return</span> neighborhood</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_update_order_seeds</span>(<span class="params">neighborhood, minpts, reach_dists, processed, seeds</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> _is_core_object(neighborhood, minpts):</span><br><span class="line">        <span class="keyword">return</span> seeds</span><br><span class="line">    core_dist = np.sort(neighborhood[<span class="string">&#x27;dist&#x27;</span>])[minpts - <span class="number">2</span>]</span><br><span class="line">    <span class="keyword">for</span> obj <span class="keyword">in</span> neighborhood[<span class="string">&#x27;id&#x27;</span>]:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> processed[obj]:</span><br><span class="line">            obj_dist = neighborhood[neighborhood[<span class="string">&#x27;id&#x27;</span>] == obj][<span class="string">&#x27;dist&#x27;</span>][<span class="number">0</span>]</span><br><span class="line">            newRdist = <span class="built_in">max</span>(core_dist, obj_dist)</span><br><span class="line">            <span class="keyword">if</span> np.isnan(reach_dists[obj]):</span><br><span class="line">                reach_dists[obj] = newRdist</span><br><span class="line">                seeds[obj] = newRdist</span><br><span class="line">            <span class="keyword">elif</span> newRdist &lt; reach_dists[obj]:</span><br><span class="line">                reach_dists[obj] = newRdist</span><br><span class="line">                seeds[obj] = newRdist</span><br><span class="line">    <span class="keyword">return</span> seeds</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optics</span>(<span class="params">data, eps, minpts</span>):</span></span><br><span class="line">    processed = np.array([<span class="literal">False</span>] * <span class="built_in">len</span>(data))</span><br><span class="line">    core_dists = np.full(<span class="built_in">len</span>(data), np.nan)</span><br><span class="line">    reach_dists = np.full(<span class="built_in">len</span>(data), np.nan)</span><br><span class="line">    ordered_res = []</span><br><span class="line">    seeds = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> p_id <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(data)):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> processed[p_id]:</span><br><span class="line">            ordered_res.append(p_id)</span><br><span class="line">            processed[p_id] = <span class="literal">True</span></span><br><span class="line">            neighbors = _eps_neighborhood(data, p_id, eps)</span><br><span class="line">            <span class="keyword">if</span> _is_core_object(neighbors, minpts):</span><br><span class="line">                core_dists[p_id] = np.sort(neighbors[<span class="string">&#x27;dist&#x27;</span>])[minpts - <span class="number">2</span>]</span><br><span class="line">            seeds = _update_order_seeds(neighbors, minpts, reach_dists, processed, seeds)</span><br><span class="line">            <span class="keyword">while</span> <span class="built_in">len</span>(seeds) &gt; <span class="number">0</span>:</span><br><span class="line">                nextId = <span class="built_in">sorted</span>(seeds.items(), key=operator.itemgetter(<span class="number">1</span>))[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">                <span class="keyword">del</span> seeds[nextId]</span><br><span class="line">                processed[nextId] = <span class="literal">True</span></span><br><span class="line">                ordered_res.append(nextId)</span><br><span class="line">                newNeighbors = _eps_neighborhood(data, nextId, eps)</span><br><span class="line">                <span class="keyword">if</span> _is_core_object(newNeighbors, minpts):</span><br><span class="line">                    core_dists[nextId] = np.sort(newNeighbors[<span class="string">&#x27;dist&#x27;</span>])[minpts - <span class="number">2</span>]</span><br><span class="line">                    seeds = _update_order_seeds(newNeighbors, minpts, reach_dists, processed, seeds)</span><br><span class="line">    <span class="comment"># 这里只是我的一个操作，强迫症看不惯有个nan存在。</span></span><br><span class="line">    reach_dists[ordered_res[<span class="number">0</span>]] = core_dists[ordered_res[<span class="number">0</span>]]</span><br><span class="line">    <span class="keyword">return</span> ordered_res, reach_dists, core_dists</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cluster_optics_dbscan</span>(<span class="params">data, eps_reachable, eps, minpts</span>):</span></span><br><span class="line">    orders, reach_dists, core_dists = optics(data, eps, minpts)</span><br><span class="line">    n_samples = <span class="built_in">len</span>(data)</span><br><span class="line">    labels = np.zeros(n_samples, dtype=<span class="built_in">int</span>)</span><br><span class="line">    far_reach = reach_dists &gt; eps_reachable</span><br><span class="line">    near_core = core_dists &lt;= eps_reachable</span><br><span class="line">    labels[orders] = np.cumsum(far_reach[orders] &amp; near_core[orders])</span><br><span class="line">    labels[far_reach &amp; ~near_core] = -<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> labels, orders, reach_dists, core_dists</span><br></pre></td></tr></table></figure>
<p>OPTICS的优点就是，不管是什么形状的密度，基本上都可以把这个凹槽给跑出来，但是问题就是最后的这个抽取群组小算法。目前我还没找到一个比较好的方法来自动抽取，如果是按照论文里面的分层抽取，我试过会抽的太细，如果是按照论文里面的DBSCAN来抽，就是我实现的这个，不过是一刀切的方式，太复杂的样本效果就不好了。目前还在探索用其他平滑方法来替代，有突破再来更新。</p>
<p>试了两种平滑方式，最后的做法是，先用一维高斯平滑，然后用max filter抹掉因为高斯平滑最大值向左漂的问题。</p>
<h1 id="hdbscan">HDBSCAN</h1>
<p>然后又发现了一个新的聚类算法，类似OPTICS算法，叫<a href="https://link.springer.com/chapter/10.1007/978-3-642-37456-2_14">HDBSCAN</a>，顾名思义，就是H就是hierarchical。事实上就是OPTICS算法的一个改进版本。</p>
<p>与OPTICS类似，HDBSCAN一样要算core distance和reachability distance。这个算法跟OPTICS是一样的。但是reachability distance的公式是：<span class="math inline">\(d_{\text{reach}-k}(a, b) = \max\{\text{core}_k(a), \text{core}_k(b), d(a, b)\}\)</span>，这里的<span class="math inline">\(d(a, b)\)</span>是两个点用距离公式算的距离。如下图所示：</p>
<p align="center">
<img data-src='https://nbviewer.jupyter.org/github/scikit-learn-contrib/hdbscan/blob/master/notebooks/distance4.svg' width=50%>
</p>
<p>蓝绿两点直接的reachability distance就是绿点的core distance，红绿两点的就是红绿两点的距离。</p>
<p>那么这样一来，就可以得到每个点之间的reachability distance。然后就可以用这个距离作为权重来绘制一个带权重的连接图。<a href="https://arxiv.org/pdf/1506.06422v2.pdf">Beyond Hartigan Consistency</a>这篇论文认为，无论是那种概率密度分布，这种相互距离可以更好的表示单链接聚类的层次结构。</p>
<p>然后可以用<a href="https://en.wikipedia.org/wiki/Prim%27s_algorithm">Prim算法</a>构建最小生成树，然后转化为层次结构，就类似层次聚类了。这个可以参考一下基于最小生成树的层次聚类。然后通过设定最小簇大小，对这个树进行剪枝。也就从下面的第一个图变成第二个图：</p>
<p align="center">
<img data-src='https://hdbscan.readthedocs.io/en/latest/_images/how_hdbscan_works_12_1.png' width=50%>
</p>
<p align="center">
<img data-src='https://hdbscan.readthedocs.io/en/latest/_images/how_hdbscan_works_15_1.png' width=50%>
</p>
<p>之后就是抽取簇。上面剪枝后的树其实已经有了结果，但是我们希望抽的簇可以自动抽取，且是稳定的。因此我们需要一种度量方式来衡量簇的稳定性。我们定义一个值<span class="math inline">\(f(x) = \lambda = \frac{1}{\text{distance}_{\text{core}}}\)</span>。这里的distance就是一个点。所以用<span class="math inline">\(\lambda_{\text{birth}}\)</span>和<span class="math inline">\(\lambda_{\text{death}}\)</span>分别表示簇生成和簇分裂时候的<span class="math inline">\(\lambda\)</span>大小，也就是<span class="math inline">\(\lambda_{\text{min}}C_i\)</span>和<span class="math inline">\(\lambda_{\text{max}}C_i\)</span>，那么簇的稳定性就是<span class="math inline">\(S = \sum_{p \in \text{cluster}} (\lambda_p - \lambda_{\text{birth}})\)</span>，<span class="math inline">\(\lambda_p = \lambda_{\text{max}}(x, C) = \min(f(x), \lambda_{\text{max}}C)\)</span>每轮优化的方向就是让这个稳定性最大，同时满足最小簇大小。</p>
<p>那么HDBSCAN跟DBSCAN比的话，DBSCAN实际上就是上面剪枝前的hierarchical tree切一刀，而HDBSCAN会自适应地去寻找合适的划分。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>台大李宏毅深度学习——常见神经网络结构</title>
    <url>/deep_learning_step1/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>深度学习入门——常见网络结构</p>
<span id="more"></span>
<h1 id="全连接网络">全连接网络</h1>
<p>深度学习讲到底其实就是个各种神经网络的变种。最基础的神经网络结构就是全连接层。全连接就是上一层的神经元都与下一层的神经元相互连接。这样的网络结构看上去就像下面的图一样：</p>
<p><img data-src='https://i.imgur.com/Lg9XkVz.png'></p>
<p>那出于方便的考虑，我们这样声明一个权重<span class="math inline">\(w_{ij}^l\)</span>这里的上标<span class="math inline">\(l\)</span>用来表示第<span class="math inline">\(l\)</span>层的神经元，而下标<span class="math inline">\(j\)</span>表示起始的位置，而<span class="math inline">\(i\)</span>表示结束的位置。这样的定义实际上是为了看起来方便。原因是，我们将<span class="math inline">\(z_1^l\)</span>拆开来看，可以表达为<span class="math inline">\(z^l_1 = w_{11}^{l}a^{l-1}_1 + w_{12}^{l}a^{l-1}_2 + \cdots + w_{1n}^{l}a^{l-1}_n\)</span>。如果我们将<span class="math inline">\(z^l_i\)</span>表示为一个向量，这样就能得到一个权重的矩阵。我们将这个表示如下： <span class="math display">\[
\begin{bmatrix}
z^l_1 \\
z^l_2 \\
\vdots \\
\end{bmatrix} =
\begin{bmatrix}
w^l_{11}, &amp;w^l_{12}, &amp;\cdots \\
w^l_{21}, &amp;w^l_{22}, &amp;\cdots \\
\vdots, &amp;\vdots, &amp;\ddots \\
\end{bmatrix}
\begin{bmatrix}
a^{l-1}_1 \\
a^{l-1}_2 \\
\vdots
\end{bmatrix} +
\begin{bmatrix}
b^l_1 \\
b^l_2 \\
\vdots
\end{bmatrix}
\]</span> 由于我们之前的下标定义方式是输入层在后，输出层在前，因此我们的权重矩阵看上去是这样的，那么这样我们将这个公式简写成上图中的格式的时候<span class="math inline">\(W^l\)</span>就不需要转置。如果下标定义跟上文的定义相反，采用输入层在右边，输出层在左边的方法，那么这里的权重矩阵就需要做一个转置。</p>
<h1 id="循环神经网络recurrent-neural-network">循环神经网络（Recurrent Neural Network）</h1>
<p>循环神经网络有多种多样的变形，最基本的深度循环神经网络的结构如下： <img data-src='https://i.imgur.com/oCwYYVp.png'> 一个循环神经网络由这样一个个的block组成。每一层的block用的是同样的function。每个function接受同样两个输入，同时有两个输出，表示为<span class="math inline">\(h, y = f(h, x)\)</span>。</p>
<p>一个深度循环神经网络是需要将上一轮的<span class="math inline">\(y\)</span>作为下一层的输入的，因此他们的dimension必须是一致的。</p>
<h2 id="naive-rnn">Naive RNN</h2>
<p>最简单的RNN结构就是上图的样子，每一个block有两个输入两个输出。计算的逻辑是： <span class="math display">\[
h&#39; = \sigma(W^h h + W^i x) \\
y = \sigma(W^o h&#39;)
\]</span> 这里如果我们需要输出概率，也可以将sigmoid激活函数改成softmax。另外这里的<span class="math inline">\(W^o\)</span>指的是output weight。</p>
<p>最简单的RNN结构也可以是双向的： <img data-src='https://i.imgur.com/XMQj4ve.png'></p>
<h2 id="lstm">LSTM</h2>
<p>LSTM是RNN的一个变种，也是目前主流的RNN基本结构。LSTM的结构比naive RNN复杂一些。简化的block如下图：</p>
<p><img data-src='https://i.imgur.com/6ovTum1.png'></p>
<p>LSTM之所以被叫做是有memory的网络，是因为这里的两个参数<span class="math inline">\(c和h\)</span>更新速度是不是一样的。</p>
<p><span class="math inline">\(c\)</span>的更新速度比较慢，通常<span class="math inline">\(c^t\)</span>就是<span class="math inline">\(c^{t-1}\)</span>加上某一个值，因此这里可以有很长时间的记忆。也就是long term的memory。</p>
<p>而<span class="math inline">\(h\)</span>的更新速度比较快，前后两个阶段的<span class="math inline">\(h\)</span>可以毫无关系。因此这里就是short term的memory。</p>
<p>这样一个复杂的block的计算方法是这样的，首先我们将<span class="math inline">\(x^t和h^{t-1}\)</span>拼成一个很大的vector，我们为了方便考虑这里就记做<span class="math inline">\(V\)</span>。首先我们做四个计算： <span class="math display">\[
z = \tanh(WV) \\
z^i = \sigma(W^i V) \\
z^f = \sigma(W^f V) \\
z^o = \sigma(W^o V)
\]</span> 计算这四个值是因为扒开LSTM的block，一个block除了对输入做activate，还有三个gate，分别是input gate，forget gate和output gate。大概的结构如下：</p>
<p><img data-src='https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml109.png'></p>
<p>具体的一个计算过程可以看之前的一篇<a href="https://samaelchen.github.io/machine_learning_step16/">博客</a>。这篇博客里有一个toy LSTM的分步计算过程。</p>
<p>另外还有一种做法是将<span class="math inline">\(c\)</span>跟<span class="math inline">\(x和h\)</span>一起拼成一个更大的vector，[x, h, c]这样的顺序。然后我们可以看到其实我们前面计算的大weight matrix可以看做是三个部分的权重，分别对应这三块。一般而言，我们会要求对应<span class="math inline">\(c\)</span>这部分的权重是diagonal的，原因是参数过多可能会过拟合，因此我们会希望这部分额外加入的部分尽量参数简单一些。这个过程我们叫做peephole。</p>
<p>这些计算完成以后，我们就要计算三个输出： <span class="math display">\[
c^t = z^f \odot c^{t-1} + z^i \odot z\\
h^t = z^o \odot \tanh(c^t) \\
y^t = \sigma(W&#39; h^h)
\]</span> 这里都是elementwise的乘法。</p>
<p>我们计算完成之后的三个输出就可以作为下一个block的输入继续计算。</p>
<h2 id="gru">GRU</h2>
<p>GRU可以看做是对LTSM的一个简化版本。不同于LSTM还需要更新<span class="math inline">\(c\)</span>，GRU不需要这部分的参数，因此需要更新的参数量较LSTM少了很多，可以更快计算完成。GRU简化的block如下：</p>
<p><img data-src='https://i.imgur.com/qHgq70I.png'></p>
<p>这个结构是比较简单的，跟naive RNN一样只有两个输入两个输出。GRU的计算逻辑是这样的，首先一样将<span class="math inline">\(x^t和h^{t-1}\)</span>合并为一个大vector，还是记做<span class="math inline">\(V\)</span>，然后计算 <span class="math display">\[
z^u = \sigma(W^u V) \\
z^r = \sigma(W^r V) \\
h&#39; = \sigma(W&#39; (h^{t-1} \odot z^r))
\]</span> 这里的<span class="math inline">\(r和u\)</span>分别代表GRU里面的reset和update。然后我们开始计算两个输出： <span class="math display">\[
h^t = z^u \odot h^{t-1} + (1-z^u) \odot h&#39; \\
y = \sigma(W h^t)
\]</span></p>
<p>所以在GRU中，reset gate其实是给过去longterm的memory给一个权重。</p>
<h1 id="卷积网络convolution-neural-network">卷积网络（Convolution Neural Network）</h1>
<p>和RNN不一样的，RNN主要用在NLP领域，而CNN则在图像领域大放异彩。</p>
<p>实际上卷积网络是一个对全连接层的特殊简化版本，关于卷积网络可以参考之前的另一篇<a href="https://samaelchen.github.io/machine_learning_step8/">博客</a>。这篇博客将基本原理讲的比较清楚，这里就不做更多阐述。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>台大李宏毅深度学习——计算图模型</title>
    <url>/deep_learning_step2/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Computational Graph实际上是一种描述计算过程的“语言”。这种语言中用node表示variable，用edge表示operation。</p>
<span id="more"></span>
<p>举个简单的例子，比如有一个函数<span class="math inline">\(y = f(g(h(x)))\)</span>，我们可以定义<span class="math inline">\(u = h(x), v = g(u), y = f(v)\)</span>，这样我们就可以用计算图表示如下：</p>
<p><img data-src='https://i.imgur.com/yxaoMlD.png'></p>
<p>下面是一个具体的实例: <img data-src='https://i.imgur.com/Bh3JsOn.png'></p>
<p>从这个图我们可以计算，当<span class="math inline">\(a = 2, b = 1\)</span>的时候，按照图的走向，我们可以算出<span class="math inline">\(e=6\)</span>。</p>
<p>那么计算图的一个好处是我们可以比较简单实现梯度下降。如果现在我们要计算<span class="math inline">\(\frac{\partial e}{\partial a} 和 \frac{\partial e}{\partial b}\)</span>，那么我们可以逆着图的方向，一步一步计算，首先计算<span class="math inline">\(\frac{\partial e}{\partial c} = d = 2\)</span>，<span class="math inline">\(\frac{\partial e}{\partial d} = c = 3\)</span>，然后我们发现，<span class="math inline">\(a\)</span>只对<span class="math inline">\(c\)</span>有影响，而<span class="math inline">\(b\)</span>则同时对<span class="math inline">\(c\)</span>和<span class="math inline">\(b\)</span>有影响。那么我们顺着相反的路线就可以得到<span class="math inline">\(\frac{\partial c}{\partial a} = 1\)</span>，<span class="math inline">\(\frac{\partial c}{\partial b} = 1\)</span>，<span class="math inline">\(\frac{\partial d}{\partial b} = 1\)</span>。这样我们很容易可以计算出两个偏微分分别是<span class="math inline">\(\frac{\partial e}{\partial a} = \frac{\partial e}{\partial c} \frac{\partial c}{\partial a} = 2\)</span>，<span class="math inline">\(\frac{\partial e}{\partial b} = \frac{\partial e}{\partial c} \frac{\partial c}{\partial b} + \frac{\partial e}{\partial d} \frac{\partial d}{\partial b}= 5\)</span></p>
<p>那么如果现在碰到的是参数共享的计算图怎么办呢？例如下面的实例： <img data-src='https://i.imgur.com/DdfMZRf.png'></p>
<p>那么这时候我们需要先把每个<span class="math inline">\(x\)</span>假装是完全不一样的变量计算。最后的时候再全部合并到一起。</p>
<p>认识了计算图之后，我们看如何计算神经网络的反馈。神经网络计算梯度下降分成两个步骤，一个是前馈，一个是反馈。公式上我们表示为：<span class="math inline">\(\frac{\partial C}{\partial w_{ij}^l} = \frac{\partial z^l_i}{\partial w^l_{ij}} \frac{\partial C}{\partial z^l_i}\)</span>。</p>
<p>前半部分是前馈，将计算传递到最后；后半部分是反馈，将误差传递到前面。纯数学上的推导在之前的一篇<a href="https://samaelchen.github.io/machine_learning_step6">笔记</a>中有介绍。这里讲一下如何利用计算图模型推导。</p>
<p>一个典型的前馈神经网络是这样的：</p>
<p><img data-src='https://i.imgur.com/qjPbqyR.png'></p>
<p>非常复杂的神经网络结构，用计算图表示很简洁。这里需要注意的是，对于任意一个神经网络，最后的cost只是一个scalar。但是实际上我们在计算的时候会发现一个事情，当我们计算<span class="math inline">\(\frac{\partial z}{\partial a}\)</span>的时候，我们在计算的实际上是vector对vector的偏微分。那么应该怎么计算呢。这里介绍Jacobian Matrix。</p>
<p>比如我们现在有<span class="math inline">\(y = f(x), x = \begin{bmatrix} x1 \\ x2 \\ x3 \end{bmatrix}, y = \begin{bmatrix} y1 \\ y2 \end{bmatrix}\)</span>。那么如果我们要求<span class="math inline">\(\frac{\partial y}{\partial x}\)</span>，其实我们得到的就是<span class="math inline">\(\begin{bmatrix} \partial y_1 / \partial x_1 &amp;\partial y_1 / \partial x_2 &amp;\partial y_1 / \partial x_3 \\ \partial y_2 / \partial x_1 &amp;\partial y_2 / \partial x_2 &amp;\partial y_2 / \partial x_3 \end{bmatrix}\)</span>这样的一个矩阵。这个矩阵我们就叫做是Jacobian Matrix。</p>
<p>首先我们算一下<span class="math inline">\(\frac{\partial C}{\partial y}\)</span>，假设我们现在计算的是一个分类网络，那么我们得到的是：</p>
<p><img data-src='https://i.imgur.com/8QJl3kA.png'></p>
<p>因为这里我们用的是cross entropy：<span class="math inline">\(C = -\log y_r\)</span>，所以我们可以知道当我们预测的<span class="math inline">\(y_i\)</span>跟<span class="math inline">\(\hat{y}_r\)</span>在<span class="math inline">\(i=r\)</span>的时候有<span class="math inline">\(\partial C / \partial y_r = -1 / y_r\)</span>，其余的位置因为真实值都是0，所以没有梯度。这一步还是比较好算的，我们得到的是一个很长的vector。</p>
<p>然后我们要计算的是<span class="math inline">\(\frac{\partial y}{\partial z}\)</span>。因为这两个都是vector，所以很自然我们得到的是一个matrix：</p>
<p><img data-src='https://i.imgur.com/TLlyVqM.png'></p>
<p>这里有个点要注意的是，如果我们没有对<span class="math inline">\(z\)</span>做softmax的操作，那么我们最后得到的一定是一个diagonal的matrix。此外，因为<span class="math inline">\(z\)</span>到<span class="math inline">\(y\)</span>只是做了一个activate function，所以也一定是相同维度的，所以必定会是一个方阵。</p>
<p>这里我们没有做softmax的情况下，同样只有在<span class="math inline">\(y\)</span>和<span class="math inline">\(z\)</span>下标一致的地方才有梯度。</p>
<p>接下去是比较棘手的地方，需要计算<span class="math inline">\(\frac{\partial z}{\partial a}\)</span>和<span class="math inline">\(\frac{\partial z}{\partial w}\)</span>。<span class="math inline">\(\frac{\partial z}{\partial a}\)</span>还是比较好算的，因为这个计算好的结果刚好就是<span class="math inline">\(W\)</span>，这个看公式就能看出来<span class="math inline">\(z = \sum w_i a_i\)</span>，bias对<span class="math inline">\(a\)</span>没有产生影响，所以这里不考虑。（备注：这边的PPT都是假设放进了一个矩阵<span class="math inline">\(X\)</span>，行表示sample，列表示feature）</p>
<p><img data-src='https://i.imgur.com/IANyU0v.png'></p>
<p>相对难理解的是<span class="math inline">\(\frac{\partial z}{\partial w}\)</span>。因为这里我们的计算是一个向量对一个矩阵的偏导数，最后得到的是一个三维的张量（tensor）。</p>
<p><img data-src='https://i.imgur.com/U0TTb17.png'></p>
<p>强行从二维的角度来看，其实就是每一个对角线上都是一个输入的样本矩阵<span class="math inline">\(X\)</span>。这样一来，我们要做的事情就是一步步把计算的各个矩阵乘起来，就得到了梯度。</p>
<p>最后看一下在RNN里面如何做。RNN的基本结构是：</p>
<p><img data-src='https://i.imgur.com/FVM31Gr.png'></p>
<p>那么这里需要注意的是，因为我们每个RNN的block用的都是一样的function，所以实际上这些block是共享权重的，所以实际上我们要计算<span class="math inline">\(\frac{\partial C}{\partial W^h}\)</span>在这个图里面需要计算三个，然后全部加起来：</p>
<p><img data-src='https://i.imgur.com/5OuUGzn.png'></p>
<p>现在基于计算图的框架比较多，MXNet的gluon，PyTorch都是。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>台大李宏毅深度学习——神经网络一些骚操作</title>
    <url>/deep_learning_step4/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>神经网络自2012年躲过世界末日后开始寒武纪大爆发的既视感，从最原始的FC开始出现了各种很风骚的操作。这里介绍三种特殊的结构：spatial transformer layer，highway network &amp; grid LSTM，还有recursive network。</p>
<span id="more"></span>
<h1 id="spatial-transformer">Spatial Transformer</h1>
<p>CNN是这一次深度学习大爆发的导火线，但是CNN有非常明显的缺陷。如果一个图像里的元素发生了旋转、位移、缩放等等变换，那么CNN的filter就不认识同样的元素了。也就是说，对于CNN而言，如果现在训练数字识别，喂进去的数据全是规整的，那么倾斜的数字可能就不认识了。</p>
<p>其实从某种意义上来说，这个就是过拟合了，每个filter能做的事情是非常固定的。不过换个角度来看，是不是也能理解为数据过分干净了？</p>
<p>那么为了解决这样的问题，其实有很多解决方案，比如说增加样本量是最简单粗暴的方法，通过image augment就可以得到海量的训练数据。另外一般CNN里面的pooling层也是解决这个问题的，不过受限于pooling filter的size大小，一般来说很难做到全图级别的transform。另外一种做法就是spatial transformer。实际上，spatial transformer layer我感觉上就是嵌入网络的image augment，或者说是有导向性的image augment。这样的做法可以减少无脑augment带来太多的噪音。个人理解不一定对。这种方法是DeepMind提出的，论文就是<a href="https://arxiv.org/pdf/1506.02025.pdf">Spatial Transformer Networks</a>。</p>
<p>首先看一下如果要对图像进行transform的操作，我们应该怎么做？对于一个图像里的像素而言有两个下标<span class="math inline">\((x,y)\)</span>来表示位置，那么我们就可以将这个看作是一个向量。这样以来，我们只需要通过一个二阶方阵就可以操作图像的缩放和旋转，然后加上一个二维向量就可以控制图片的平移。也就是说 <span class="math display">\[\begin{bmatrix}x&#39; \\ y&#39; \end{bmatrix} = \begin{bmatrix}a &amp; b \\ c &amp; d \end{bmatrix} \begin{bmatrix}x \\ y \end{bmatrix} + \begin{bmatrix}e \\ f \end{bmatrix}\]</span></p>
<p>当然，简洁一点可以写成： <span class="math display">\[
\begin{bmatrix}x&#39; \\ y&#39; \end{bmatrix} = \begin{bmatrix}a &amp; b &amp; c \\ d &amp; e &amp; f \end{bmatrix} \begin{bmatrix}x \\ y \\ 1 \end{bmatrix}
\]</span> 两个公式的元素没有严格对应，不过意思一样。</p>
<p>但是这里需要注意的事情是，比如我们原来输入的图片是<span class="math inline">\(3 \times 3\)</span>的，我们输出的还是一个<span class="math inline">\(3 \times 3\)</span>的图片，而位置变换后超出下标的部分我们就直接丢弃掉，而原来有值，现在没有值的部分就填0。示意图如下：</p>
<p><img data-src='https://i.imgur.com/CSnxAU6.png'></p>
<p>然后有一点我一直没理解的点就是，在论文里面上一个等式的左边是source layer，右边的是target layer。直观上从forward的方向上看，数据从上一层到下一层，那么变化就应该是第一层经过变化后变到第二层。</p>
<p>论文里面没有太解释为什么会是这样的操作，看了一些别人的博客，大部分人也说得不清不楚的。个人的感觉吧，为了这么做是为了保证输出的feature map的维度能够保持不变，论文里面有一个示例图：</p>
<p align="center">
<img data-src='https://i.imgur.com/XIBMatZ.png' width=70%>
</p>
<p>从图上面看的话，target和source都是保持不变的，唯一变换的是source上的sampling grid（感觉这么说也不太对，sampling grid的像素点数量其实也没变，就是位置或者说形状变了）。而这个sampling grid就是将target的网格坐标通过上面的公式做仿射变换得到的。那如果反过来，也就是说我们直接用source做放射变换的话，很可能得到的target是不规整的。所以应该说spatial transformer layer做的事情是学习我们正常理解的仿射变换的逆矩阵。比较神奇的是这个用bp居然可以自己学出来。</p>
<p>那么这里就会有个问题，因为sampling grid的像素点其实是没有变过的，所以这就意味着说仿射变换的结果很可能得到是小数的index。比如说<span class="math inline">\(\begin{bmatrix}1.6 \\ 2.4 \end{bmatrix} = \begin{bmatrix}0 &amp; 0.5 \\ 1 &amp; 0 \end{bmatrix} \begin{bmatrix}2 \\ 2 \end{bmatrix} + \begin{bmatrix}0.6 \\ 0.4 \end{bmatrix}\)</span>，那么这个时候要怎么办呢？如果我们按照就近原则的话，那么这个位置又会被定位到原图的<span class="math inline">\([2, 2]\)</span>这个位置，那么梯度就会变成0。所以这样是不行的，那么为了可以进行bp，论文里面采用了双线性插值的方法。也就是说，用离这个位置最近的四个顶点的像素，按照距离的比例作为权重，然后加权平均来填补这个位置的像素。</p>
<p>这个算法大概原理如下：</p>
<p align="center">
<img data-src='https://i.imgur.com/b7IprgN.png' width=50%>
</p>
<p>我们现在想要求中间绿色点的像素，那么我们先算出<span class="math inline">\(R_1\)</span>和<span class="math inline">\(R_2\)</span>的像素： <span class="math display">\[
R_1 = \frac{x_2 - x}{x_2 - x_1}Q_{11} + \frac{x - x_1}{x_2 - x_1}Q_{21} \\
R_2 = \frac{x_2 - x}{x_2 - x_1}Q_{12} + \frac{x - x_1}{x_2 - x_1}Q_{22}
\]</span> 然后计算<span class="math inline">\(P\)</span>的像素： <span class="math display">\[
\boxed{P = \frac{y_2 - y}{y_2 - y_1}R_1 + \frac{y - y_1}{y_2 - y_1}R_2}
\]</span></p>
<p>那么在DeepMind的试验里面，在卷基层里面加入了ST层之后，收敛以后target得到的输出大体上都是不变的。就像下图：</p>
<p align="center">
<img data-src="https://i.imgur.com/x0Za3Tx.gif" />
</p>
<p>另外就是这个变换矩阵，如果我们强行让这个矩阵长成<span class="math inline">\(\begin{bmatrix}1 &amp; 0 &amp; a \\ 0 &amp; 1 &amp; b \end{bmatrix}\)</span>，那么就会变成attention模式，网络自己会去原图上面扫描，这样就会知道模型在训练的时候关注图片的哪个位置。看起来就像下图：</p>
<p align="center">
<img data-src='https://i.imgur.com/IDeic8W.png' width=70%>
</p>
<p>上面那一排的网络有两个ST layer，大体上可以看出来，红色的框都是在鸟头的位置，绿色的框都是在鸟身的位置。</p>
<h1 id="highway-network-grid-lstm">Highway network &amp; Grid LSTM</h1>
<p>Highway network最早是<a href="https://arxiv.org/pdf/1505.00387.pdf">Highway Networks</a>和<a href="https://arxiv.org/pdf/1507.06228.pdf">Training Very Deep Networks</a>这两篇论文提出的。Highway network实际上受到了LSTM的启发，从结构上来看，深层的前馈网络其实和LSTM非常的像，如下图：</p>
<p align="center">
<img data-src='https://i.imgur.com/L4cqtdk.png' width=80%>
</p>
<p>所以二者的差别就在于，在前馈中只有一个input，而LSTM中每一层都要把这一个时刻的x也作为输入。所以很自然的一个想法，在LSTM中有一个forget gate决定要记住以前多久的信息，那么在前馈网络中也可以引入一个gate来决定有哪些之前的信息干脆就不要了，又或者有哪些以前的信息直接在后面拿来用。那最简单LSTM变种是GRU，所以highway network借鉴了GRU的方法，把reset gate拿掉，再把每个阶段的x拿掉。</p>
<p>所以将GRU简化一下再竖起来，我们就可以得到highway network：</p>
<p align="center">
<img data-src='https://i.imgur.com/SsDSDuy.png' width=70%>
</p>
<p>那么模仿GRU的计算方法，我们计算<span class="math inline">\(h&#39; = \sigma(Wa^{t-1})\)</span>，<span class="math inline">\(z = \sigma(W^z a^{t-1})\)</span>，所以<span class="math inline">\(a^t = z \odot a^{t-1} + (1-z) \odot h&#39;\)</span>。</p>
<p>而后面微软的<a href="https://arxiv.org/pdf/1512.03385.pdf">ResNet</a>其实就是一个highway network的特别版本：</p>
<p align="center">
<img data-src='https://i.imgur.com/hDTBRrE.png' width=60%>
</p>
<p>当然感觉也可以将ResNet看做是竖起来的LSTM。那ResNet里面的变换可以是很多层的，所以在现在的实现中，很常见的一个情况是将这个东西叫做一个residual block。</p>
<p>所以利用highway network有一个非常明显的好处就是可以避免前馈网络太深的时候会导致梯度消失的问题。另外有一个好处就是通过highway network可以让网络自己去学习到底哪个layer是有用的。</p>
<p>那既然可以将深度的记忆传递下去，那么这样的操作也可以用到LSTM里面，也就是grid LSTM。一般的LSTM是通过forget gate将时间方向上的信息传递下去的，但是并没有将layer之间的信息传递下去。因此grid LSTM就是加一个参数纵向传递，从而将layer的信息传递下去，直观上来说，就是在<span class="math inline">\(y\)</span>后面再拼一个vector，然后这个vector的作用跟<span class="math inline">\(c\)</span>一样。具体的可以看一下DeepMind的这篇论文，<a href="https://arxiv.org/pdf/1507.01526v1.pdf">Grid LSTM</a>。粗略来说，结构上像这样：</p>
<p align="center">
<img data-src='https://i.imgur.com/BUWr2kn.png' width=60%>
</p>
<p>那有2D的grid LSTM很自然就会有3D的grid LSTM，套路都是差不多的。不过我还没想到的是，3D的grid LSTM要用在什么场景当中，多个output？！</p>
<h1 id="recursive-structure">Recursive Structure</h1>
<p>遥想当年刚接触RNN的时候根本分不清recursive network和recurrent network，一个是递归神经网络，一个是循环神经网络，傻傻分不清。但是实际上，recurrent network可以看作是recursive network的特殊结构。Recursive network本身是需要事先定义好结构的，比如：</p>
<p align="center">
<img data-src='https://i.imgur.com/SgEEBbw.png' width=60%>
</p>
<p>那常见的recurrent network其实也可以看做是这样的一个树结构的recursive network。Recursive network感觉上好像也没什么特别有意思的东西，比较有趣的就是这边<span class="math inline">\(f\)</span>的设计。比如说现在想要让机器学会做句子的情感分析，那么很简单的一个想法就是把每一个词embedding，然后放到网络里面训练，那么我们可以用这样的一个结构：</p>
<p align="center">
<img data-src='https://i.imgur.com/YFhqVos.png' width=60%>
</p>
<p>因为在自然语言里面会有一些类似否定之否定的语法，所以我们希望说very出现的时候是加强语气，但是not出现的时候就是否定之前的。如果用数学的语言来表达，这不就是乘以一个系数嘛。所以在这样的情况下，如果我们只是简单的加减操作，那么就没有办法实现这种“乘法”操作。所以这个时候，我们的<span class="math inline">\(f\)</span>设计就会有点技巧：</p>
<p align="center">
<img data-src='https://i.imgur.com/E4Gt3UC.png' width=60%>
</p>
<p>那么看一下这个设计。如果我们直接采用最传统的做法，就是将<span class="math inline">\(a\)</span>和<span class="math inline">\(b\)</span>直接concat起来，然后乘以一个矩阵<span class="math inline">\(W\)</span>，再经过一个激活函数变换，这样的操作其实只能做到线性的关系，个人感觉，实际上这样的设计会将<span class="math inline">\(a\)</span>和<span class="math inline">\(b\)</span>的一些交互特性变成隐藏特征保存在<span class="math inline">\(W\)</span>当中，但是一旦输入变化了，这些隐藏的特征却不能被传递出来，所以效果不好。</p>
<p>因此下面的一种设计就比较骚气，后面还是传统的做法，但是前面加上了一个vector。这个vector的元素就是来学习这些词之间的interaction，然后将这些interaction变成bias，因为recursive network的function都是不变的，因此这些bias就这样被传递下去了。那么这里有一个需要注意的就是，我们这里有几个词，那我们就需要多少个bias，而且每个bias中间的这个矩阵<span class="math inline">\(W\)</span>都是不一样的。</p>
<p>这是三个比较骚气的网络结构变换，感觉看了这么多，好多网络之间都是殊途同归啊，会不会最后有一个非常general的网络结构出现，使得现在的每一种网络都是其一种特殊情况呢？</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>台大李宏毅深度学习——RNN语言模型</title>
    <url>/deep_learning_step3/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>RNN用于语言模型</p>
<span id="more"></span>
<p>在RNN出现以前，一般我们用的是N-gram model。所谓的N-gram model就是一个条件概率模型。比如2-gram用公式表示就是： <span class="math display">\[
P(w_1, w_2, w_3, \cdots, w_n) = P(w_1 | start)P(w_2 | w_1)\cdots P(w_n | w_{n-1})
\]</span> 这边的概率就是计算<span class="math inline">\(P(w_n | w_{n-1}) = \frac{\text{count}(w_{n-1} w_n)}{\text{count}(w_{n-1})}\)</span>。那相应的3-gram，4-gram就一样的道理。一般而言，n越大肯定效果就越好，但是计算量可想而知。</p>
<p>另外N-gram的问题就是，如果我们的语料库不够大的话，那么其实没有办法学到真正在语言空间中的概率。另外实操N-gram的时候，为了避免因为语料库太小导致一些条件概率变成0，我们一般会给一个非常非常小的概率，这个操作叫做smoothing。</p>
<p>另外还有一些smoothing的方法，比如说我们可以做matrix factorization。这个就跟我们平时推荐系统里面使用的矩阵分解一样，比如说SVD或者是NMF都可以。</p>
<p>那么RNN的好处就是，在参数量不增加的情况下，我们可以看得比N-gram更多。如下图：</p>
<p><img data-src='https://i.imgur.com/gQgaden.png'></p>
<p>呃，基于RNN的语言模型好像也就这些内容了。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>CNN中文验证码识别</title>
    <url>/deeplearning_mxnet_step1/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>基于CNN的中文验证码识别</p>
<span id="more"></span>
<p>出于对深度学习的实践目的，选择了CNN这一古老的工具以及OCR这个古老的问题。之前用AdaBoost，通过二值化，字符分割，再进行字符识别。准确率么是惨不忍睹，不过练练手还是可以的。</p>
<p>传统方法上最难的是除噪和分割，尤其是碰到字符粘连，干扰线等等。</p>
<p>深度学习的一个好处就是可以做end-to-end的学习。参考项亮大神的<a href="http://blog.xlvector.net/2016-05/mxnet-ocr-cnn/">博客</a> <sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="端到端的OCR：基于CNN的实现 http://blog.csdn.net/acdreamers/article/details/24883305
">[1]</span></a></sup>，深度学习解决OCR问题有两种解决思路：</p>
<blockquote>
<ol type="1">
<li>把OCR的问题当做一个多标签学习的问题。4个数字组成的验证码就相当于有4个标签的图片识别问题（这里的标签还是有序的），用CNN来解决。</li>
<li>把OCR的问题当做一个语音识别的问题，语音识别是把连续的音频转化为文本，验证码识别就是把连续的图片转化为文本，用CNN+LSTM+CTC来解决。</li>
</ol>
</blockquote>
<p>作为深度学习门外汉，这里就先做CNN的版本。刚好项亮大神用的是MXNet，而我对MXNet又有蜜汁好感，就在其基础上做了一些修改，变成自己的版本。</p>
<p>首先需要搞定数据。深度学习对数据量的饥渴是很可怕的，我的目标是尽可能模拟新浪的验证码: <img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/weibocn2.png width=350></p>
<p>为了方便生成数据，其实我还是对图片做了二值化，转化之后的图片是这样的： <img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/weibocn2_wb.png></p>
<p>通过这样处理，我在生成数据的时候就不需要模拟背景色。另外新浪很友好没加干扰线，所以这一步也省了。</p>
<p>参考另一篇Python生成中文验证码的<a href="http://blog.csdn.net/acdreamers/article/details/24883305">博客</a><sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="PIL模块与随机生成中文验证码 http://blog.csdn.net/acdreamers/article/details/24883305
">[2]</span></a></sup>，这里做一些简单的修改。为什么不用项亮大神的代码，是因为不知道为毛我cv2就是没法用。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RandomChar</span>():</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;用于随机生成汉字&quot;&quot;&quot;</span></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">Unicode</span>():</span></span><br><span class="line">        val = random.randint(<span class="number">0x4E00</span>, <span class="number">0x9FBF</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">chr</span>(val)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">GB2312</span>():</span></span><br><span class="line">        <span class="comment"># head = random.randint(0xB0, 0xD7)</span></span><br><span class="line">        <span class="comment"># body = random.randint(0xA1, 0xFE)</span></span><br><span class="line">        head = <span class="number">0xB0</span></span><br><span class="line">        body = random.randint(<span class="number">0xA1</span>, <span class="number">0xAA</span>)</span><br><span class="line">        val = (head &lt;&lt; <span class="number">8</span>) | body</span><br><span class="line">        <span class="built_in">str</span> = <span class="string">&quot;%x&quot;</span> % val</span><br><span class="line">        <span class="built_in">str</span> = codecs.decode(<span class="built_in">str</span>, <span class="string">&#x27;hex&#x27;</span>)</span><br><span class="line">        <span class="built_in">str</span> = <span class="built_in">str</span>.decode(<span class="string">&#x27;gb2312&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">str</span>, val</span><br></pre></td></tr></table></figure>
<p>首先是随机生成汉字。汉字字数其实非常多，尤其是Unicode编码的，所以这里我选择了GB2312的一级汉字，一共会有3755个。为了模型跑得快一点，这里放了前10个汉字，编码从B0A1到B0AA。</p>
<p>然后是生成验证码图片：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ImageChar</span>():</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, fontColor=(<span class="params"><span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="function">                 size=(<span class="params"><span class="number">100</span>, <span class="number">20</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="function">                 fontPath=<span class="string">&#x27;ukai.ttc&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                 bgColor=(<span class="params"><span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span></span>),</span></span></span><br><span class="line"><span class="params"><span class="function">                 fontSize=<span class="number">20</span></span>):</span></span><br><span class="line">        self.size = size</span><br><span class="line">        self.fontPath = fontPath</span><br><span class="line">        self.bgColor = bgColor</span><br><span class="line">        self.fontSize = fontSize</span><br><span class="line">        self.fontColor = fontColor</span><br><span class="line">        self.font = ImageFont.truetype(self.fontPath, self.fontSize)</span><br><span class="line">        self.image = Image.new(<span class="string">&#x27;RGB&#x27;</span>, size, bgColor)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># def rotate(self):</span></span><br><span class="line">    <span class="comment">#     self.image = self.image.rotate(10, expand=0)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">drawText</span>(<span class="params">self, pos, txt, fill</span>):</span></span><br><span class="line">        draw = ImageDraw.Draw(self.image)</span><br><span class="line">        draw.text(pos, txt, font=self.font, fill=fill)</span><br><span class="line">        <span class="keyword">del</span> draw</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">drawTextV2</span>(<span class="params">self, pos, txt, fill</span>):</span></span><br><span class="line">        image = Image.new(<span class="string">&#x27;RGB&#x27;</span>, (<span class="number">20</span>, <span class="number">20</span>), (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">        draw = ImageDraw.Draw(image)</span><br><span class="line">        draw.text((<span class="number">0</span>, -<span class="number">3</span>), txt, font=self.font, fill=fill)</span><br><span class="line">        w = image.rotate(random.randint(-<span class="number">10</span>, <span class="number">10</span>), expand=<span class="number">1</span>)</span><br><span class="line">        self.image.paste(w, box=pos)</span><br><span class="line">        <span class="keyword">del</span> draw</span><br><span class="line"></span><br><span class="line">    <span class="comment"># def randRGB(self):</span></span><br><span class="line">    <span class="comment">#     return (random.randint(0, 255),</span></span><br><span class="line">    <span class="comment">#             random.randint(0, 255),</span></span><br><span class="line">    <span class="comment">#             random.randint(0, 255))</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">randPoint</span>(<span class="params">self, num</span>):</span></span><br><span class="line">        (width, height) = self.size</span><br><span class="line">        draw = ImageDraw.Draw(self.image)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num):</span><br><span class="line">            draw.point([random.randint(<span class="number">0</span>, width),</span><br><span class="line">                        random.randint(<span class="number">0</span>, height)], (<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>))</span><br><span class="line">        <span class="comment"># return (random.randint(0, width), random.randint(0, height)</span></span><br><span class="line">        <span class="keyword">del</span> draw</span><br><span class="line"></span><br><span class="line">    <span class="comment"># def randLine(self, num):</span></span><br><span class="line">    <span class="comment">#     draw = ImageDraw.Draw(self.image)</span></span><br><span class="line">    <span class="comment">#     for i in range(0, num):</span></span><br><span class="line">    <span class="comment">#         draw.line([self.randPoint(), self.randPoint()], self.randRGB())</span></span><br><span class="line">    <span class="comment">#     del draw</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">randChinese</span>(<span class="params">self, num</span>):</span></span><br><span class="line">        gap = <span class="number">5</span></span><br><span class="line">        start = <span class="number">0</span></span><br><span class="line">        label = []</span><br><span class="line">        <span class="keyword">while</span> <span class="built_in">len</span>(label) &lt; num:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                char, val = RandomChar().GB2312()</span><br><span class="line">            <span class="keyword">except</span> UnicodeDecodeError:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            x = start + self.fontSize * \</span><br><span class="line">                <span class="built_in">len</span>(label) + random.randint(<span class="number">0</span>, gap) + gap * <span class="built_in">len</span>(label)</span><br><span class="line">            self.drawTextV2((x, random.randint(-<span class="number">3</span>, <span class="number">2</span>)),</span><br><span class="line">                            char, (<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>))</span><br><span class="line">            <span class="comment"># self.image.rotate(180)</span></span><br><span class="line">            <span class="comment"># self.rotate()</span></span><br><span class="line">            label.append(s.index(val))</span><br><span class="line">        self.randPoint(<span class="number">18</span>)</span><br><span class="line">        <span class="keyword">return</span> label</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">save</span>(<span class="params">self, path</span>):</span></span><br><span class="line">        self.image.save(path)</span><br></pre></td></tr></table></figure>
<p>原来代码里面的旋转其实并没有真正生效，因此修改成了</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">drawTextV2</span>(<span class="params">self, pos, txt, fill</span>):</span></span><br><span class="line">    image = Image.new(<span class="string">&#x27;RGB&#x27;</span>, (<span class="number">20</span>, <span class="number">20</span>), (<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>))</span><br><span class="line">    draw = ImageDraw.Draw(image)</span><br><span class="line">    draw.text((<span class="number">0</span>, -<span class="number">3</span>), txt, font=self.font, fill=fill)</span><br><span class="line">    w = image.rotate(random.randint(-<span class="number">10</span>, <span class="number">10</span>), expand=<span class="number">1</span>)</span><br><span class="line">    self.image.paste(w, box=pos)</span><br><span class="line">    <span class="keyword">del</span> draw</span><br></pre></td></tr></table></figure>
<p>这里要注意的是，原来博客里实现的是RGB的三通道，但是二值化后单通道就可以了，所以后面再压回单通道就好了。另外因为旋转后会有黑边，所以我改成了生成黑底白字。实现后生成的图像是这样婶的： <img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/generation1.png></p>
<p>再用255减一下就可以了： <img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/generation2.png></p>
<p>乍一看还是有点像的。</p>
<p>接下来就是根据MXNet的规则开始搭建网络。首先需要定义Data Iterator，防止爆内存。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OCRIter</span>(<span class="params">mx.io.DataIter</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, count, batch_size, num_label</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(OCRIter, self).__init__()</span><br><span class="line">        <span class="comment"># self.ic = ImageChar()</span></span><br><span class="line"></span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.count = count</span><br><span class="line">        self.num_label = num_label</span><br><span class="line">        self.provide_data = [(<span class="string">&#x27;data&#x27;</span>, (batch_size, <span class="number">1</span>, <span class="number">100</span>, <span class="number">20</span>))]</span><br><span class="line">        self.provide_label = [(<span class="string">&#x27;softmax_label&#x27;</span>, (self.batch_size, num_label))]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">int</span>(self.count / self.batch_size)):</span><br><span class="line">            data = []</span><br><span class="line">            label = []</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.batch_size):</span><br><span class="line">                ic = ImageChar()</span><br><span class="line">                num = ic.randChinese(self.num_label)</span><br><span class="line">                <span class="comment"># ic.save(str(k) + str(i) + &#x27;.jpg&#x27;)</span></span><br><span class="line">                tmp = np.array(ic.image.convert(<span class="string">&quot;L&quot;</span>))</span><br><span class="line">                tmp = <span class="number">255</span> - tmp</span><br><span class="line">                tmp = tmp.reshape(<span class="number">1</span>, <span class="number">100</span>, <span class="number">20</span>)</span><br><span class="line">                data.append(tmp)</span><br><span class="line">                label.append(np.array(num))</span><br><span class="line">            data_all = [mx.nd.array(data)]</span><br><span class="line">            label_all = [mx.nd.array(label)]</span><br><span class="line">            data_names = [<span class="string">&#x27;data&#x27;</span>]</span><br><span class="line">            label_names = [<span class="string">&#x27;softmax_label&#x27;</span>]</span><br><span class="line"></span><br><span class="line">            data_batch = OCRBatch(data_names, data_all, label_names, label_all)</span><br><span class="line">            <span class="keyword">yield</span> data_batch</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reset</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>这里一定要注意： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.provide_data = [(<span class="string">&#x27;data&#x27;</span>, (batch_size, <span class="number">1</span>, <span class="number">100</span>, <span class="number">20</span>))]</span><br><span class="line">self.provide_label = [(<span class="string">&#x27;softmax_label&#x27;</span>, (self.batch_size, num_label))]</span><br></pre></td></tr></table></figure> 这两段必须要有。MXNet做图像需要的是4D-matrix，分别是(batch size, channel, width, length)。因为这里我是单通道，因此channel只用了1，如果是RGB三通道，那就是3。</p>
<p>然后这里定义了一个简单的卷积网络： <img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/network.png></p>
<p>三个Full Connect层用来做三个汉字的识别，然后Concat回去。这有就能同时学习三个汉字。</p>
<p>接下去就是让机器开始训练。训练过程中也有一些比较尴尬的情况发生。我用的是GTX 1080，CUDA 8.0，cuDNN 5的配置，训练的过程中发生了一件非常离奇的准确率跳崖事件。在learning rate为0.001的时候，一开始看着还正常，但是中间发生了从82%掉到40%的情况，好在后面又爬起来了。 <img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/log1.png> 但是后面突然掉到了0，接着就再也爬不起来了。我原以为是偶然，掐掉重跑，又发生了这个事情。而且发生的更早。 <img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/log2.png> 我又怀疑是learning rate设大了，于是改为0.0005，好吧，这已经小的有点逆天了。 <img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/log4.png> 一切看起来都很好。而且准确率比0.001还高了一点。中间也会发生跳崖，但是都爬起来了。意外很快发生，下一个epoch快结束的时候又自杀了。 <img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/log3.png></p>
<p>由于之前的epoch都在10以内，最后下了狠心干脆一样的参数跑100个epoch看是不是真的自杀完活不过来了。开了后台跑，结果自杀现象又消失了。中间有一次跳崖，但爬起来了。 <img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/log5.png> 大概十来个Epoch后基本上就刷到了90%以上，后面不但没有再自杀，甚至出现了100%的情况。 <img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/log8.png> 哪怕到了最后也一切安好。 <img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/log6.png> 完整的log可以看<a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/mxnet/ex03/nohup.out">这个</a>。</p>
<p>那么问题来了，自杀到底是Epoch不够还是learning rate太大？</p>
<hr />
<p>2017-04-25更新： 我做了一个282字的识别，learning rate设定为0.0005的情况下，在32个epoch时候跳崖自杀，39个epoch爬回来，40个epoch又自杀，从此一蹶不振。于是被我提前掐死了。但是当我将lr改到0.0001，到了168个epoch，已经到了97%了。所以目测跳崖自杀是lr太大了。</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/log7.png></p>
<p>不过这种防止自杀的手段是有明显短板的，那就是lr太小收敛很慢。如果跟我一样仅有一块1080卡，然后像我多放几个filter和hidden，速度就是上图，呵呵哒了。</p>
<div id="footnotes">
<hr>
<div id="footnotelist">
<ol style="list-style: none; padding-left: 0; margin-left: 40px">
<li id="fn:1">
<span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">端到端的OCR：基于CNN的实现 http://blog.csdn.net/acdreamers/article/details/24883305<a href="#fnref:1" rev="footnote"> ↩︎</a></span>
</li>
<li id="fn:2">
<span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">PIL模块与随机生成中文验证码 http://blog.csdn.net/acdreamers/article/details/24883305<a href="#fnref:2" rev="footnote"> ↩︎</a></span>
</li>
</ol>
</div>
</div>
]]></content>
      <categories>
        <category>MXNet</category>
      </categories>
  </entry>
  <entry>
    <title>台大李宏毅深度学习——Batch normalization &amp; SELU</title>
    <url>/deep_learning_step5/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>学习一下batch normalization和SELU，顺便看点深度学习的八卦。</p>
<span id="more"></span>
<h1 id="bn">BN</h1>
<p>Batch normalization是一个比较新的深度学习技巧，但是在深度学习的实作中有非常迅速成为中流砥柱。</p>
<p>normalization是以前统计学习比较常用的一种方法，因为对于损失函数而言，<span class="math inline">\(L(y, \hat{y})\)</span>会受到输入数据的影响。这个其实是非常直观的，比如说一个数据有两个维度，一个维度都是1-10的范围内波动的，另一个维度是1000-10000之间波动的，那么如果<span class="math inline">\(y=x_1 + x_2\)</span>很明显后一个维度的数据对<span class="math inline">\(y\)</span>的影响非常大。</p>
<p>那么在这种情况下，我们做梯度下降，在scale大的维度上梯度就比较大，但是在scale小的地方梯度就比较小。这个在我之前学<a href="‘https://samaelchen.github.io/machine_learning_step3/’">梯度下降的博客</a>里面也有。大概图形上看就是下面这样：</p>
<p><img data-src='https://i.imgur.com/mb0vi91.png'></p>
<p>那这样我们在不同维度上的梯度下降步长是不一样的。所以在统计学习或者传统的机器学习里面，为了加快收敛的速度，虽然用二阶导可以解决，但是一般用feature scaling就可以了。</p>
<p>而batch normalization其实也是使用了这样的理念。一般而言，我们做normalization就是<span class="math inline">\(\frac{x-\mu}{\sigma}\)</span>，那batch normalization其实就是在每一个layer的input前做这么一下操作。</p>
<p>那batch normalization和normalization的差别其实就在于batch这个地方。我们知道平时我们训练深度学习网络的时候避免炸内存，会将数据分批导进去训练，在这种情况下，我们其实是没有办法得到全局的<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\sigma\)</span>的。所以事实上，batch normalization每一次算的都是一个batch的<span class="math inline">\(\mu \ \&amp; \ \sigma\)</span>。</p>
那整个流程看上去就是下图这样的：
<p align="center">
<img data-src='https://i.loli.net/2018/09/04/5b8e3c359e425.png' width=70%>
</p>
<p>那实际上可以将这个过程看作是一个hidden layer来处理。</p>
如果说觉得这样全部normalization到0，1这样的形式可能有些activation function效果不好，所以我们可以考虑一下再加一层linear layer来转换一下，那流程上就是：
<p align="center">
<img data-src='https://i.loli.net/2018/09/04/5b8e3e03c9485.png' width=70%>
</p>
<p>当然，如果好巧不巧，机器学着学着，刚好<span class="math inline">\(\beta\)</span>和<span class="math inline">\(\gamma\)</span>跟前面的一样，那么这轮的batch normalization就白做了。不过一般来说不会这么巧。</p>
<p>那么在训练过程中，我们一般都是一个batch一个batch喂进去，但是test的时候，我们一般是一口气全部过模型一遍，那么我们并没有办法得到一个合适的<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\sigma\)</span>。那么一种解决方法是计算一下全部training set的均值和标准差，另一种方法是，每次训练后，我们都保留最后一个batch的均值和标准差。</p>
<p>BN的好处非常显而易见，一个是可以减少covariate shift。也就是说，以前为了避免每个layer的方差太大，我们会减小步长，但是用了BN以后就可以用大的步长加速训练。此外，对于sigmoid或者tanh这样的激活函数来说，可以有效减少深层网络的梯度爆炸或者消失的问题。另外BN的一个副产物是可以减少过拟合。</p>
<h1 id="selu">SELU</h1>
<p>ReLu是一种比较特殊的激活函数，本身是为了解决sigmoid在叠加多层后会出现梯度消失的问题。ReLu的函数其实非常简单，就是： <span class="math display">\[
a =
\begin{cases}
0, &amp;\mbox{if }z&lt;0 \\
z, &amp;\mbox{if }z&gt;0
\end{cases}
\]</span> 不过现在回过头看ReLu，其实某种程度上效果很像是dropout？！</p>
<p>但是ReLu相对来说还是比较激进的，所以后来有各种各样的变种，比如说Leaky ReLu，就是： <span class="math display">\[
a =
\begin{cases}
0.01z, &amp;\mbox{if }z&lt;0 \\
z, &amp;\mbox{if }z&gt;0
\end{cases}
\]</span> 还有parametric ReLu： <span class="math display">\[
a =
\begin{cases}
\alpha z, &amp;\mbox{if }z&lt;0 \\
z, &amp;\mbox{if }z&gt;0
\end{cases}
\]</span></p>
<p>再后来在竞赛中还有人提出了randomized relu，其实就是上面的parametric relu的<span class="math inline">\(\alpha\)</span>每次训练的时候都随机生成一个，而不是让机器去学习，然后test的时候再固定一个就可以了。据说效果还不错。</p>
<p>但是这种形式的ReLu都是负无穷到正无穷的值域，于是又有人修正为ELU（exponential linear unit），函数是： <span class="math display">\[
a =
\begin{cases}
\alpha(e^z - 1), &amp;\mbox{if }z&lt;0 \\
z, &amp;\mbox{if }z&gt;0
\end{cases}
\]</span> 这样一来，ELU的值域就是<span class="math inline">\(\alpha\)</span>到正无穷。</p>
<p>之后横空出世了一个SELU，其实就是ELU前面乘了一个参数<span class="math inline">\(\lambda\)</span>，函数表示为： <span class="math display">\[
a =\lambda \begin{cases}
\alpha(e^z - 1), &amp; \mbox{if }z&lt;0 \\
z, &amp; \mbox{if }z&gt;0
\end{cases}
\]</span> 不过，这里的两个参数是有确定值的，而不是随便学习出来的。这里<span class="math inline">\(\alpha=1.6732632423543772848170429916717\)</span>，<span class="math inline">\(\lambda=1.0507009873554804934193349852946\)</span>。</p>
<p>这两个非常神奇的数据说是可以推导出来的，有兴趣的同学可以去看一下原文93页的证明。看不下去的可以看一下作者放出来的<a href="https://github.com/bioinf-jku/SNNs">源码</a>。</p>
<p>那么为什么要定这样两个实数，其实目的是保证每次的layer吐出来的都是一个标志正态分布的数据。</p>
<h1 id="花式调参">花式调参</h1>
<p>最后是现在有的一些花式调参的方法。毕竟实作的时候基本上也就是调参了，菜如我这种也不可能提出什么突破性的方法。</p>
<p>深度学习说白了也就是机器学习的一种，所以传统机器学习中的grid search这种非常暴力的方法当然也适用。不过为了加速搜索，一般会用random search的方法，通常也不会太差。</p>
另外现在有一些非常非常骚气的方法，一种就是learn to learn。其实就是用一个RNN去学习另一个网络的所有参数。看上去就是下图的样子：
<p align="center">
<img data-src='https://i.loli.net/2018/09/05/5b8f7f31490b4.png' width=70%>
</p>
<p>还有一个很重要的调参方向其实就是learning rate，因为深度学习很多时候是一个非凸优化的问题，所以我们以为loss下不去了可能待在了saddle point，实际上也可能是在一个local minimum的山谷里来回震荡。这种时候只要降低lr就可以继续收敛了。所以很多时候我们在训练的过程中，每50个epoch或者100个epoch就缩小一下lr，很多时候loss会出现一次很明显的降低。</p>
<p>最后是Google brain提出了一些非常神奇的激活函数，具体可以看看这篇<a href="https://arxiv.org/pdf/1710.05941.pdf">论文</a>。</p>
<h1 id="深度学习究竟有没有学到东西">深度学习究竟有没有学到东西</h1>
这个其实是非常有意思的一个争论点。很多人质疑深度学习其实只是强行记忆了数据的特征，并没有学到潜在的规律。于是有人做了相关的研究，<a href="https://arxiv.org/pdf/1706.05394.pdf">A Closer Look at Memorization in Deep Networks</a>这篇论文就是相关的研究，里面有一个很有意思的地方就是对label加noise。不论加了多少noise，模型都可以train到一个百分百正确的地方。但是test上的表现很自然会变得很差。过程如下图：
<p align="center">
<img data-src='https://i.loli.net/2018/09/05/5b8f861ddf832.png' width=70%>
</p>
<p>这个其实是非常风骚的一个操作，就是说故意给一些错误的信息让机器去学习。这个图里面的实线是train，虚线是test，我们可以看到其实一开始test是上升的，然后才下降。所以实际上一开始模型还是正常学到了一些正确的规律的。但是后面就被噪声带跑偏了。</p>
<p>不过从某种程度上来说，传统的决策树不是更像是强行记住一些东西么。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>日常的丧</title>
    <url>/depressed/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>日常很丧的各种不开心，小确丧。</p>
<span id="more"></span>
<h1 id="section">2018-09-21</h1>
<p>一直在试char-rnn生成，可能notebook硬盘io频繁了一点，终于把工作站的硬盘搞到写保护了。现在整个硬盘全是坏道。情绪稳定。</p>
<p>顺便，Linux检查硬盘坏道的方法：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo badblocks -s -v /dev/sdXX &gt; badblocks.txt</span><br></pre></td></tr></table></figure>
<p>-s可以显示检查进度，不过一般显示进度的话实际检查速度貌似会变慢。</p>
<p>然后可以用recovery模式去修复一下，fsck -a /dev/sdXX。运气好是逻辑坏道的话能修复好，如果跟我一样修复好一会儿会儿就又写保护了，估计十有八九是物理坏道。</p>
]]></content>
      <categories>
        <category>今天份的不开心</category>
      </categories>
  </entry>
  <entry>
    <title>优化算法</title>
    <url>/deeplearning_sp1/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>上班需要看运筹，顺便就把常见的优化算法复习一下。</p>
<span id="more"></span>
<h1 id="运筹常见优化算法">运筹常见优化算法</h1>
<p>运筹、动态规划需要使用到的算法大部分是最优化算法。常见有:</p>
<ul>
<li>仓库调度——匈牙利算法</li>
<li>路径规划——遗传算法、蚁群算法等</li>
<li>仓库规划——线性规划、整数规划等</li>
</ul>
<h2 id="遗传算法">遗传算法</h2>
<ul>
<li><p>最优化算法之一，灵感神他瞄来源于达尔文的进化论。</p></li>
<li><p>基本概念：遗传算法将优化问题的解叫做个体，表示为一个变量序列。每一个序列中的值叫做基因。通常序列表示为0,1向量。</p></li>
<li><p>基本过程：</p>
<ul>
<li>遗传算法首先会随机生成一定数量的个体</li>
<li>计算每一个个体的适应度</li>
<li>使用轮盘法等方法挑选最优的个体，适应度越高，留下概率越大。</li>
<li>接着对该种群进行交叉和变异，重复第二步直到满足循环条件。
<ul>
<li>交叉就是俩数列抽一段互换</li>
<li>变异就是一个数列随机抽一个值替换成别的值</li>
</ul></li>
</ul></li>
<li><p>通俗表达：在广袤的菜园里有一群无忧无虑的豌豆，每隔一代就有个叫孟德尔的恶魔把其他的豌豆掐死，只保留黄色的豌豆，过了一千代后豌豆全变成了黄色的。（我居然还记得高中生物的这个恶魔）</p></li>
</ul>
<h2 id="退火算法">退火算法</h2>
<ul>
<li><p>灵感来源于合金加热冷却的过程。用一个变量表示温度，一开始非常高，而后逐渐降低。这个变量可以看作是对误差的容忍度。</p></li>
<li><p>基本过程：</p>
<ul>
<li>首先设定一个很高的容忍值，随机生成一个数列。</li>
<li>随机让该数列一个值<span class="math inline">\(\pm1\)</span></li>
<li>如果这个新的数列误差较小,或者当概率小于在温度<span class="math inline">\(T\)</span>的条件下，出现能量差为<span class="math inline">\(dE\)</span>的降温概率<span class="math inline">\(P(dE) = \exp(dE / (T))\)</span>时，接受这个新的数列。</li>
<li>降温，重复第三步直到温度降到接受范围。</li>
</ul></li>
<li><p>通俗表达：一个喝醉酒的仙人要到谷底，一开始到处乱跳，可能跳到最高处也可能跳到最低处，随着酒精浓度越来越低，最终仙人朝着最低的路线跳下去。（于是有了仙人跳）</p></li>
</ul>
<h2 id="爬山法">爬山法</h2>
<ul>
<li><p>灵感来源于人类爬山的过程。每一次都向最高或最低的方向移动，当移动到顶点的时候就停止移动。</p></li>
<li><p>基本过程：</p>
<ul>
<li>随机生成一个数列</li>
<li>向左向右移动一个单位，选择损失最小的点</li>
<li>重复第二步直到无法移动或满足步数要求</li>
</ul></li>
<li><p>通俗表达：一只单身狗要爬山看日出，每次都往高的地方爬，最后爬到一个往前挪一步就是下降的地方停止。（最后举起了FFF团的旗帜）</p></li>
</ul>
<p><font size='5' color='#F08080'>在这之前的三个算法都可以在《集体智慧编程》第五章找到实现代码。如果愿意也可以看我fork的<a href='https://github.com/SamaelChen/hexo-practice-code/blob/master/sp/optimization.py'>这部分代码</a>。如果有人用R的话，有个叫genalg的包是提供遗传算法的，代码质量肯定比自己实现的好。</font></p>
<h2 id="梯度下降">梯度下降</h2>
<ul>
<li>加强版爬山法。爬山法的步长不容易控制，方向不定。梯度下降指定了爬山最快的方向，即导数方向，沿着该方向按照指定步长移动。</li>
<li>梯度下降的数学表达就是： <span class="math display">\[
\boldsymbol{\theta}:= \boldsymbol{\theta} - \alpha \nabla f(\boldsymbol{\theta}) \\
\text{or} \\
\boldsymbol{\theta}:= \boldsymbol{\theta} - \alpha \frac{\partial{f}}{\partial{\boldsymbol{\theta}}}
\]</span></li>
<li>BGD（批梯度下降）就是最原始的梯度下降，每次更新的时候都是处理全量数据。需要的计算量大。但可以保证收敛到局部最优。</li>
<li>SGD（随机梯度下降）每次下降按照某一个点更新。需要迭代次数多，但是计算量小，收敛快。可以收敛到局部最优附近。</li>
<li>MBGD（小批梯度下降）一个介于BGD和SGD之间的优化算法，兼顾二者的优点。</li>
</ul>
<p><font size='5' color='#F08080'>实现了一个<a href='https://github.com/SamaelChen/hexo-practice-code/blob/master/sp/optimization/gd.py'>非常简单的梯度下降</a>，高质量的随机梯度下降实现可以看Machine Learning in Action，或者Data Science from Scratch。</font></p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>台大李宏毅深度学习——seq2seq</title>
    <url>/deep_learning_step6/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>这节课的内容讲的有点浅，所以我看到是李沐的gluon教程，配合这节课的内容。</p>
<span id="more"></span>
<h1 id="seq2seq">Seq2Seq</h1>
<p>这个是encode-decode的过程。之前写的LSTM做文档分类是限定了输入的长度。超出规定长度的句子我们是截断，没达到长度的我们是padding。但是用seq2seq可以接受不定长的输入和不定长的输出。</p>
实际上seq2seq是有两个循环神经网络，一个处理输入序列，另一个处理输出序列。处理输入序列的叫编码器，处理输出序列的叫解码器。流程上如下图：
<p align="center">
<img data-src='https://i.loli.net/2018/09/06/5b90c7ddec62a.png'>
</p>
<h2 id="encoder">encoder</h2>
<p>编码器是将一个不定长的输入序列变换成一个定长的背景向量<span class="math inline">\(c\)</span>。根据不一样的任务，编码器可以是不一样的网络。例如在对话系统或者机器翻译的场景下，我们用的编码器可以是LSTM，如果在caption的场景下，CNN就是编码器。</p>
<p>现在假设我们做一个机器翻译的任务，那么有一句话可以拆成<span class="math inline">\(x_1, \dots, x_T\)</span>个词的序列。下一个时刻的隐藏状态可以表示为<span class="math inline">\(h_t = f(x_t, h_{t-1})\)</span>。<span class="math inline">\(f\)</span>是循环网络隐藏层的变换函数。</p>
<p>然后我们定义一个函数<span class="math inline">\(q\)</span>将每个时间步的隐藏状态变成背景向量：<span class="math inline">\(c=q(h_1, \dots, h_T)\)</span>。</p>
<h2 id="decoder">decoder</h2>
<p>之前的编码器将整个输入序列的信息编码成了背景向量<span class="math inline">\(c\)</span>。而解码器就是根据背景信息输出序列<span class="math inline">\(y_1, y_2, \dots, y_{T&#39;}\)</span>。解码器每一步的输出要基于上一步的输出和背景向量，所以表示为<span class="math inline">\(P(y_{t&#39;}|y_1, \dots, y_{t&#39;-1}, c)\)</span>。</p>
<p>像机器翻译的时候，我们的解码器也会是一个循环网络。我们用<span class="math inline">\(g\)</span>表示这个循环网络的函数，那么当前步的隐藏状态<span class="math inline">\(s_{t&#39;}=g(y_{t&#39;-1}, c, s_{t&#39;-1})\)</span>。然后我就可以自定义一个输出层来计算输出序列的概率分布。</p>
<h2 id="损失函数">损失函数</h2>
<p>一般而言，会用最大似然法来最大化输出序列基于输入序列的条件概率： <span class="math display">\[
\begin{split}\begin{aligned}
\mathbb{P}(y_1, \ldots, y_{T&#39;} \mid x_1, \ldots, x_T)
&amp;= \prod_{t&#39;=1}^{T&#39;} \mathbb{P}(y_{t&#39;} \mid y_1, \ldots, y_{t&#39;-1}, x_1, \ldots, x_T)\\
&amp;= \prod_{t&#39;=1}^{T&#39;} \mathbb{P}(y_{t&#39;} \mid y_1, \ldots, y_{t&#39;-1}, \boldsymbol{c}),
\end{aligned}\end{split}
\]</span></p>
<p>因此损失函数可以表示为： <span class="math display">\[
- \log\mathbb{P}(y_1, \ldots, y_{T&#39;} \mid x_1, \ldots, x_T) = -\sum_{t&#39;=1}^{T&#39;} \log \mathbb{P}(y_{t&#39;} \mid y_1, \ldots, y_{t&#39;-1}, \boldsymbol{c})
\]</span></p>
<h1 id="beam-search">beam search</h1>
<p>通常情况下，我们会在输入和输出序列前后分别加一个特殊符号'&lt;bos&gt;'和'&lt;eos&gt;'，分别表示句子的开始和结束。不过很多时候好像'&lt;bos&gt;'不是必须加的，虽然我觉得不加很奇怪。</p>
<p>假设我们输出一段文本序列，那么输出辞典<span class="math inline">\(\mathcal{Y}\)</span>，大小为<span class="math inline">\(|\mathcal{Y}|\)</span>，输出的序列长度为<span class="math inline">\(T&#39;\)</span>，那么我们一共有<span class="math inline">\(|\mathcal{Y}|^{T&#39;}\)</span>种可能。</p>
<p>那么如果按照穷举检索，我们要评估的序列数量就是全部的可能性。假设我们有10000个词，输出长度为10的序列，那么我们的可能性就是<span class="math inline">\(10000^{10}\)</span>这么多种可能性。这几乎是不可能评估完的。</p>
<p>那么换个思路，如果每一次我们都只拿概率最高的那一个词，也就是说每一次拿的是<span class="math inline">\(y_{t&#39;} = \arg\max_{y_{t&#39;} \in \mathcal{Y}} P(y_{t&#39;}|y_1, \dots, y_{t&#39;-1}, c)\)</span>。只要遇到'&lt;eos&gt;'就停止检索。这就是一个非常典型的贪婪算法。这样的话我们的计算开销会显著下降。</p>
但是贪婪算法会有典型的问题，就是检索空间太小，无法保证最优解。比如下图：
<p align="center">
<img data-src='https://i.loli.net/2018/09/06/5b90ebf798140.png'>
</p>
<p>这里的数字表示每一个state，ABC表示每一个词。中间的数字是条件概率，比如B2这里的0.4表示在<span class="math inline">\(P(B|A)\)</span>，而A2就是表示<span class="math inline">\(P(A|A)\)</span>。如果我们按照贪婪算法的话，我们会得到的结果是ABC，那么概率是<span class="math inline">\(0.5 \times 0.4 \times 0.2 \times 0.6\)</span>，而如果不是贪婪算法的话，我们得到ACB，概率是<span class="math inline">\(0.5 \times 0.3 \times 0.6 \times 0.6\)</span>明显概率更大。</p>
<p>所以我们为了保证有更大的概率可以检索到较多的可能性，我们可以采用束搜索的方法，也就是说，我们每一次不再只看概率最高的那一个词，而是看概率最高的数个词。我们用束宽（beam size）<span class="math inline">\(k\)</span>来表示。之后根据<span class="math inline">\(k\)</span>个候选词输出下一个阶段的序列，接着再选出概率最高的<span class="math inline">\(k\)</span>个序列，不断重复这件事情。最后我们会在各个状态的候选序列中筛选出包含特殊符号'&lt;eos&gt;'的序列，并将这个符号后的子序列舍弃，得到最后的输出序列。然后再在这些序列中选择分数最高的作为最后的输出序列： <span class="math display">\[
\frac{1}{L^\alpha} \log \mathbb{P}(y_1, \ldots, y_{L}) = \frac{1}{L^\alpha} \sum_{t&#39;=1}^L \log \mathbb{P}(y_{t&#39;} \mid y_1, \ldots, y_{t&#39;-1}, \boldsymbol{c}),
\]</span> 其中<span class="math inline">\(L\)</span>是最终序列的长度，<span class="math inline">\(\alpha\)</span>一般选0.75。这<span class="math inline">\(L\)</span>的系数起到的作用是惩罚太长的序列得分过高的情况。</p>
事实上，贪婪搜索可以看做是beam size为1的束搜索。过程上就像下图：
<p align="center">
<img data-src='https://i.loli.net/2018/09/06/5b90f03697a3c.png'>
</p>
<p>那么不同于贪婪搜索，束搜索其实并不知道什么时候停下来，所以一般来说要定义一个最长的输出序列长度。</p>
<h1 id="attention">Attention</h1>
<p>前面说的解码器是将编码器的整个序列都作为背景来学习。那比如说机器翻译里面，我们翻译的时候其实可能没必要全部都看一遍，只要看一部分，然后就可以将这部分翻译出来。比如说“机器学习”翻译为“machine learning”，“机器”对应的是“machine”，而“学习”是“learning”，所以翻译machine的时候只要关注机器就可以了。</p>
其实所谓的关注点，如果用数据来表示也就是权重大小，关注度越高权重越高。如下图：
<p align="center">
<img data-src='https://i.loli.net/2018/09/06/5b90fb86eac3c.png' width=70%>
</p>
<p>我们在输出背景向量的时候做一个softmax，然后每一个state给一个权重，作为<span class="math inline">\(t&#39;\)</span>时刻的输入，这样jointly训练就可以学出一个attention的形式。</p>
那么这里的<span class="math inline">\(\alpha\)</span>是这样计算出来的：
<p align="center">
<img data-src='https://i.loli.net/2018/09/06/5b90fd0f9519e.png' width=70%>
</p>
<p>其实就是每一个state的decoder的input拿来和encoder的hidden做一个match。至于match的函数可以自己随意定义。</p>
<p>这样一来，我们就可以让解码器在不同的state的时候关注输入序列的不同部分。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>深度学习工作站配置</title>
    <url>/deeplearning_set_up/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>深度学习工作站配置简介。</p>
<span id="more"></span>
<p>这是我在意识到已经落后别人太多的情况下，姗姗来迟的深度学习环境搭建。虽然我觉得深度学习已经被过度消费，但是现状是，如果现在还不追赶，就彻底没机会了。大部分的东西都是轻车熟路的，就不赘述了，说几个碰到的坑。</p>
<ul>
<li><p>显卡是GTX 1070往上的机器，安装Ubuntu的时候有可能出现显示器无法显示的状况。不同显示器报错不一，解决方案是拆掉独显，用核显装机并安装显卡驱动，最后插回显卡。</p></li>
<li><p>装机过程，首先安装系统，固态硬盘挂载系统，机械硬盘挂/home。</p></li>
<li><p>装好系统换软件源，一般我选阿里云。之后就是常见的sudo apt update &amp;&amp; sudo apt upgrade。</p></li>
<li><p>第一件事情，安装小飞机。GFW，用Ubuntu你懂得。 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:hzwhuang/ss-qt5</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install shadowsocks-qt5 tsocks</span><br></pre></td></tr></table></figure></p></li>
<li><p>第二件事情，搜狗官网下载输入法安装包，注销重新登录生效。 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo apt install gdebi</span><br><span class="line">sudo gdebi sougou.deb</span><br></pre></td></tr></table></figure></p></li>
<li><p>后面很多基本上靠谷歌搜，基本上能搞定全部的东西。</p></li>
<li><p>多线程运算是个坑。</p>
<ul>
<li>OpenBLAS是个坑，用Ubuntu的好处是用apt安装libopenblas-base和libopenblas-dev后，R、NumPy可以自动调用。但是NumPy要注意一个神坑，所有的BLAS加速仅对float类型有效。</li>
<li>MKL是个大坑，安装完，R、NumPy均不能自动调用，需要从源码编译。 R还有一个简单解决方案是安装微软改造的R，原来的RRO，安装完用/path/to/RRO/R/lib下的所有文件替换掉/usr/lib/R/lib下的文件。NumPy的解决方法可以是使用英特尔的ICC编译源码，也可以使用anaconda。MKL效率比OpenBLAS高。但是有原生工具就坚决不用第三方的我表示，MKL我放弃了。</li>
</ul></li>
<li><p>安装CUDA，cuDNN。现在安装CUDA已经非常容易了，deb一装就行。</p></li>
<li><p>安装MXNet/TensorFlow。TF已经可以用pip直接安装了，CPU版本就是tensorflow，GPU版本是tensorflow-gpu。MXNet稍微有点搞，需要修改配置文件，但也比较简单，默认直接编译是CPU版本的。好处是MXNet支持R，单纯倒腾数据，其实R的坑比Python少一点，对小白更友好。</p></li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>经济思想史读书笔记——导言</title>
    <url>/economics_thoughts_notes1/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>人类一切事业的开始和终结都是凌乱的——John Galsworthy</p>
<span id="more"></span>
<h1 id="现代经济学核心思想">现代经济学核心思想</h1>
<p>经济学是一门研究稀缺性问题的学科。历史上人类曾经有过四种机制来解决稀缺性问题。</p>
<blockquote>
<p>第一种，强制方式。<br />
第二种，传统方式。也就是依据以往的惯例方式分配。<br />
第三种，权威方式。也就是政府，教会的方式。<br />
第四种，市场方式。这也是现在在不断发展的分配方式。</p>
</blockquote>
<p>一般而言，这些方式之间并不相互排斥。市场方式是现在最主流的分配方式。</p>
<p>现代经济学分为微观经济学和宏观经济学。微观经济学研究配置问题（生产什么和如何生产）与分配问题（实际收入如何在社会成员间分配）。因此微观经济学更多关注于供给和需求的理论，希望解释决定相对价格的力量。</p>
<p>而宏观经济学关注的社会的总体分析，自上而下到个体。主要聚焦于经济体的总体分析，主要聚焦于经济体的稳定和增长，关注整体经济的总变量：收入和就业水平、价格总水平、经济增长率等。</p>
<h1 id="研究经济思想史的方法">研究经济思想史的方法</h1>
<p>研究经济思想史的方法有两种，一种是相对论史学家，一种是绝对论史学家。</p>
<p>相对论史学家关心的是：（1）引起人们考察某些经济问题的历史、经济、社会和政治力量是什么。（2）这些力量塑造新兴理论内容的方式。他们主张在每个经济理论的发展过程中，历史都扮演一定的角色。</p>
<p>绝对论史学家又被称作辉格党，强调内在力量对经济理论发展的解释。绝对论者认为，理论的进步不仅仅反映了历史环境，而且取决于训练有素的专业人员对未决问题或似是而非论点的发现与解释。</p>
<p>相对论者和绝对论者在经济学历史上交互站上历史舞台，但是经济思想史一定是学科的外在力量和内在力量相互作用的动态过程。</p>
<p>研究经济思想史可以帮助我们更好理解正统经济理论和非正统经济理论。正统经济理论家主要集中在资源配置、分配、稳定和增长这四个问题上面；而非正统经济理论家则研究社会与经济中产生变换的力量。正统经济学家认为具体的社会制度、政治制度与经济制度是既定的，并在这些制度背景下研究经济行为；非正统经济学家则试图去进行解释。</p>
<p>正统经济学家和非正统经济学家的差别主要在于他们关注的问题不一致，而不是理论本身的直接对立。</p>
<h1 id="非正统经济学家的地位">非正统经济学家的地位</h1>
<p>所谓的正统非正统，或者说主流非主流，只是在竞争中最受欢迎的一组成为主流，不太成功的一组成为非主流。主流研究院更倾向采纳主流思想中较为狭窄的观点，而相比之下，非正统经济学家可能会更重视思想的多样性。</p>
<h1 id="方法论问题">方法论问题</h1>
<p>经济思想中需要区分经济学艺术（the art of economics），实证经济学（positive economics）和规范经济学（normative economics）。</p>
<p>实证经济学关心的是支配经济活动的力量。而经济学艺术关心的是政策问题。规范经济学则明确地关注应当是什么的问题。</p>
<p>实证经济学的方法是形式化的、抽象的，试图将经济力量与政治和社会力量分开。而经济学艺术则需要致力于政治社会力量与经济力量相互关系的研究。</p>
<p>以史为鉴，才能看懂现在莫名其妙的经济形式。</p>
<h1 id="经济学方法论进化路线">经济学方法论进化路线</h1>
<h2 id="逻辑实证主义logical-positivism的兴起">逻辑实证主义（logical positivism）的兴起</h2>
<p>逻辑实证主义家的典型代表是维也纳派，他们试图通过描述科学家实际遵循的方法，来使科学家的方法形式化。他们认为，只有当一种演绎理论在经验上被检验与核实之后，它才能被认同为正确的。逻辑实证主义将“科学的目的是确立‘真理’”的观点推到了极致。</p>
<h2 id="证伪主义">证伪主义</h2>
<p>证伪主义在波普尔的著作中得到很好的表达。她提出，经验检验不能确定一种理论的真相，只能确定假象。波普尔声称，科学的目的应当是运用经验上可检验的假设来发展理论，然后对理论进行证伪，放弃那些被证明是错误的理论。</p>
<p>但是证伪主义存在三个问题。第一个问题是，一些理论的经验预言并不能被检验，因为尚不存在对它们进行检验的方法。第二个问题是，难以决定是否理论被证伪或者没有被证伪。第三个问题是研究者的心态，他们未能检验已确立理论的含义，便假定理论的含义是正确的（让我想起李祥林的高斯相依函数，号称摧毁华尔街的公式）。</p>
<h2 id="范式">范式</h2>
<p>托马斯·库恩将范式引入方法论的争论中，将方法论远离了证伪主义。范式是一种既定的方法以及构成研究者分析组成部分的知识题，它遵循着任何既定时期所公认的对主流科学思想教科书陈述。</p>
<p>范式隐藏着这样的观点，现有理论可能并不包含真理。</p>
<h2 id="研究纲领">研究纲领</h2>
<p>Imre Lakatos发现，科学家们从事发展竞争性研究纲领时，每个研究纲领不仅包括一系列的数据进行分析和证伪，而且包括无可非议地接受一系列的硬核逻辑假设。每项研究都从硬核中得出一系列周边假设，并试图对它们进行证伪。只有当“足够多”的周边假设被证伪，硬核假设才会被重新考虑。Lakatos认为，如果对周边假设进行证伪的过程在继续，那就是进步（progressive）的，否则就是退化（degenerative）的。</p>
<p>他的研究有两个特性：（1）它承认了理论证伪过程的复杂性；（2）早起的分析要求某一种理论成为主流，Lakatos则提出多种可利用理论同时存在，这些理论的优点不太容易辨别。</p>
<h2 id="社会学方法与修辞方法">社会学方法与修辞方法</h2>
<p>社会学方法与修辞方法拒绝了假设存在终极的、神圣的真理。修辞方法强调语言的说服力。该方法主张，一种理论被接受，不是因为它本身是正确的，而是因为理论的提倡者借助他们出众的修辞，成功地使其他人相信理论的价值。</p>
<p>社会学方法考察社会与制度约束，这些约束影响着对一种理论的认可。</p>
<p>这两种方法都对人们发现真理的能力表示怀疑，甚至怀疑真理是否存在。在这些方法中，理论的发展并不一定因为离真理近，理论的发展有多种理由，而真理——如果存在——仅仅只是其中之一。</p>
<p>这两种方法最有代表性的观点就是费耶阿本德的“一切尽随其便”。</p>
<h2 id="后修辞学方法">后修辞学方法</h2>
<p>后修辞经济学家会以怀疑论来看待与研究者自身利益或预想观点相符的研究结果。他们非常有可能遵循贝叶斯统计而不是古典统计。</p>
<p>贝叶斯主义者认为，人们能够发现语句中更高级或更低级的真理，但不是终极真理。</p>
<p>事实上，在大多数教科书中，经济学的主流方式依旧是逻辑实证主义，而它在学术杂志上已经死了很久了。形式主义更有可能运用逻辑实证主义或者证伪主义，并且相信绝对论方法。而非形式主义更可能运用社会的或修辞的方法，并相信相对主义方法。</p>
]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
  </entry>
  <entry>
    <title>经济思想史读书笔记——重商主义、重农主义及其他先驱</title>
    <url>/economics_thoughts_notes3/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>值得注意的是，发明家们并不具备被称为“科学态度”的客观性。——威廉·莱特温</p>
<span id="more"></span>
<p>1600年至1750年的150年间，经济活动极大增加，文艺复兴完成了资本主义的萌芽，教会的权威极大被削弱，这些为后来的工业革命的爆发奠定了基础。</p>
<p>这一时期，经济思想从简单的个人、家庭、生产者的观点，向更复杂的将经济体视为有其自身规律与相互关系的一个系统的观点演进。</p>
<h1 id="重商主义">重商主义</h1>
<p>重商主义的主要贡献出自英国人和法国人之手。经院哲学主要来自中世纪牧师，而重商主义的经济理论主要来自商人。重商主义的理论更像是个人的智力反应，而不是一个成体系的理论框架。</p>
<p>重商主义时期，由于领地减少，名族国家增多，重商主义理论试图确定能够推动国家权利与财富增加的最佳政策。</p>
<p>重商主义者的理论是基于世界总财富不变的假设下入手的。经院哲学在这个假设下论证了个人的财富获取，必定伴随另一个人的失去。重商主义将这个观点推广到了国家之间。因此，在他们看来，一个国家的财富是依靠很多国家的贫困来支撑的。重商主义者强调，国际贸易是增加一国财富和权力的一种手段，并且特别集中研究国家之间的贸易平衡问题。</p>
<p>大多数重商主义者认为，经济活动的目的是生产（古典经济学认为是消费）。因此，他们为了在国际贸易中保持优势，提倡低工资、低消费。乍一听，这不就是拥有劳动力优势的发展中国家的情况么。在重商主义的理论下，国民的贫困将使国家受益。所以在重商主义的思想下，为了实现贸易顺差，一国应该通过关税、配额、津贴和税收等手段来鼓励出口、限制进口；应当通过政府干预国内经济，以及通过对外贸易规则来刺激生产；应当对从国外进口的制造业产品征收保护性关税；应当鼓励进口用于制造出口产品的廉价原材料。诶，是不是很像某国现在在搞的贸易战的样子。</p>
<p>重商主义的货币观点是，一国的财富等同于国内持有的贵金属存量。他们深信，货币因素而非实际因素是经济活动与经济增长的首要决定因素。后面看到古典经济学的时候再做一个具体的比较。</p>
<p>从现在的角度来看，重商主义者被利益驱动，利用政府为他们自身获取经济特权。他们通常是商人，支持政府允许垄断，使得商人垄断者可以索要比没有垄断时更高的价格。</p>
<p>重商主义者最重要的成就或许是认可了对经济体进行分析的可能性。他们意识到了经济体中非常机械的因果关系，并且相信如果一个人弄懂了这些关系的规则就能控制经济体。他们认为政府干预可以达到既定的目的，但是不能随便干预使得一些基本的经济原理变得复杂。后期的重商主义意识到早期重商主义的很多理论不足，例如硬币不能代表一国的财富；对于所有国家不可能存在一种贸易顺差等。后期重商主义出现了早期古典自由主义的观点。但是古典主义和重商主义还是存在一个重要的差别，那就是重商主义认为私人利益和公关福利之间存在根本的冲突，而古典主义经济学家认为公共利益是个人追求自我利益自然而然的结果。</p>
<h1 id="重农主义">重农主义</h1>
<p>重农主义主要在18世纪的法国兴起。重农主义的著作与重商主义不同的是，重农主义有显著的一致性。重农主义的发展是短时间在法国集中出现的，而且有共同的知识领袖——弗朗索瓦·魁奈。</p>
<p>重农主义认为存在自然法则支配着经济体的运作，这些法则独立于人类意志，而人类可以客观发现它们。他们觉得，有必要通过分离主要经济变量来构建理论模型。重农主义并不集中研究货币，而是重点研究导致经济发展的实际力量上。相对于重商主义认为的财富源于交换过程的观点，重农主义推断财富起源于农业或者自然。</p>
<p>因为重农主义发展的时代，生产的产品用于支付实际生产成本之后，产生了剩余。对这种剩余的探索让他们形成了净产品的概念。根据他们的观点，劳动只能生产出支付劳动成本的产品，只有土地例外。因此土地的生产产生了净产品，而其他的非农业活动不能产生净产品。所以重农主义者集中注意力于物质生产力而不是价值生产力。</p>
<p>重农主义的理论巅峰是魁奈的《经济表》。事实上，这个表格的价值流动，看上去很像“投入——产出表”。经济表证明了经济体不同部门之间相互依赖的存在。</p>
<p>在经济政策上，重农主义者认为存在一种比人类设计的秩序更优的自然秩序，所以政府的任务是实行自由放任的政策。他们推断自由竞争将会导致最优价格；如果每个个体都追求自己的私利，那么社会将从中获利。</p>
<p>事实上，重商主义者发现净产品的源泉是交换，尤其是国家贸易形式的交换，因此他们提倡贸易顺差。而重农主义者则认为净产品源自农业，因此主张放任自由会引发农业生产的增加，最终引起更大的经济增长。</p>
<h1 id="其他思想先驱">其他思想先驱</h1>
<h2 id="托马斯孟">托马斯·孟</h2>
<p>托马斯·孟是一个主要的重商主义者，但是他的观点跟原始的重商主义不一样。作为一个英国人，他指出，尽管与所有其他国家实现了贸易顺差是合意的，贵金属流出到其他国家是不合意的。但是在与印度的贸易逆差以及出口贵金属到印度却有利于英国，因为这些实践扩大了英国与所有国家的贸易平衡，增加了金银的流入。虽然托马斯·孟是一个重商主义者，但是他以及看到了早期重商主义范例的严重错误。</p>
<h2 id="威廉配第">威廉·配第</h2>
<p>配第是第一个提倡测量经济变量的经济学家。配第最早明确提倡用我们所谓的统计方法来度量社会现象。他设法度量一国的人口、国民收入、出口、进口、资本量。尽管配第对统计学的早期应用显得有些原始，但是，他所代表的方法论立场却具有一种世系，这种世系始于他所处时代的经验归纳，止于当代经济学期刊上盛行的计量经济学线代应用。</p>
<h2 id="伯纳德曼德维尔">伯纳德·曼德维尔</h2>
<p>曼德维尔发现世界是邪恶的，但主张“在一个熟练政治家的灵巧管理下，私人恶习有可能变成公众利益”。重商主义的信仰具体表现为对产品的恐惧，对生产过剩与消费不足的关注。个人储蓄并不受欢迎，因为它会引发更低的消费、更低的产量，以及更低的就业。曼德维尔是一个纯粹的重商主义者，他坚决主张政府管制对外贸易，从而保证出口总是超过进口。因为社会的目标是生产，所以曼德维尔甚至主张大量拥有人口和童工，并谴责懒惰。他注意到了一条向下倾斜的劳动力供给曲线。根据曼德维尔的观点，较高的工资将减少劳动供给。他的主要观点就是应当接受满身恶习的人类，并通过规则和制度将其引导到社会利益上来。</p>
<h2 id="大卫休谟">大卫·休谟</h2>
<p>休谟被称作自由的重商主义者。他认为一个经济体的经济活动水平取决于货币数量及其周转速度，并对一国的贸易平衡、货币数量以及价格总水平之间的关系做出了相当完整的描述。<strong>黄金流动价格机制</strong>被认为是休谟在国际贸易理论中的重要贡献。</p>
<p>休谟不是一个纯粹的重商主义者在于，他指出，一个经济体不可能持续保持贸易顺差。贸易顺差将导致经济体内金银的增加。货币增加将使得具有贸易顺差的经济体价格上升。那么具有贸易逆差的经济体就会货币减少，价格下降。那么在这样的情况下，原来具有贸易顺差的经济体出口就会减少，进口就会增加。逆差的经济体则相反。这一过程最终将会导致贸易平衡的自动调整。</p>
<p>休谟认为，尽管一国的货币绝对量不能影响实际产量，但是货币供给的逐渐增加将会引起产量的增加。这一点，休谟并没有跳出重商主义的框架。</p>
<p>最后休谟主张经济自由，他认为经济自由和政治自由的增长是结合在一起的。</p>
<h2 id="理查德坎蒂隆">理查德·坎蒂隆</h2>
<p>理查德·坎蒂隆是部分重商主义，部分重农主义，以及部分的重农主义-古典学派。坎蒂隆通过推理的过程来建立经济学基本原理，并试图收集数据，并在检验原理的过程中加以使用。他最具有影响力的见解是关于市场体制的，该体制通过个人私利这一媒介来协调生产者和消费者的活动。直观感受上，非常接近完全竞争市场。</p>
<p>他总是倾向于将任何经济成分当做是一个完整结构的一部分。例如，在他的体制中人口变化是内生的，而不是外生的。他区分了由短期因素决定的市场价格与他所谓的内在价值即长期均衡价格。他最熟练的技术分析主要在宏观经济学中，即货币供给变化对价格和生产的影响。他将经济体划分为部门，分析收入在部门之间的流动。他注意到，新的资金进入经济体，价格总水平可能改变，但是相对价格也可能改变，并对经济体的不同部门产生影响。</p>
<h1 id="西班牙思想">西班牙思想</h1>
<p>因为地理大发现带来了大量的金银，黄金大量流入西班牙，西班牙国内的价格水平上升，西班牙的知识分子开始评价这些迅速变换的经济现象。在西班牙思想中，货币数量理论表明，货币价值即货币购买力是由流通中的货币数量决定的。其中，路易斯·摩里纳对于市场机制的描述就是我们今天的需求与供给定理，以及货币数量理论。</p>
<blockquote>
<p>……产品短缺，促使公平价格上升……丰裕使得公平价格下降。进入市场的购买者的数量在一些时候比另一些时候多一些，他们热切的购买愿望引起价格上升……一个地方缺少货币，会导致其他物品价格下降，货币充裕则会使价格上升……</p>
</blockquote>
<h1 id="小结">小结</h1>
<p>重商主义者和重农主义者都认为经济体可以被正式地加以研究，并且发展了一种抽象方法来发觉能够调节经济体的法则。他们第一次让经济理论立足于抽象的模型构建过程。</p>
<p>重商主义者就货币在确定价格总水平中的作用，以及对外贸易平衡对国内经济活动的影响方面取得了最初的尝试性见解。重农主义者则是提出了经济体不同部门相关性的概念。在面对经济体的基本冲突上，重商主义者和经院哲学都提倡，要么借助政府，要么借助教会，对经济体施加干预。但是重农主义者则认识到利益冲突的结果基本上是协调的，是相对稀缺性所固有的，他们不提倡政府干预，而是提倡自由放任。</p>
<p>这一时期的一些英国经济学家既不完全符合重商主义，也不完全符合古典阵营。他们否决了交换中的固有冲突这一较为原始的重商主义观点；反驳了永远保持有利贸易平衡的必要性；也正是他们了解了市场是如何运作来调整个别经济活动的。</p>
<p>这些重要的思想者没能完成理论的大一统，这就是未来亚当·斯密的任务。</p>
]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
  </entry>
  <entry>
    <title>经济思想史读书笔记——亚当·斯密</title>
    <url>/economics_thoughts_notes4/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>“亚当·斯密在政治经济学的历史中占据如此中心的位置，以至于这个谨慎的水手不愿在这样广阔的海洋上开始着手航行。”——亚历山大·格雷</p>
<span id="more"></span>
<p>亚当·斯密是经济思想史上一个非常重要的人物，他的著作是经济思想发展中的一道分水岭。</p>
<h1 id="博学多才的经济学家">博学多才的经济学家</h1>
<p>斯密的思想涉及到很多领域，包括经济、政治还有哲学。在斯密所处的时代，从事研究的知识精英被要求掌握最宽广的人类知识，而不是仅仅专攻某一个领域的知识。这种多学科方法的一个后果是，像亚当·斯密探求如今称为社会科学的人认为，牛顿在物理学中建立的科学严密性，他们也可以做到。</p>
<p>斯密常被称为经济学之父，尽管很多先贤看到了经济学很多方面，但是没有人能够将决定国民财富的力量、培育经济增长与发展的适宜的政策以及通过市场力量有效协调大量经济决策的方式，整合成一个全面的观点。</p>
<p>斯密对经济学范围的认识继承了英国重商主义的观点。他对解释国民财富的性质与原因感兴趣。但是斯密考察的范围比现代经济学家研究的要宽泛，他用政治的、社会的、历史的材料来填充其经济模型。</p>
<p>虽然斯密在经济学史上的地位非常重要，但是斯密的理论模型缺少优雅与严密。</p>
<h1 id="斯密的市场分析与政策结论">斯密的市场分析与政策结论</h1>
<p>斯密在经济思想史的重要性基于（1）他对经济体相互关联性的广泛了解和（2）他对经济政策的影响。斯密作为一位经济学家的强大实力在于，他洞察了：（1）经济体组成部分的相互依赖；（2）用来推动已过财富的政策。他不仅是一个经济学家，而且是一位指出了经济发展与富足方法的哲学家。</p>
<h2 id="前后关联的经济政策">前后关联的经济政策</h2>
<p>斯密的方法论塑造了他对经济体的分析，以及他关注政府政策的决心。斯密对自由放任的主张部分依赖于市场如何产生某些结果的理论模型，还来自于他对现有历史与制度环境的观察。所以，斯密的经济政策是前后关联的。斯密对自由市场的拥护并不是因为他认为市场是完美的，而是因为他所处的时代，英国的历史与制度结构导致，市场通常比政府干预产生更好的结果。</p>
<p>在之前讨论过，经济学科学处理经济变量之间实证的、事务性的关系，即探讨“是什么”。规范经济学涉及的是“应当怎样”的问题。经济学艺术是以政策为导向的。亚当·斯密是一个超群的经济学艺术大师。因而，前后关联的经济政策，就是另一种表达经济学艺术观点的方式。</p>
<h2 id="自然秩序和谐和自由放任">自然秩序、和谐和自由放任</h2>
<p>斯密的经济学与重商主义有很多相同的基本因素。他们认为通过科学的调查能够揭示事务性的因果关系。斯密也像重商主义一样，提出人类本性的基本假设：人类是理性的，是有私心的，很大程度上受经济利己主义的驱使。也就是古典经济学里面的一个基本假设——理性人假设。</p>
<p>但是斯密体系和重商主义有一个很大的不同点在于，他假设竞争性市场在极大程度上是存在的，在这些市场内部，生产要素自由流动，从而提升了它们的经济优势。第二个区别是，经济体的自然运作，能够比人类做出的任何安排更有效地解决冲突。这也是斯密体系里面一个非常重要的假设基础，就是存在完全竞争市场。实际上，斯密的自由放任政策只有在完全竞争市场下才能成立。</p>
<p>斯密的推论过程非常简单。人类是理性的，是有私心的，受利己主义驱使。如果放任不管，每个个体都会追求自己的私利，在促进私利的同时也促进了社会利益。政府不应当干预这一过程，而应当遵循自由放任的政策。在斯密体系内，私利与公共利益是和谐的。也就是说，在斯密看来，在没有政府干预的竞争性市场上会出现资源的最佳配置。</p>
<h2 id="竞争性市场的运作">竞争性市场的运作</h2>
<p>斯密对经济理论最重要的贡献是他对竞争性市场运作的分析。他能比以前的经济学家更详细说明，源于竞争的价格在长期中为什么等于生产成本这一道理。他对价格形成与资源配置的分析中，他将短期价格称为“市场价格”， 将长期价格称为“自然价格”。他认为，竞争从根本上要求有大量的销售者；数值经济体中利润、工资、租金的一群资源拥有者；资源在行业间自由流动。给定这些条件，资源拥有者的私利将会形成长期自然价格，该价格将经济体不同部门之间的利润率、工资、租金均等化。</p>
<p>确立了竞争性市场的优越性后，斯密毫无困难地构建起他反对垄断与政府干预的论据。但是斯密的自由放任是基于竞争市场存在的假设的。</p>
<p>斯密认为，尽管重商主义者关于政府干预的很多主张都声称是促进了社会利益，但是其实是增进了个人私利，这种管制不是有利于国家，而是利于商人。但是斯密也并不主张完全的自由放任，他认为在他所处时代历史的、政治的、制度的背景下，他认为，保护幼稚产业的关税是有必要的；国防的贸易管制也是有必要的；政府还应该提供具有极大社会收益但是私人市场因为没有充足利润而不去提供的产品，以此来限定他对自由放任的主张。</p>
<h2 id="资本与资本家">资本与资本家</h2>
<p>斯密提出了关于资本在财富生产过程中和在经济发展中的作用的一些重要概念。首先，他指出，一国的现有财富取决于资本积累，原因在于资本积累决定了劳动分工和参加生产性劳动的人口比例。其次，斯密断定资本积累也会导致经济发展。再次，与资本积累相结合的个人私利导致资本在各产业之间的最佳配置。</p>
<p>在斯密体系中，资本家在经济体运行中扮演主要的角色。资本家对财富与利润的追逐，引导经济体实现资源的有效配置和经济增长。在私有财产经济体中，资本的来源是个人储蓄。斯密认为，劳动并不能够积累资本，原因在于工资水平仅仅能够满足直接的消费欲望。</p>
<p>斯密断定，恰恰是一部分正在星期的产业接济是对社会有益的人，他们为了利润而奋斗，努力积累资本，通过储蓄和投资来增加他们的财富。因此有利于资本家的收入不平等分配具有巨大的社会重要性。没有收入的不平等分配，就不可能有经济增长，因为全部的年产出都会被消费掉。</p>
<h2 id="斯密对政策的影响">斯密对政策的影响</h2>
<p>斯密的重要贡献在于他对市场经济在多种用途之间配置稀缺资源方式的广泛看法。他的主要政策结论是政府应当接受自由放任的政策。</p>
<h1 id="国民财富的性质与原因">国民财富的性质与原因</h1>
<p>斯密不认同重商主义的一国的贵金属就等同于一国财富的观点。他认为，财富是产品与服务的年流量，而不是贵金属的累计储备量。他也解释了出口与进口之间的关系，认识到出口的基本作用是支付进口。此外他暗示，经济活动的最终目的是消费。这将斯密的经济学与重商主义的经济学加以区分，后者将生产视为目的。而在国家财富源泉的认识上，斯密与重农主义也不一样，后者强调的是消费。</p>
<p>斯密继而建议，国家的财富应当按照人均指标来衡量，这就是现在经常被提及的人均GDP。</p>
<h2 id="国民财富的原因">国民财富的原因</h2>
<p>斯密主张，一个国家的财富，也就是国家的收入取决于：（1）劳动生产力；（2）有效使用或者生产性使用的劳动者的比例。因为在他的完全竞争市场下，经济体将自动实现资源的充分利用，因此他只需要考察那些决定一国产品与服务生产能力的因素。</p>
<h3 id="劳动生产力">劳动生产力</h3>
<p>斯密认为，劳动生产力取决于劳动分工。专业化与劳动分工提高了劳动生产力。尽管斯密认为专业化与劳动分工的经济利益，但是他也察觉到一些严重的社会成本。劳动分工的一个社会缺点是公认被赋予重复性的任务，这些任务很快变得单调乏味。这一点其实与管理学里面的科学管理法非常相似，另外可以看一下霍桑试验。但是我们不得不承认，劳动分工增加了人类福利。</p>
<p>劳动分工一次取决于斯密所谓的市场的范围与资本的积累。市场越大，可销售的数量越多，劳动分工的机会就越多。另一方面，有限的市场仅允许有限的劳动分工。劳动分工收到资本积累的限制，原因在于生产过程是耗时的：在生产开始与成品的最终销售之间存在时间间隔。</p>
<p>在一个简单的经济体中，劳动分工是微小的，只需要很少的资本来维持劳动力。但是随着劳动分工的增加，劳动者不再为其自身的消费生产产品，在耗时的生产过程期间，必须保持一定的消费品储备来维持劳动者。这一定数量的产品来自储蓄，也就是斯密所说的资本。资本家的一个主要功能是缩短生产开始与最终产品销售之间的时间间隔而提供手段。因此可以使用劳动分工的生产过程的范围，收到可以利用的资本积累数量的限制。换而言之，斯密认为，劳动分工随着越来越多的储蓄而越来越细分。如果将这一个观点放到小一点的经济体上，比如公司，我们就会很直观理解，创业公司，一个人当好几个人用，当公司庞大到商业帝国的程度，几个人当一个人用。</p>
<h3 id="生产性与非生产性劳动">生产性与非生产性劳动</h3>
<p>按照斯密的观点，资本积累也决定了生产性：劳动者与非生产性劳动者的比例。他主张，生产可销售商品所使用的劳动是生产性劳动，而生产服务所使用的劳动则是非生产性劳动。斯密的观点其实非常的朴素，资本应当用于再生产。他的观点，如果一个做法对个体是正确的，那么对国家也是正确的，因此，国家的资本越多，就更应该用于支持生产性劳动。在斯密的时代，他并没有意识到第三产业的作用。</p>
<p>斯密强调，将大量收入分配给进行储蓄和投资的资本家，将少量收入分配给地主，可以获得最高的经济增长率。某种程度上，是不是也可以看作是需要大力扶持实业。此外，因为经济增长收到政府非生产性劳动支出的约束，例如军队，所以拥有较小的政府，就可以对资本家征收较少的税，以便他们可以积累更多的资本。这里有一点点小国寡民的意味。</p>
<h2 id="对国民财富的总结">对国民财富的总结</h2>
<p>其实在斯密的观点中，这一点一句话就可以概括，资本是国家财富的主要决定因素。</p>
<p>对斯密而言，资本积累毫无疑问要求一个自由市场与私人财产的制度框架。在自由市场体制中，既定的投资支出水平，在没有政府指导的运转中被加以分配，以确保最高的经济增长率。在私人财产体制中，对高资本积累率的进一步要求就是不平等的收入分配。</p>
<h1 id="国际贸易">国际贸易</h1>
<p>在斯密的观点中，只有错误地认为一个国家的财富取决于它所持有的贵金属和借据，贸易顺差才是有利的。而斯密的主张是非规制的对外贸易，理由是如果英国能够以低于法国的成本生产一种产品，例如羊毛，并且法国可以以低于英国的成本生产另一种产品，例如葡萄酒，那么两国各自用生产成本较低的产品，去交换生产成本较高的产品，这种交换对双方来说都是有利的。也就是现在经济学中的对外贸易的绝对成本学说。实际上这种学说不局限于国际贸易，还适用于一国的内部贸易。</p>
<p>在现代经济学的观点中，随着劳动越来越专业化，存在收益递增（成本递减）的情况。斯密的对外贸易优势的部分观点，明显基于收益递增这一动态概念。但是这种观点是有非常明显的缺陷的，事实上，任何一国国家都不可能长期保持一种生产方式不变。</p>
<p>在这个方面，古典经济学和重商主义者有一个重大的区别。重商主义者认为国际贸易是一种零和博弈，而古典经济学认为不是。但是斯密只是认识到贸易对各国都有利，而没有认识到贸易过程中的价格机制。这一点在之后的李嘉图等人的理论中得到解释。</p>
<h1 id="价值理论">价值理论</h1>
<p>区分价值与价格困惑了早期经济学家。这主要集中于三个问题：（1）什么决定了产品的价格？也就是什么决定了相对价格？（2）什么决定了价格总水平？（3）什么是福利的最佳度量标准。实际上，斯密也没有非常明确给出答案。</p>
<h2 id="相对价格">相对价格</h2>
<p>按照现代的经济学术语，相对价格指的是商品间的价格比例关系。实际上相对价格是由李嘉图提出来的。另外一个概念是绝对价格，绝对价格其实就是用货币单位表示的价格水平。</p>
<p>斯密认为，市场价格或者短期价格是由供需双方决定的。自然价格或者长期均衡价格通常取决与生产成本。在现代经济学里面，我们认为自然价格是达到供需平衡时候的价格。</p>
<p>斯密对他所处时代经济体中相对价格形成的分析，区分为两个时间段和经济体的两个宽泛的部门，分别是短期与长期、农业与制造业。在短期或者市场阶段，斯密在制造业与农业中都发现了乡下倾斜的需求曲线与向上倾斜的供给曲线；因此市场价格取决于需求与供给。斯密对长期中发生的更为复杂的“自然价格”的分析，包含着一些矛盾。对于农业部门而言，自然价格取决于供给与需求，原因在于长期供给曲线向上倾斜，表明成本递增。但是对制造业部门来说，长期供给曲线有时假定为完全富有弹性（水平的），表明成本不变；在分析的另一些地方又向下倾斜，表明成本递减。在制造业中，当长期供给曲线完全富有弹性时，价格就完全取决于生产成本；但是，当长期供给曲线向下倾斜时，自然价格就取决于需求与供给双方。不过，无视长期供给曲线在制造业中的形状，主要强调生产成本对自然价格的决定，这是斯密以及后来的古典经济学家的特点。</p>
<p>经院哲学对相对价格问题感兴趣，因为他们关注交换过程中的道德问题；重商主义者则是认为财富在交换过程中产生。斯密认为，一旦经济体实行专业化和劳动分工，交换就变成必需。如果交换发生在斯密时代的市场中，就会出现一些显而易见的问题。第一，如果交换处于高于物物交换水平的状况下，就会存在交换媒介的问题。第二，存在价值或者相对价格的问题。</p>
<h2 id="价值的含义">价值的含义</h2>
<p>斯密认为价值一词有两种不同的含义，它有时表示一些特定物品的效用，有时又表示因占有物品而取得的对其他产品的购买力。前者可以称为“使用价值”，后者被称为“交换价值”。使用价值很大的东西，其交换价值往往很小，甚至没有；相反，交换价值很大的东西，其使用价值往往极小。就像水的使用价值非常高，但是交换价值很低；钻石的交换价值很高，但是使用价值非常低。</p>
<p>这里的交换价值是指一种商品购买其他产品的能力。这是市场所表达的一种客观度量。他关于使用价值的概念是含糊的。一方面，使用价值有道德内涵，这是对经院哲学的回归。另一方面，使用价值是一件商品满足需要的能力，是因持有或消费一件产品而获得的效用。当一件商品被消费时，可以获得几种效用：（1）它的总效用，（2）它的平均效用，（3）它的边际效用。斯密的关注点是总效用，这就模糊了他对需求如何在价格决定中发挥作用的理解。显然，水的总效用超过了钻石的总效用。然而，因为商品的边际效用经常随着其消费得更多而递减，所以水的边际效用比钻石的边际效用低。</p>
<p>我们愿意为一件商品所支付的价格——我们对获得又一单位商品所寄予的价值——不仅取决于商品的总效用，而且取决于其边际效用。因为斯密没有意识到这一点，因此他既不能为“钻石-水悖论”找到满意的解决办法，也不能了解其使用价值与交换价值之间的关系。某种程度上是不是认为这是供给量上的区别？大多数情况下水比钻石多，所以边际效用就低。</p>
<h2 id="斯密关于相对价格">斯密关于相对价格</h2>
<p>因为斯密对相对价格的决定因素有些困惑，所以，他发展了与这些因素相关的三个独立的理论：（1）劳动成本价值理论，（2）劳动支配价值理论，（3）生产成本价值理论。</p>
<p>他假设了经济体两种截然不同的状态：初期野蛮状态或者原始社会，它被界定为这样的一个经济体，其中资本还没有被积累起来，土地未被使用；发达经济体，其中资本与土地不再是资源充足的产品（它们具有超过零以上的价格）。</p>
<h2 id="原始社会中的劳动成本理论">原始社会中的劳动成本理论</h2>
<p>如果在一个狩猎国家，杀死一头海狸所需的劳动通常两倍于杀死一头野鹿所需要的劳动，那么，一头海狸自然就可以换来或者值两头野鹿。按斯密的劳动成本理论，在还不存在土地与资本的经济体中，或者土地与资本还是无限充足的自然资源的经济体中，一件产品的交换价值或者价格，由生产产品所需的劳动量决定。</p>
<p>这让我们认识到劳动成本价值理论的第一个难点。我们应当如何度量生产一件商品所需要的劳动量。斯密认识到生产一件产品所需的劳动量不能简单地用时钟表示的时间数量来度量，原因在于，除了时间之外，也必须考虑有关的精巧或者技能，以及任务的艰难与困苦。</p>
<p>在这一点上，斯密遇到了所有的劳动成本价值理论都遇到的，扔未被后来的经济学家成功地予以解决的一个难题。如果劳动量是一个以上变量的函数，那么，我们必须找到一种方法来说明所有变量的相对重要性。斯密主张时间、艰难程度以及精巧程度上的差异都反映在支付给劳动者的工资中。</p>
<p>但是斯密的观点只是重申了问题，他是在表明一件产品是依照支付给劳动者的工资，而不是依照包含在产品中的劳动量拥有价值。这是一个循环推论。斯密利用一套价格解释另一套价格。</p>
<h3 id="原始社会中的劳动支配">原始社会中的劳动支配</h3>
<p>按照斯密的观点，一件产品的价值“对于那些拥有产品的人以及那些想用它去交换一些新产品的人来说，正好等于产品能够使他们购买或支配的劳动量”。也就是说，如果捕获一头海狸或者两头野鹿需要两小时，那么两头野鹿就等于一头海狸。</p>
<h3 id="发达经济体中的劳动理论">发达经济体中的劳动理论</h3>
<p>因为资本已经被积累，土地也已经被利用，并且一件产品的最终价格也必须包括当做利润的资本家的收益以及当做地租的地主的收益。最终价格形成了由工资、利润、地租这些要素报酬构成的收入。</p>
<h3 id="相对价格的生产成本理论">相对价格的生产成本理论</h3>
<p>斯密最终放弃了任何劳动价值理论都适用于他所处时代一样发达的经济体。斯密似乎发现，一旦资本被积累起来，土地被加以利用，并且一旦必须支付利润、地租，还有劳动，能唯一适当解释价格的就是生产成本理论。</p>
<p>在成本理论中，一件商品的价值取决于对所有生产要素的支付：除了劳动之外还有土地和资本。在斯密的体系中，利润这一术语既包括今天的利润，也包括利息。在斯密假设平均成本不随着产量的增加而增加的地方，无论使用总成本还是使用平均成本，通过加总工资、利润和地租，这样的相对价格都是不变的。在平均成本随着产量变化的地方，价格就取决于需求与供给双方。然而，在分析长期自然价格的决定时，即使当供给曲线不被假定为完全富有弹性时，斯密也强调供给与生产成本。斯密主张，竞争占优势的地方，商人、劳动者、地主的私利将导致与生产成本相等的自然价格。</p>
<h1 id="分配理论">分配理论</h1>
<p>收入的个人分配取决于个人所出售的生产要素的价格与数量。劳动是大部分家庭拥有的唯一生产要素，因此家庭的收入一般取决于工资率与工作时间的长度。拥有财产的那些家庭所获得的财产收入量，取决于家庭所拥有的资本与土地的数量以及这些要素的价格。因为工资、利润、地租都是经济体中的价格，所以它们的相对价格——连同个人出售的劳动、资本、土地数量一起——决定了收入的分配。</p>
<h2 id="工资">工资</h2>
<p>斯密认为，在对工资的讨价还价过程中，劳动处于劣势。因为劳动市场是买方市场，雇主少，可以容易联合起来巩固他们的地位。即使罢工，雇主也有足够资源维持他们的生活，但是没有工作，工人生存困难。在这一部分，斯密削弱了市场力量的有益运作过程，并似乎已经意识到其完全竞争市场的假设受到了限制。</p>
<h2 id="工资基金">工资基金</h2>
<p>因为生产过程是耗时的，所以从生产过程开始到最终销售，需要一部分的产品库存或资本来维持劳动者的衣食住行，这部分被称为工资基金，来源于资本家的储蓄或者消费中断。给定劳动力和工资基金的规模，工资率=工资基金/劳动力。</p>
<h2 id="利润">利润</h2>
<p>斯密很自然接受了利润是因资本家执行了对社会有用的功能而对他的一种支付，这种功能就是在耗时的生产过程期间，为劳动者提供生活必需品，提供工作所用的原料和机器。按照斯密的观点，劳动者允许从其产量中进行利润的扣除，原因在于，劳动者并不拥有工作所用的原料和独立的支持手段。于是利润分为两部分：纯利息收入和风险收入。在原始经济体中，劳动者获得了全部的产品，但是在发达经济体中，劳动者却需要被扣除利润和地租，这一点斯密并没有做出解释。在深信资本主义制度基本和谐的理论家里，这一点是非常自然无需质疑的。</p>
<h2 id="地租">地租</h2>
<p>斯密提出了四种地租理论：（1）地主的需求；（2）垄断；（3）差异化的优势；（4）大自然的施舍。在《国富论》前面的部分，地租被视为决定价格的因素，而后面则视为价格被决定的因素。</p>
<h2 id="随时间变化的利润率">随时间变化的利润率</h2>
<p>斯密认为一个国家的经济增长取决于资本积累。他预测，随着时间的推移，利润率会下降，原因有三。（1）劳动市场的竞争。资本家的竞争导致工资上升利润下降。（2）商品市场的竞争。生产者竞争家具，商品价格下降，利润减少。（3）投资市场的竞争，因为投资机会有限，所以资本的积累增加会导致利润下降。</p>
<h1 id="福利与价格总水平">福利与价格总水平</h1>
<p>斯密努力去发现：第一，决定价格总水平的因素；第二，不同时期福利变化的最佳度量。其实这个问题还是比较复杂的，对于一个生产两种或者更多产品的经济体来说，有没有可能界定和度量其福利的变化。</p>
<p>如果用总消费或者产量来界定福利，那么需要解决的问题就是，寻找一种度量总消费或者总产品数量的方式。通常而言，这种度量方式就是国家的货币单位。现在的社会，我们通常会用GDP来衡量。</p>
<p>但是这里会有另外一个问题，那就是货币的本质是一般等价物。这就意味着，货币也是有价格的，它本身也会变化。所以斯密转向劳动，却发现劳动价格也会变化。最后，他采用劳动的复效用作为衡量标准。也就是说，如果我们能够使用较少的劳动生产相同的产量，那么我们就拥有更多的线下，经济状况就会更好。</p>
<p>事实上这种度量方式比斯密想象的要复杂的多，现在的经济学家在这一方面有了更长远的度量方式，例如度量“生活质量”。</p>
<p>斯密在经济学史上如此之重要，他对自由放任体系的推崇引导了西方市场经济数百年的发展。</p>
]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
  </entry>
  <entry>
    <title>经济思想史读书笔记——早期古典经济学</title>
    <url>/economics_thoughts_notes2/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>“我们从哪儿出发？”红皇后问。 “从开始出发。”渡渡鸟答道。——Lewis Carroll</p>
<span id="more"></span>
<p>早期的经济研究并不系统，没有出现重大的分析体系。直到18世纪中期，随着Adam Smith引领的“古典经济学”的出现，经济学才向着成熟的社会科学状态大踏步前进。</p>
<p>古典经济学始于1776年Adam Smith《国富论》的出版，通常将1776年之前划分为两个部分。公元前800年到1500年的早期前古典阶段，以及从1500年1776年的前古典时期。</p>
<p>从时间线上看，早期前古典经济思想可以分为中国、希腊、阿拉伯——伊斯兰思想以及经院哲学。</p>
<h1 id="东方经济思想">东方经济思想</h1>
<p>中国的早期经济思想以管仲为代表。熊彼特认为早期的著作“基本上被限制在道德的框架内考察公共事务管理，而不是严格的、‘科学的’研究”。但是《管子》极大程度上超越了公共事务管理的模子。</p>
<p>管仲的经济思想核心可以用“轻/重”理论来表达。他提出当一种产品充裕时，就会变“轻”，价格就会下降。当产品被“锁藏”时，就会变“重”，价格上涨。产品会依据这种变化进入或退出市场，最后向一个均衡的价格运动。</p>
<p>事实上，这种理论是非常类似微观经济学里的供需理论。只不过，这样的供需理论需要基于一个基本的前提，就是市场是完全竞争的市场。</p>
<p>管仲甚至将这一理论延伸到了货币数量。也就是说，货币“重”的时候，货币价格上涨，产品价格就下降了，反之，货币价格下降，产品价格上涨。如果类比到现在的概念，就是通货紧缩和通货膨胀。而管仲认为，应对通货紧缩时候国家应该购买产品，目的是让价格上涨；而通货膨胀的时候就应该卖出产品，让产品价格下降。</p>
<p>当代应对通货膨胀或者通货紧缩的问题一般都是采取货币政策，也就是央妈通过操控市场上的现金解决问题。最简单的，通货膨胀时，减少货币供给量，提高银行准备金率，提高储蓄利率等等。而通货紧缩就反过来。</p>
<p>齐桓公曾向管仲询问，丰收之年如何来遏制粮食价格的过度下滑。管仲认为应当减少粮食的流通量，为此必须采取政府强制手段，他说：“请以令与大夫城藏，使卿诸侯藏千钟，令大夫藏五百钟，列大夫藏百钟，富商蓄贾藏五十钟。内可以为国委，外可以益农夫之事。”这里需要注意的是，在春秋时代粮食实际上不仅是一种商品，也具备一定的货币功能。管仲认为强制减少粮食流通量，自然可以制止价格的下滑。既可以增加国家的粮食储备，又可以维护农民的利益；相反，在歉收之年，则将储备之粮食投入市场。以遏制粮商囤积哄抬价格，又可以使政府得到一笔收入，所以管仲说：“夫民有余则轻之，故人君敛之以轻；民不足则重之，故人君散之以重。敛积之以轻，散行之以重，故君必有十倍之利。”</p>
<p>如果将粮食当做是一种流通货币，那么这不就是现在的货币政策么？</p>
<p>在管仲的思想体系下，一方面存在市场“无形的手”，另一方面，又存在国家调控这样“有形的手”。是不是听上去又很像某种经济形式。</p>
<h1 id="希腊思想">希腊思想</h1>
<h2 id="赫西奥德和色诺芬">赫西奥德和色诺芬</h2>
<p>赫西奥德和色诺芬都对效率感兴趣。经济学家通常用产出和投入的比率衡量效率。早期的经济学家对社会层面的效率问题并不感兴趣。他们更多关注一组和生产者与家庭层面的效率相关的问题。而色诺芬则在赫西奥德之后采用了有效管理的概念，并且在家庭、生产者、军事和公关事务层面使用了这些概念。这种思想的进步在于，他充分认识到了劳动分工可以提高效率。</p>
<p>这种思想影响了包括亚里士多德在内的很多希腊经济学家。并且在之后的时间里也影响了经院哲学。</p>
<h2 id="亚里士多德">亚里士多德</h2>
<p>德谟克利特认为，不仅需要劳动分工，而且认为财产私有化可以促进更多的经济活动。但是亚里士多德的老师柏拉图则认为，统治者不应该拥有私人财产，而应该掌控公共财产，避免对财产的争夺。</p>
<p>但是亚里士多德则不一样。亚里士多德一方面谴责追求经济利益的行为，一方面又认可私有财产的权利。亚里士多德对经济思想的最大贡献在于商品交换与交换中货币的使用。</p>
<p>亚里士多德认为人的需要是适度的，但是欲望是无穷的。因此满足需要的商品生产是恰当的，但是力图满足无穷欲望的产品生产就是不正常的。他推崇以物易物的交易方式，而不是通过货币这一媒介，通过交易获取货币收益。事实上，现在主流的经济学家不区分人类的需求和欲望，在高度分工化的现在，客观上区分需求和欲望是不太现实的。</p>
<p>亚里士多德一个值得关注的论点是，减少消费来改变人们的态度，希望通过排除稀缺性固有的冲突来解决社会冲突。这是大多数乌托邦人士和社会主义者的有力观点。</p>
<h1 id="阿拉伯伊斯兰思想">阿拉伯——伊斯兰思想</h1>
<p>阿拉伯——伊斯兰思想填补了亚里士多德到经院哲学之间的空白。</p>
<p>艾布·哈米德·安萨里认识到了物物交换的困难，以及货币对交易的便利。同时他也考察了很多其他的经济话题，例如公共支出、征税与借贷等等。</p>
<p>而伊本·赫勒敦考察的更多是人口、利润、供给、需求、价格、奢侈品、总剩余、资本等。</p>
<h1 id="经院哲学">经院哲学</h1>
<p>经院哲学的基础是封建社会。经院哲学经济学家们试图提供适用于世俗活动的宗教指导方针。他们的的目的不是分析发生了什么样的经济活动，而是订立与宗教教义相一致的经济行为规则。经院哲学关注价格体系中的公正或公正缺失。最重要的经院哲学经济学家是圣托马斯·阿奎那。</p>
<p>经院哲学的核心经济问题还是：私有财产制度以及公平价格与高利贷的概念。</p>
<p>在私有财产方面，阿奎那的杰出贡献在于，他融合了宗教教义与亚里士多德的著作。在《圣经》中，基督教思想谴责私人财产，但是他认为，私人财产不是违背自然法则，而是自然法则的一种补充。他论证说，裸体是自然法则，但是服装是对自然法则的补充，私有财产也是为了人们的利益而设计的。他认可私人财产的不平等分配，但是对于坚定投身宗教的人，短缺与公共生活是一种理想状态。</p>
<p>而对于产品价格，经院哲学经济学家不关心经济体中价格的形成，或者去了解价格在稀缺资源配置中所扮演的角色。他们关心价格的道德性，也就是价格的公平与公正的概念。不同的理论史学家对公正价格的理解是不一样的。但是经院哲学的公正价格可以看作是李嘉图——马克思劳动价值理论、边际效用观点，以及古典——新古典理论中按时的竞争性市场产生理想公正价格观点的先驱。另一些主张也认为，所谓的公正价格就是市场上的所有价格。但是经院哲学对于经济分析的缺失让我们难以确定究竟“公正价格”意味着什么。</p>
<p>从公正价格会很自然推论到高利贷的观点。高利贷在现在代表着索要过高的利率，但是在经院哲学和亚里士多德的著作里，高利贷代表着任意的（any）获利行为。但是相对于亚里士多德对借贷的谴责以及宗教对通过货币获利的严厉禁止，经院哲学至少在为了商业目的通过货币获利这点上观点逐渐缓和。</p>
<p>阿奎那本身也是一个充满矛盾的思想家，一方面他强调道德问题来抑制经济思想；另一方面，又推动了经济学与所有社会科学的前进。</p>
<h1 id="小结">小结</h1>
<p>事实上，早期的经济思想并没有关注价格系统的特性与关注，只有管仲是例外的。而希腊思想则考察了私人财产的作用，亚里士多德的许多观点成为后来经院哲学的考察焦点。阿拉伯——伊斯兰思想有效填补了希腊思想到经院哲学之间的历史空白。而经院哲学的目的更多在于确定宗教标准，借此判断经济行为，而不是分析经济体。不过，经院哲学的存在背景是封建制度，随着技术变革的破坏，经济生活对精神生活构成了极大的挑战。但是从历史的进程上看，将经济体从教会下解放出来，既发生在实践层面上，也发生在学术层面上。实际上，这种解放极大促进了西方资本主义的发展，从而构建起现代金融货币体系。关于货币从宗教解放的资料可以看看央视的纪录片《货币》第二集，犹太人最早从宗教总解脱出来，施展金融才华，这一步在货币历史上至关重要。</p>
]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
  </entry>
  <entry>
    <title>遗传算法</title>
    <url>/genetic_algorithm/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>遗传算法的简要实现</p>
<span id="more"></span>
<p>遗传算法的理论基础其实很简单，将每一个解当作种群中的一个个体，足够多的解构成一个种群。然后各种迭代，迭代过程中随机替换两个解中的元素，模拟基因重组；或者按照一定概率变异，改变一个解中的某个元素，模拟基因突变。然后适者生存就找到最优解。</p>
<p>整个算法的理解上没啥数学的东西，不懂为什么能把这玩意儿说明白的就能被当作大神。另外，如果不是正经学生物的，别太较真，反正我是不能理解单倍体生物在有丝分裂的时候怎么实现交叉的。另外需要注意的是，为了提高寻优的效率，突变概率，交叉概率都比自然界中真实的概率高多了。</p>
<p>所以实现上基本上可以拆成以下几部分的功能：</p>
<ul>
<li><p>生成种群</p></li>
<li><p>基因重组</p></li>
<li><p>突变</p></li>
<li><p>适者生存</p></li>
</ul>
<p>这里需要注意，适者生存有多种实现方法，最简单的例如每次取前10%。稍微符合自然规律的可以使用一些轮盘法啥的，简单理解，其实同性恋在野生环境下是不具有任何生存优势的，因为同性无法产生后代。但是同性恋有时候会产生意想不到的效果，例如《伊利亚特》里的阿喀琉斯和帕特罗克洛斯。所以经过几万年的进化，仍然有同性恋存在，在这些同性恋中，可能存在某个最优解。</p>
<blockquote>
<p>生成种群 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generatepop</span>(<span class="params">lowerbound, upperbound, popsize=<span class="number">100</span></span>):</span></span><br><span class="line">    <span class="comment"># 生成种群</span></span><br><span class="line">    <span class="comment"># lowerbound, upperbound很好理解，设定值域</span></span><br><span class="line">    <span class="comment"># popsize设定种群中个体数量</span></span><br><span class="line">    pop = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(popsize):</span><br><span class="line">        vec = [random.randint(lowerbound[j], upperbound[j])</span><br><span class="line">               <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(lowerbound))]</span><br><span class="line">        pop.append(vec)</span><br><span class="line">    <span class="keyword">return</span> pop</span><br></pre></td></tr></table></figure></p>
</blockquote>
<blockquote>
<p>基因重组 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">crossover</span>(<span class="params">gene1, gene2</span>):</span></span><br><span class="line">    <span class="comment"># 基因重组</span></span><br><span class="line">    i = random.randint(<span class="number">1</span>, <span class="built_in">len</span>(gene1) - <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> gene1[<span class="number">0</span>:i] + gene2[i:]</span><br></pre></td></tr></table></figure></p>
</blockquote>
<blockquote>
<p>基因突变 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mutate</span>(<span class="params">gene, lowerbound, upperbound, mutation_prob</span>):</span></span><br><span class="line">    <span class="comment"># 基因突变</span></span><br><span class="line">    new = gene.copy()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(new)):</span><br><span class="line">        tmp = random.randint(lowerbound[i], upperbound[i])</span><br><span class="line">        <span class="keyword">if</span> random.random() &gt; mutation_prob[i] <span class="keyword">and</span> new[i] != tmp:</span><br><span class="line">            new[i] = tmp</span><br><span class="line">    <span class="keyword">return</span> new</span><br></pre></td></tr></table></figure></p>
</blockquote>
<blockquote>
<p>主函数（这里适者生存使用了最简单的排序取最优） <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">geneticoptimize</span>(<span class="params">lowerbound, upperbound, cost,</span></span></span><br><span class="line"><span class="params"><span class="function">                    crossover_prob=<span class="number">0.3</span>, elite=<span class="number">0.1</span>, maxiter=<span class="number">100</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                    popsize=<span class="number">100</span>, argmin=<span class="literal">True</span></span>):</span></span><br><span class="line">    populations = generatepop(lowerbound, upperbound, popsize)</span><br><span class="line">    <span class="comment"># mutation_prob是一个list，表示每个DNA突变概率不同</span></span><br><span class="line">    mutation_prob = [random.random() * <span class="number">0.8</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(lowerbound))]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(maxiter):</span><br><span class="line">        scores = [(cost(v), v) <span class="keyword">for</span> v <span class="keyword">in</span> populations]</span><br><span class="line">        <span class="keyword">if</span> argmin:</span><br><span class="line">            scores.sort()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            scores.sort(reverse=<span class="literal">True</span>)</span><br><span class="line">        ranked = [v <span class="keyword">for</span> (s, v) <span class="keyword">in</span> scores]</span><br><span class="line">        <span class="comment"># 适者生存</span></span><br><span class="line">        elites_size = <span class="built_in">int</span>(elite * popsize)</span><br><span class="line">        populations = ranked[<span class="number">0</span>:elites_size]</span><br><span class="line">        <span class="comment"># 开始产生后代</span></span><br><span class="line">        <span class="keyword">while</span> <span class="built_in">len</span>(populations) &lt; popsize:</span><br><span class="line">            <span class="keyword">if</span> random.random() &gt; crossover_prob:</span><br><span class="line">                c1 = random.randint(<span class="number">0</span>, elites_size)</span><br><span class="line">                c2 = random.randint(<span class="number">0</span>, elites_size)</span><br><span class="line">                populations.append(crossover(ranked[c1], ranked[c2]))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                c = random.randint(<span class="number">0</span>, elites_size)</span><br><span class="line">                populations.append(mutate(ranked[c], lowerbound,</span><br><span class="line">                                          upperbound, mutation_prob))</span><br><span class="line">        <span class="built_in">print</span>(scores[<span class="number">0</span>][<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">return</span> scores[<span class="number">0</span>][<span class="number">1</span>]</span><br></pre></td></tr></table></figure></p>
</blockquote>
<p>详细可以看我写的渣<a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/sp/optimization/genetic_algorithm.py">code</a>。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>线性代数 01</title>
    <url>/linear_algebra_step1/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>线性代数基本概念。</p>
<span id="more"></span>
<h1 id="vector">Vector</h1>
<p>vector就是一组数字，有两种，一种是row vector，一种是column vector。一般而言，我们没有特殊声明，说一个vector就是一个column vector。</p>
<p><span class="math display">\[
\text{row vector: } \begin{bmatrix} 1 \ 2 \ 3 \end{bmatrix} \\
\text{column vector: } \begin{bmatrix}1 \\ 2 \\ 3 \end{bmatrix}
\]</span></p>
<p>通常来说，我们用小写加粗的字母表示一个向量，比如<span class="math inline">\(\mathbf{v}\)</span>，或者<span class="math inline">\(\vec{v}\)</span>，不过一般来说出于方便，基本上也用普通小写的字母表示。每一个向量的元素用相同字母带上下标表示，比如<span class="math inline">\(v_i\)</span>。</p>
<p>向量常见运算如下： <span class="math display">\[
c \vec{v} = [cv_i] \\
\vec{a} + \vec{b} = [a_i + b_i]
\]</span></p>
<p>向量乘法其实可以看作是单维矩阵，所以放到后面矩阵部分。</p>
<h1 id="matrix">Matrix</h1>
<p>matrix就是一组vector。一般我们用大写加粗字母表示，比如<span class="math inline">\(\mathbf{M}\)</span>。每一个元素用相同字母带上下表表示，比如<span class="math inline">\(m_{ij}\)</span>，其中<span class="math inline">\(i\)</span>表示第<span class="math inline">\(i\)</span>行，<span class="math inline">\(j\)</span>表示第<span class="math inline">\(j\)</span>列。同样处于方便，很多时候直接用大写的字母表示矩阵。我们会叫一个行列相同的矩阵是方阵（square matrix）。</p>
<p>矩阵的常见运算如下： <span class="math display">\[
c \mathbf{M} = [c m_{ij}] \\
\mathbf{A} + \mathbf{B} = [a_{ij} + b_{ij}]
\]</span> 这里需要注意的是，只有两个矩阵的形状相同才可以做加减法。矩阵的加法运算是符合加法结合律和交换律的。</p>
<p>矩阵还有一个是transpose，也就是按照对角线互换元素。一般记做<span class="math inline">\(A^{\top}\)</span></p>
<p>后面讨论矩阵乘法。矩阵运算是机器学习或者深度学习最重要的事情，尤其是矩阵的乘法，求导。下面讨论矩阵的乘法。</p>
<p>这里以矩阵乘向量为例。假设有一个矩阵<span class="math inline">\(A\)</span>和向量<span class="math inline">\(x\)</span>，我们将这个记做<span class="math inline">\(Ax\)</span>这里需要注意，<span class="math inline">\(A\)</span>的列数必须跟<span class="math inline">\(x\)</span>的行数一致。</p>
<p>具体看这个计算过程是这样的： <span class="math display">\[
\begin{align}
Ax &amp; =
\begin{bmatrix}
a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1n} \\
a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mn}
\end{bmatrix}
\times
\begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{bmatrix} \\
&amp; =
\begin{bmatrix}
a_{11} x_1 + a_{12} x_2 + \cdots + a_{1n} x_n \\
a_{21} x_1 + a_{22} x_2 + \cdots + a_{2n} x_n \\
\vdots \\
a_{m1} x_1 + a_{m2} x_2 + \cdots + a_{mn} x_n
\end{bmatrix} \\
&amp; =
x_1 \begin{bmatrix} a_{11} \\ a_{21} \\ \vdots \\ a_{m1} \end{bmatrix} +
x_2 \begin{bmatrix} a_{12} \\ a_{22} \\ \vdots \\ a_{m2} \end{bmatrix} +
\cdots +
x_n \begin{bmatrix} a_{1n} \\ a_{2n} \\ \vdots \\ a_{mn} \end{bmatrix}
\end{align}
\]</span></p>
<p>这是从两个不同的角度来看待矩阵乘法，第一个是我们最习惯使用的，用左边矩阵的行，乘以右边矩阵的列。第二个是相对不那么常见的方法，用右边矩阵的行乘左边矩阵的列，然后再加起来。本质上是一样的，但是相对而言第一种方法我自己比较习惯。矩阵乘矩阵其实就是将右边的矩阵拆成一个个vector来乘，不赘述。矩阵乘法最后得到的结果形状是<span class="math inline">\(A_{mp} B_{pn} = M_{mn}\)</span>，也就是左边的行数，右边的列数。</p>
<p>这里说的是矩阵叉乘矩阵的计算方法，还有一种是矩阵点乘矩阵，英文上前者是product，后者是element-wise product。一般我们是将点乘记成<span class="math inline">\(A \odot B\)</span>，这里要求两个矩阵形状一致。</p>
<p>点乘的计算规则就很简单了，就是各个位置的元素相乘。计算方式是： <span class="math display">\[
A \odot B =
\begin{bmatrix}
a_{ij} b_{ij}
\end{bmatrix}
\]</span></p>
<p>最基本的矩阵运算就是这样。推荐一个参考书，<a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf">Matrix Cookbook</a>，可以作为日常参考书用。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>经济思想史读书笔记——李嘉图&amp;马尔萨斯</title>
    <url>/economics_thoughts_notes5/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>古典经济学，李嘉图与马尔萨斯部分。主要是李嘉图，简直是个宝藏男孩。一个人引导了整个经济界，构建了引导经济思想一百年的理论框架。</p>
<span id="more"></span>
<h1 id="一马尔萨斯人口学说">一、马尔萨斯人口学说</h1>
<p>马尔萨斯的人口学说成因：</p>
<ol type="1">
<li>人口对英国食物造成压力</li>
<li>工业化带来的收入分化</li>
<li>为了证明社会制度以及政府制度不是贫困与不幸的成因</li>
</ol>
<p>人口学说的基本假设：</p>
<ol type="1">
<li>食物对人类生存是必须的</li>
<li>性爱需求是必须且不会变化的</li>
</ol>
<p>马尔萨斯的人口学说假设实质上否定了技术发展对解决人口问题的可能性，比如说安全套、避孕药这种避孕手段，或者是我国杂交水稻等生命科学手段。因此马尔萨斯认为，若不控制人口，人口增长率将会超过食物供给增长率。</p>
<p>马尔萨斯在他第一版理论中提出积极的和预防的两种人口控制方法。积极的人口控制方法就是通过战争、饥荒、疾病和类似的灾难来提高死亡率。预防的方法就是通过降低出生率，主要通过延缓婚姻实现。但是马尔萨斯认为，延缓婚姻只会导致恶行、不幸以及人性退化。因为它会产生婚前性行为。因此人类对性和食物的渴求无法通过制度变革。这使得经济学被称为“沉闷的科学”。</p>
<p>马尔萨斯的第二版理论使用了科学的统计数据来支持他的论点。在第二版中，马尔萨斯引入新的人口控制方法，也就是道德控制，或者说是没有婚前性行为的延缓婚姻。</p>
<p>马尔萨斯的人口论题并没有讨论通过避孕控制人口的可能性。此外，马尔萨斯对性冲动的本能欲望感到困惑。事实上随着富裕程度和受教育水平的提升，在性欲和生孩子之间人们会自己做出取舍。换而言之，虽然人类的性欲依旧是旺盛的，但是人类可能并不想抚养一个孩子。所以，在这种情况下，人们会采用各种各样的避孕手段来享受性爱，同时避免生育。其实从某种程度上来说，做爱这事情挺麻烦的，至少要两个人，还要两个人互相配合，随着经济水平的发展，性爱频率实际上会下降。最近的一些文章也支持了这种观点，有兴趣的可以自行搜索。</p>
<p>另一个马尔萨斯面临的难题是马尔萨斯没有考虑农业科技的进步可以能使食物供给提高。不过经济学家从来不发展解释技术进步比率的理论。因此，历史上经济学家容易低估科学进步对经济体的影响。</p>
<p>结合马尔萨斯的人口学说，以及斯密的工资基金学说，得到经济学中一个非常重要的概念，工资铁律（the iron law of wages）。也就是说，提高低收入人群的经济福利将带来人口规模的增长，那么工资率就又恢复到以前的水平。因此，工资将会一直保持在维持最低生活成本的水平上。</p>
<h1 id="二李嘉图">二、李嘉图</h1>
<p>李嘉图代表了忙碌的纯理论家，他从他所处的时代的经济体中进行抽象，构建了基于演绎方法的一种分析。他的主张理论是具体分析现实世界的政策问题的先决条件。李嘉图的方法更像是斯密方法的一脉相承，斯密的方法是：（1）运用演绎理论来分析他所处时代的经济体。（2）呈现同时代人描述性的、非正式的叙述以及呈现历史上的制度。</p>
<p>李嘉图所处的时代面临着升高的谷物价格，升高的地租以及应该经济结构变革导致的工业的相对增长与农业的相对下降。这些问题最后都集中在一个关键点，也就是实行自由国际贸易还是管制的国际贸易。</p>
<p>从农场主角度考虑，规制的国际贸易可以免受外来农产品的竞争，而新兴的工业家则可以进口廉价的原材料。</p>
<p>李嘉图主义的两个成分一直保留到了今天：高度抽象和基于抽象模型的非关联的政策制定。</p>
<p>李嘉图是经济学基本任务这一观念上的转折点。斯密延续了重商主义对国民财富决定力量的关注，而李嘉图则主张经济学的主要目的是确定支配地主、资本家以及劳动者之间收入分配的法则。</p>
<p>李嘉图专注于现在被称为收入的功能性分配的研究，收入的功能性分配是指年产量流向劳动、土地以及资本的相对份额。他在由三个阶级组成的社会背景下考虑这一问题：获得利润与利息的资本金、获得地租的地主以及获得工资的劳动者。为了解释利润、利息、地租、工资，李嘉图不得不和斯密一样从经济体的微观层面进行理论阐述。因此，他将后来的经济研究引导到微观经济问题，而不是宏观经济问题，这恰好与他的期望相反。此外，在宏观经济稳定问题上，他对马尔萨斯的胜利，在将近一个世纪中结束了正统理论家对这一问题的进一步争论。</p>
<h2 id="李嘉图的模型">李嘉图的模型</h2>
<p>李嘉图的模型有三个主要群体：资本家、劳动者、地主。在经济体中，资本家是主要角色，他们是生产者、指挥者以及最重要的参与者。他们有两个主要功能：一、有助于资源的有效分配；二、通过储蓄和投资来拉动经济增长。</p>
<p>尽管李嘉图采用劳动成本理论来解释不同时期相对价格的变化，但是在他的模型里面，劳动本质上是被动的。他用工资基金学说和马尔萨斯的人口理论来解释劳动者的实际工资（real wage）：实际工资=工资基金/劳动力。这就是前面提到的工资铁律。</p>
<p>当然，不同国家地区的最低生活水平是不一样的，因此，维持一个人最基本的生活成本的工资也是不一样的。比如说，同样是“贫困”人口，美国的贫困工人与尼泊尔的贫困工人也是不一样的。</p>
<p>在李嘉图的体系中，地主就是蛀虫。在李嘉图看来，土地的供给曲线是完全无法动弹的，土地的社会机会成本为0。地主获得地租收入，仅仅是因为拥有一种生产要素，他并没有提供任何对社会有益的作用。古典经济学家认为，地主阶级的行为（主要指地主阶级的财富都用于消费，而不是储蓄和投资）有害于新型工业社会的增长与发展。</p>
<p>李嘉图的模型表达了国民财富增长与三个主要经济群体之间的关系：总产量或者说经济体的总收入分配给了劳动者、资本家和地主。总产量中的一部分，没有用来支付劳动者维持最低生活水平的工资，也没有用来替换生产过程中报废的资本产品，这一部分可以被称为净收益或者经济剩余。也就是“总收益-（维持最低生活水平的工资+折旧）=净收益”。因此，净收益由利润、地租以及维持最低生活水平的工资之上的部分组成。因为长期均衡中工资铁律的存在，净收益就等于利润加上地租。工人与地主都将他们的全部收入花在消费上，因此，利润成为了储蓄或资本积累的唯一来源。利用他的地租理论，李嘉图断定，作为经济增长率减小的结果，当利润下降、地租上升的时候，随着时间的变化，将会发生有利于地主的收入再分配。</p>
<p>李嘉图要解决经济问题，面临着一个时代难题，《谷物法》。《谷物法》是对进口到英国的谷物征收关税的规定。对于《谷物法》当时有很多主张。一种主张认为高关税会鼓励农业更多的投资，导致产量的上升，价格的下降。另一种主张认为，谷物的高价格是高地租的结果。但是李嘉图认为，《谷物法》的根本问题在于收入的分配。较高的关税将使得收入分配有利于地主。</p>
<p>当李嘉图为了处理由《谷物法》引起的许多政策问题的时候，他利用了很多分析工具与假设。这些工具包括：（1）劳动成本理论（labor cost theory）。不同时期相对价格的变化，可以用时间度量的劳动成本的变化来解释。（2）中性货币（neutral money）。货币供给的变化，可以引起绝对价格水平相对价格水平两者的变化。李嘉图的假设是，货币供给的变化不引起相对价格的变化。（3）劳动与资本的固定生产系数（fixed coefficients of production for labor and capital）。只能使用劳动与资本投入的一种联合来生产既定的产量。换句话说，对每种类型的经济生产而言，技术上的考虑使得劳动-资本比例是固定的，且不随着产量的变化而变化。（4）制造业收益不变，农业收益递减（constant returns in manufacturing and diminishing returns in agriculture）。制造业的供给曲线是水平的，或者是富有弹性的（随着产量的增加，边际成本不变）；农业供给曲线是向上倾斜的，也就是随着产量扩张，边际成本增加。（5）充分就业（full employment）。长期中，经济体在资源充分利用的水平上趋向自动运转。（6）完全竞争（perfect competition）。市场包含很多独立的生产者，他们的产品具有同质性，任何一个单独的销售者都不能影响市场的价格。（7）经济参与者（economic actors）。个人在他们的经济活动中都是理性的和精于算计的。在完全竞争市场中，这种社会作用会导致利润率统一、工资统一以及地租统一。（8）马尔萨斯人口论题（Malthusian population thesis）。人口趋向以快于食物供给的速度增加。（9）工资基金学说（wages fund doctrine）。工资率等于工资基金除以劳动力规模。</p>
<h2 id="李嘉图的地租理论">李嘉图的地租理论</h2>
<p>李嘉图、马尔萨斯、韦斯特以及托伦斯阐述了收益递减原理，这一原理成为了一个重要的经济学概念。</p>
<p>收益递减原理表明，当其他生产要素保持不变，一种生产要素稳定增加时候，总产量的增长率最终会变小。</p>
<p>李嘉图对地租做出了定义，他将地主视为给地主的一种支付，等于不同肥力土地上的利润率。产生地租的原因在于：（1）肥沃土地的稀缺性；（2）收益递减规律。</p>
<p>这里引入集约边际（intensive margin）和粗放边际（extensive margin），并假设有三种土地，A级土地使用三个单位的劳动和资本组合，B级土地使用2个单位，C级使用一个单位。集约边际描述了连续追加资本与劳动组合对既定地块的影响。例如一个单位的组合投入A级土地，生产出100单位小麦，继续投入第二单位的组合，总产出190单位小麦，那么第二组合的边际产品为90单位。集约边际反映了边际收益递减原理。因为A级土地的边际收益下降，因此次肥沃的土地进入生产。从A级到B级，代表了粗放边际，可以简单类比为从肥沃的山谷挪动到了山腰。</p>
<p>在这种基础上，我们可以得到一个这样的表格：</p>
<table>
<tr>
<td rowspan='2'>
<td colspan='3'>
粗放边际
</tr>
<tr>
<td>
A
<td>
B
<td>
C
</tr>
<tr>
<td rowspan='3'>
集约边际
<td>
100
<td>
90
<td>
80
</tr>
<tr>
<td>
90
<td>
80
<td>
</tr>
<tr>
<td>
80
<td>
<td>
</tr>
</table>
<p>因为地租是对地主的支付，因此，它等同于不同级别土地上的利润率，那么A上的地租为30单位，B是10单位，C不产生地租。如果一单位的资本劳动组合投入到三块C土地上，那么总产品将为240单位谷物。三个单位资本劳动组合投入到一块A土地上，将产生270单位谷物。随着农场主的竞争，A土地的价格（地租）将上升，直到地租等于30单位谷物为止，从而使得两种级别土地上的利润率相等。同理，B的地租将等于10单位谷物。</p>
<p>上面是从生成成本角度考虑地租，从另一个角度，也就是产品或者产量角度来看，随着土地集约化耕种，生产谷物的边际成本上升。边际成本（marginal cost）被定义为生产最终产品的一个增加量所需要的总成本。假设一单位资本劳动组合在市场上价值100美元，那么A土地上第100单位谷物的边际成本等于1美元，第190单位谷物边际成本是1.11美元（100/90），而最后一单位就是1.25美元（100/80）。B与C的最后一单位谷物边际成本也是1.25美元。如果完全竞争市场存在，那么一定是这种结果，因为随着A的边际成本上升，B得以开始生产。如果最后一单位的边际成本不一样，那么通过转移劳动与资本来减少总生产成本就变成可行的。在长期均衡下，当三种土地的边际实物产品相等时，增加量的边际成本必定相等。</p>
<p>从成本方考虑是通过货币计算地租。对A来说，总收益是270单位谷物乘以1.25美元，也就是337.5美元。那么为什么是1.25美元的单价呢？在竞争市场里，市场价格等于最后的边际成本，也就是说，市场价格是最低效率下生产出来的谷物的边际成本。如果价格比这个成本高，将不会有人购买；比这个成本低，将不会有人出售。因此单价就是1.25美元。那么成本是300美元的资本劳动组合，因此地租是37.5美元。而B就是12.5美元。C是0。</p>
<p>那么如果我们资本劳动组合中的劳动成本是75美元，假设A和B不产生地租，也就是不产生地租，那么A的利润率是337.5-75×3，也就是112.5美元。那么一单位资本就是37.5美元。同理B一单位资本就是31.25美元。这样一来，C一单位资本只有25美元，就会进入竞价，拉低A和B的资本利润，那么这一部分的利润就变成A和B的地租。竞争最终的结果就是A、B、C三个土地的资本都是25美元。</p>
<p>这个模型揭露了地租概念和竞争性市场的几个要点：（1）市场中农场主之间的竞争将使得谷物价格等于成本最高的单位产量的边际成本；（2）对土地的竞争，将使得地租支付给拥有最肥沃土地的地主；（3）竞争将导致所有级别土地拥有统一的利润率。在李嘉图的模型中，地租是价格被决定的因素，而不是决定价格的因素。谷物的高价格不是由高地租决定的，而高地租则是由谷物价格决定的。</p>
<p>因此在《谷物法》所施加的进口限制，是的集约边际与粗放边际向下推进，其原因在于肥沃土地的稀缺性和收益递减原理。新增劳动与资本组合的边际实物产品将下降，也就是说边际成本将上升，其结果是谷物价格与地租都上升了。</p>
<p>今天大部分经济学家同意李嘉图的如下观点，即将社会作为一个整体来看，地租并不是生产成本，因此也不是价格的决定因素。土地的数量接近固定；因此当供给的数量不增加的时候，需求的增加将导致较高的价格（地租）。李嘉图将社会当做一个整体，从这个角度来考虑地租，土地的机会成本为零。然而，从社会个别成员的角度来看，地租就是生产成本，从而使价格的决定因素。对农场主而言，地租就是价格的决定因素，因为他们需要向地主支付地租。地租数量等于土地的机会成本（opportunity cost）——等于土地在可替代的其他用途上——例如，土地被用来种植不同的农作物，或者再被分——能够获得的地租数量。今天的经济学家在探讨地租性支付是决定价格的因素，还是价格被决定的因素时，区分了把社会作为一个整体来看时形成的观点，与社会个别成员的角度来看时形成的观点。</p>
<h2 id="李嘉图的价值理论">李嘉图的价值理论</h2>
<p>关于《谷物法》的争执，李嘉图的理论得到了很多发展。以马尔萨斯为代表的经济学家主张对进口谷物提高关税有利于英国。但是李嘉图赞同自由国际贸易，反对关税。他推论，高关税降低了利润率，随之将意味着较低的资本积累率。而资本积累率决定了经济增长率。</p>
<p>贸易保护论者运用生产成本价值论，主张较高的关税不会导致较低的利润。一部分贸易保护论者提出，降低或取消谷物关税将使食物价格与货币工资下降，最终导致所有价格的普遍下降，从而导致经济衰退。</p>
<p>李嘉图试图反驳当时盛行的生产成本价值理论，因为《谷物法》对经济的影响是它对收入分配的影响。而当时的价值理论试图解释既定时间上相对价格的决定力量。李嘉图认为，当时的价值理论无法解释导致不同时期相对价格变化的经济力量。</p>
<p>比如之前的海狸与野鹿交换的例子。原本2D=1B，后来变成了3D=1B。那么究竟是海狸价值变高了，还是野鹿价值变低了。这两种解释都是对的，但是如果存在一种不变价值的度量，那么我们就可以知道不同时期相对价格变化的真正原因。</p>
<p>但是李嘉图意识到并不存在这样的商品。个人觉得，即使是金本位时代，如果发现了新的金矿，实际上也是一种通货膨胀。比如哥伦布发现新大陆后的西班牙实际上是发生了通货膨胀。不过李嘉图并没有完美阐释绝对价值的度量。因此李嘉图对价值关注度主要点在于，是什么导致了不同时期相对价格的变化。</p>
<p>斯密的劳动成本理论认为，支付给劳动者的工资是对必要的劳动时间的度量。但是李嘉图认为这是一个循环推论，因为工资影响了价格，而价格又会反过来影响工资。他认为价值取决于生产所需要的劳动量，而不是支付给劳动者的工资。</p>
<p>然后，李嘉图着手解决使用价值与交换价值的混淆问题。经典的水-钻石悖论中，斯密没有看到使用价值与交换价值的度量关系。而李嘉图认为，使用价值尽管不是交换价值的度量，但是对交换价值的存在是基本的。也就是说，一个商品具有实际价格之前，必须存在一种需求，但是需求不是价格的度量，稀缺性和生成所需的劳动量才是。</p>
<p>但是存在部分商品的价格仅仅由它们的稀缺性决定。这种商品属于不能自由再生产的商品，因此它们的供给不能增加，也就是说，他们的供给曲线完全没有弹性。</p>
<p>李嘉图的价值理论只适用于能自由再生产以及在完全竞争市场所产生的商品。对于制造业而言，李嘉图假设成本不变，对于农业，他假设成本增加。</p>
<p>李嘉图放弃了劳动支配价值理论与生产成本价值理论，主张劳动成本价值理论才是合适的。但是劳动成本价值理论存在5个基本难题。</p>
<ol type="1">
<li>度量劳动量。李嘉图用生产一件产品有关的时间量来度量劳动量。也就是现在说的工时。但是个人觉得应该指的是有效工时。</li>
<li>劳动的不同技能。李嘉图用工资来度量不同劳动者的技能熟练度。乍一看跟斯密的循环推断一样，但是李嘉图假设不同熟练度的工人之间的工资比例是不变的。那么在这样的情况下，商品的价格变化就可以用工资之外的因素来解释。如果我们接受李嘉图这样的假设，那么循环推论也就不存在了。</li>
<li>资本产品。几乎所有的商品都是由劳动和资本生产的。李嘉图将资本认为是储藏起来的劳动，也就是以前被使用的劳动。劳动与资本共同生产的产品包含的劳动量由立即被使用的劳动量加上储藏在用来生产最终产品的资本产品中的劳动量来度量。也就是说，资本在生产过程中贬值了。不过李嘉图的这一解释并不完美，例如两年前生产一件资本产品的一小时劳动与一年前的一小时劳动相比，它们对今年生产最终产品的价格有不同的影响。如果要正确计算，应该要加总过去所有的劳动和利息成本。但是利息成本不包括在劳动成本内。</li>
<li>地租。李嘉图认为，地租是价格被决定的因素，而不是决定价格的因素。这个在地租理论中有体现。</li>
<li>利润。尽管最终销售价格中的利润由于各种原因可能不同，例如资本密集型产业，利润是构成价格的主要成分，而劳动密集型产业劳动量是主要成分。但是李嘉图认为，利润率的影响在数量上并不重要。</li>
</ol>
<p>李嘉图始终主张劳动量是解释价格变化的最重要成分。总结一下李嘉图的价值理论。</p>
<ol type="1">
<li>与斯密不同，李嘉图主张使用价值对交换价值的存在来说是必要的。</li>
<li>仅仅对完全竞争市场中能自由再生产的产品来说，他的劳动价值理论才成立。</li>
<li>他的主要关注点是解释导致不同时期相对价格变化的经济力量。</li>
<li>尽管市场价格（短期价格）的变化可能由很多需求与供给因素决定，然而自然价格（长期均衡价格）的变化却通过生产产品所要求的劳动量的变化得到解释。</li>
<li>尽管某些因素修改了这些原理，例如利润，但是它们没有扰乱如下本质结论，即价值理论中，相对价格的变化多半由生产所需的劳动量来解释。</li>
</ol>
<h2 id="李嘉图的分配理论">李嘉图的分配理论</h2>
<p>李嘉图的价值理论和地租理论都是为了他的分配理论而准备的。</p>
<p>借助简单的图形，我们可以看到李嘉图的主张。李嘉图的模型中，资本与劳动的组合以固定的比例被添加到经济体可以利用的固定数量的土地上。如图：</p>
<p><img data-src='https://i.loli.net/2019/04/14/5cb35192e1018.png'></p>
<p>李嘉图的问题是确定地租，利润，工资之间的分配。ABHQM表示边际实际产品，OC表示资本与劳动组合以某种数量投入可利用的土地，从这一位置开始，所投入的最后一单位资本与劳动组合的边际产品由BC表示，总的农业产量等于OABC。</p>
<p>在边际量上，地租降为0，所以直线BD上的任何产品将被支付给地主，所以地租等DAB。维持最低生活水平的工资通过马尔萨斯的人口理论得出，假定是EFJQN，那么工资率就是FC，总工资是OEFC。因此利润是BF，总利润是EDBF。可以注意到，利润水平取决于最后一单位资本与劳动组合的编辑产品以及维持最低生活水平的实际工资。</p>
<p>李嘉图巧妙地通过减法来分析，因此收入分配理论又被称为剩余理论（residual theory）。</p>
<p>李嘉图极大的兴趣是资本家、地主以及劳动者所获得的国民收入的相对份额在不同时间的变化。李嘉图认为斯密随时间变化利率下降的观点是正确的，但是，他否定了斯密的所有论据。</p>
<p>对于第一个理由，劳动市场竞争太高工资，利润下降。李嘉图认为，按照马尔萨斯的人口学说，工资上升，人口增加，工资又会被拉低到以前的水平。</p>
<p>通过萨伊定律，李嘉图认为，斯密对利润下降的第二、第三理由意味着存在普通的产出过剩，只有不能按照以前的价格销售由于新的投资而增加的产量才会导致利润下降。也就是现在说的供过于求。但是萨伊定律认为不会存在这种情况。萨伊定律会在后面讨论。</p>
<p>在农业中，早起的经济体利润很高，资本积累率高。这种资本积累率提高了工资率，以马尔萨斯的人口学说，人口增加，农业的粗放边际和集约边际被向下推进。这导致了地租上升，利润下降，资本积累率降低，直到利润为0。这时经济增长停止，人口增长停止，工资处于维持生活水平的最低位置，地租很高。</p>
<p>那制造业，如果在完全竞争市场中，由于长期均衡下，整个经济体的利润都会相等，因此农业利润下降，工业利润也会下降。一旦李嘉图模型中的动力，也就是资本积累减少了，整个体系会受到影响，最终达到古典静止状态。</p>
<p>回到《谷物法》，李嘉图认为，《谷物法》使得英国的谷物产量扩大，集约边际和粗放边际向下推进，利润随地租的上升而下降。《谷物法》加速了经济体达到古典静止状态的过程。</p>
<h2 id="李嘉图的比较优势理论">李嘉图的比较优势理论</h2>
<p>利用比较优势，李嘉图强调了自由贸易主张。在讨论比较优势前，先考察每个国家在其中一种商品上具有绝对优势时候的国际贸易。假设每单位劳动产量如下表：</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>酒</th>
<th>布</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>英国</td>
<td>4</td>
<td>2</td>
</tr>
<tr class="even">
<td>葡萄牙</td>
<td>8</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>假设上表存在，英国完全生产布匹，有4个单位，葡萄牙完全生产酒有16单位。这样全世界就多了1单位布和4单位酒。因此交换价格处于8单位酒换一单位布和2单位酒换1单位布之间时候，交易可以存在。</p>
<p>那么如果一国在各种产品上都更有优势呢？</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>酒</th>
<th>布</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>英国</td>
<td>12</td>
<td>6</td>
</tr>
<tr class="even">
<td>葡萄牙</td>
<td>8</td>
<td>1</td>
</tr>
</tbody>
</table>
<p>比较优势表明，在英国为了多1单位布，需要放弃2单位酒，但是葡萄牙则需要放弃8单位酒，因此葡萄牙生产酒更有优势，英国生产布更有优势。那么英国全部生产布，得到12单位布，葡萄牙转移2单位布的劳动与资本到酒，就得到24单位酒，这样全世界的产量还是增加了。</p>
<p>因此，比较优势实际上是各国机会成本的比较。如果两国的机会成本一样，贸易就无法存在。</p>
<p>李嘉图没有考虑到的是贸易收益如何分割，约翰·斯图亚特·穆勒解决了这个问题。他提出，贸易条件或者说国际价格将取决于参与国家的商品需求的相对力量。</p>
<p>比较优势揭露了一个事实，那就是关税复旦最后还是由本国自己承担。现在的贸易战必然是两败俱伤呀。</p>
<p>与之前的重农主义或者重商主义不同，比较优势学说具有广泛而重要的意义。实际上，重农主义和重商主义都假设了产品总量是固定的，这也是一些理论暗含的假设。也就是零和博弈？</p>
<h1 id="三资本主义经济体的稳定与增长">三、资本主义经济体的稳定与增长</h1>
<p>李嘉图与马尔萨斯之间的对资本主义体质保持资源充分利用能力的争论被认为是对萨伊定律的争论。萨伊定律认为，资本主义体质将自动实现资源的充分利用和较高的经济增长速度。当然，最后这一观点被凯恩斯终结。这一争论的历史进程是从重商主义开始的。</p>
<h2 id="重商主义的总需求观点">重商主义的总需求观点</h2>
<p>大多数重商主义者认为，个人节俭与储蓄有利于国家。当然，曼德维尔抨击了这一观点，他认为增加贸易和希望减少奢侈是一种矛盾。</p>
<h2 id="斯密的总需求观点">斯密的总需求观点</h2>
<p>斯密认为，资本积累才是繁荣与增长的主要决定力量，对斯密而言，储蓄不是减少了总需求，仅仅是使得需求从消费变为投资。</p>
<h2 id="马尔萨斯的消费不足主义">马尔萨斯的消费不足主义</h2>
<p>马尔萨斯在对资本积累过程的讨论中，提出了天真又成熟的想法。他天正的主张是，劳动者没有获得全部产品。因此，单独就劳动者需求而言，不足以按照满意的价格购买全部最终产品。这是正确的，但是资本家如果以生产者产品需求的方式，将他们的储蓄返还市场上，那么就不存在总需求不足。</p>
<p>他成熟的见解在于，他主张“储蓄——投资”过程无法无限进行下去而不导致长期停滞。他主张，存在一个经济体能够吸收的适宜的资本积累率，过多的储蓄与投资将引起难以对付的问题。同事马尔萨斯意识到，要保持资本主义体制中的资源充分利用，必须保证总产量水平与总消费水平的扩张。</p>
<p>马尔萨斯断定，因为劳动者与资本家方面存在不充分的有效需求，所以，一定要通过社会上那些只消费不生产的人来填充缺口。也就是第三产业从业人员与地主。</p>
<h2 id="萨伊定律">萨伊定律</h2>
<p>正统古典经济学家认为，生产产品的过程中产生了充足的购买力，能按满意的价格将这些产品带离市场，也就是整体市场不会发生供大于求。</p>
<p>萨伊定律认为，供给创造出它自身的需求。供给创造潜在需求并没有问题，但是潜在需求是否可以变成市场上的有效需求是一个关键问题。李嘉图，詹姆士·穆勒，萨伊认为潜在购买力会作为消费者产品或生产者产品回到市场上。但是他们都只将货币当做交换媒介，而没有考虑到货币的储藏价值。实际上，货币天生的属性之一就是储藏。不过马尔萨斯没有发现并发展这一点。</p>
<h2 id="货币理论">货币理论</h2>
<p>离阿基图对萨伊定律的看法是在19世纪中期的争论中得到发展的。这些争论被成为金银争论（Bullion Debates）。争论的焦点是拿破仑战争时期通货膨胀的原因是什么。</p>
<p>金银通货主义者认为，通胀是发生在战争期间的货币扩张，也就是说是一种货币现象。</p>
<p>反金银通货主义者认为，通胀的原因很复杂，他们赞同真实票据学说，主张如果货币发行涉及短期金融商业操作，则不会有货币的过度发行。</p>
<p>李嘉图认为经济体的“活动”是在实体部门发生的，货币只是一种反映。不过亨利·桑顿曾意识到货币不仅是一种反映，还对实体经济有影响。但是受制于李嘉图的威望，最后接受了李嘉图的货币数量理论。</p>
<h2 id="技术性失业">技术性失业</h2>
<p>李嘉图在1821年的第三版《原理》中加了一章“论机器”，之前他主张使用新机器不会减少劳动的需求，但是这一版他改变了这一观点。</p>
<p>他认为，如果最新使用的机器是通过将流动资本转换为固定资本来筹集资金的，那么工资基金就会减少，失业将会发生。虽然李嘉图并没有说明失业会发生多久。</p>
<h1 id="总结">总结</h1>
<p>李嘉图代表了从斯密方法——理论与历史描述的松散结合——向高度抽象的经济模型方法的明显突破。李嘉图能够表明劳动成本价值理论的有点和缺点，并说明当时紧迫的政策问题。他利用自己的主张，通过显示来自自由和开放国际贸易中的福利收益，来强调斯密的自由放任情形。他集合了马尔萨斯的人口学说和工资基金理论来表明不可能改进低收入群体的命运。他对萨伊定律的捍卫压制了一些批评家，这些批评家发现了一些资本主义体制的缺陷，其中包括有关储蓄与投资的决策是由私人个别地做出的这种缺陷。他的经济学动摇了地主的地位，在政治力量上地主输给了新兴资本家。他对接近静止状态的分析，为资本主义未来投下了长长的阴影。到了19世纪中期，马克思将李嘉图的工具与其他分析相结合，打造了自己的理论，即资本主义仅仅是历史上的一个阶段，包含了毁灭它自身的种子。</p>
]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
  </entry>
  <entry>
    <title>一个挂逼</title>
    <url>/guabi/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>挂逼是形容深圳三和人才市场里面一些生活极其困难的打工者，有时候也代表这个人死了。</p>
<span id="more"></span>
<h1 id="三和大神">三和大神</h1>
<p>这是我实在无所事事的时候偶然发现的一个神奇存在，在看到三和大神之前，你永远不会相信，有人能够这样活着。当然，在看三和大神之前，我也不会想到，自己也就是个三和大神。</p>
关于三和大神的一段描述：
<p align="center">
<img data-src='https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/blog_0001.png'>
</p>
<p>反观自己的标准，惊讶地发现，似乎也没什么差别。996的不要，大饼画得太大的不要，考勤严格的不要，没有奖金的不要，活干不完的不要……</p>
三和大神的人物画像：
<p align="center">
<img data-src='https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/blog_0002.png'>
</p>
<p>再反观自己，似乎也并没有什么两样。喜欢算的工资是时薪；下班就是看片游戏；吃饭叫个外卖；能找到跟三四个人挤在一起的自如就谢天谢地，还在乎甲醛？……</p>
<p>除了物质层面的差别，在精神上，可能早就是一个三和大神。对，自己就是个每天洗澡的挂逼。</p>
<h1 id="小聚">小聚</h1>
<p>回乡同学婚礼小聚，谁能想到，时间是这个世界上最神奇的东西。丧几乎快成为我的个人代名词，但是我不能懂的是，为什么有人能不丧。在他们身上，我看到一种非常奇怪的感觉。可能是他们身上不同于日常见到的乐观积极，这种只是为了过日子的积极态度，让我充满了怀疑。当然，也可能是挂逼对所有积极生活的人都充满了怀疑。</p>
<p>那些在一线城市的互联网民工们，可能你们在为了改进APP图标的几个像素点能够带来多大的ROI而兴奋不已，但是，这些事情对自己的意义又在哪里。为了升职加薪？还是所谓的为了改变世界？</p>
<p>不知道是在魔都呆久了的原因，还是真的被洗脑了。我曾经觉得我还年轻，但是当一些只比自己大两三岁的同学，以一种在魔都被嘲讽的中年人身姿出现在我面前的时候，突然一阵恍惚。</p>
<p>我很难描述这次小聚的感受。我看着他们在KTV里面嘶吼，我总是会想起来三和大神200舞。如果不论生活质量，活着，到底意味着什么？</p>
<h1 id="想上岸的挂逼">想上岸的挂逼</h1>
<p>现实是，你的热血是可以被利用的，你的未来也可以被拿来成为他们的垫脚石。你以为，他们款款深情让你将所有托付给他们的以后能够让你为荣耀而活。但是你马上发现，自己的体力、脑力、技能、资源、能力都与这一切都毫无关系，你在他们的棋局里面已经被淘汰了。</p>
<p>非常感谢这段经历，我终于也成为众多三和大神中的一员。它教会了我，其实他们所说的未来一直都与你无关，那是他们的未来。当然，正因为他们的未来与自己无关，所以自己也不必有什么负罪感。</p>
<p>在三和，不是每个人都甘心成为挂逼，开始在快手直播的200舞，也许某一天他也就火了，然后也就上岸了。</p>
<p>时间有限，一生可以用来上岸的的时间其实没有多久，所以，虽然现实很丧，但是不要因为一时被人利用就做挂逼。</p>
]]></content>
      <categories>
        <category>碎碎念</category>
      </categories>
  </entry>
  <entry>
    <title>线性代数 11</title>
    <url>/linear_algebra_step11/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>向量正交，如果从几何的角度来看，向量的正交可以看作是两个向量垂直。</p>
<span id="more"></span>
<p>首先，我们下一些定义。我们将向量的长度叫做norm，记做<span class="math inline">\(\| v \| = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2}\)</span>。那么两个向量之间的距离我们用两个向量差的norm表示，记做<span class="math inline">\(\| v - u \|\)</span>。</p>
<p>然后向量有两种乘积，一种是点乘，一种是叉乘。叉乘就看作是只有一列的矩阵，然后用矩阵的叉乘方法就好了。至于点乘，实际上也可以看作是叉乘。定义如下： <span class="math display">\[
v \cdot u = \sum_i^n v_i u_i = v^{\top} u
\]</span> 这里再说明一下，默认向量是列向量。</p>
<p>现在进入正题，向量正交就是两个向量的点内积为0，也就是<span class="math inline">\(u \cdot v = 0\)</span>。那么很自然就会知道，零向量与所有的向量正交。</p>
<p>那么向量的点内积有一些运算性质： 假设有向量<span class="math inline">\(u，v\)</span>，矩阵<span class="math inline">\(A\)</span>， 常数<span class="math inline">\(c\)</span></p>
<blockquote>
<ol type="1">
<li><span class="math inline">\(u \cdot u = \| u \|^2\)</span></li>
<li><span class="math inline">\(u \cdot u = 0\)</span> if and only if <span class="math inline">\(u = 0\)</span></li>
<li><span class="math inline">\(u \cdot v = v \cdot u\)</span></li>
<li><span class="math inline">\(u \cdot (v + w) = u \cdot v + u \cdot w\)</span></li>
<li><span class="math inline">\((v + w) \cdot u = v \cdot u + w \cdot u\)</span></li>
<li><span class="math inline">\(cu \cdot v = u \cdot cv\)</span></li>
<li><span class="math inline">\(\| cu \| = |c| \| u \|\)</span></li>
<li><span class="math inline">\(Au \cdot v = (Au)^{\top} v = u^{\top}A^{\top}v = u \cdot A^{\top}v\)</span></li>
<li><span class="math inline">\(\| u+v \| \le \|u\| + \|v\|\)</span></li>
</ol>
</blockquote>
<p>如果我们现在有个向量集合，集合里所有的向量互相正交，那么我们就叫这个集合是orthogonal set。那么如果刚好这里的向量都是单位向量，这个集合就可以叫做orthonomal basis。</p>
<p>现在回过头来看，这样的一个集合有什么用呢？这个向量集合是不是非常像之前的坐标系。然后进一步来看，假设现在有一个集合<span class="math inline">\(S = \{ v_1 \; v_2 \; \cdots \; v_n \}\)</span>是一个orthogonal basis，有一个向量<span class="math inline">\(u\)</span>是这些向量的线性组合，也就是说<span class="math inline">\(u = c_1 v_1 + c_2 v_2 + \cdots + c_n v_n\)</span>，那么如果我们要求<span class="math inline">\(c_i\)</span>，其实非常简单就是<span class="math inline">\(c_i = \frac{u \cdot v_i}{\| v_i \|^2}\)</span>。如果现在再从几何的角度来看，这个<span class="math inline">\(c_i\)</span>其实就是<span class="math inline">\(u\)</span>在<span class="math inline">\(v_i\)</span>上投影的长度。</p>
<p>那如果现在随便给一个basis，<span class="math inline">\(\{u_1 \; u_2 \; \cdots \; u_n \}\)</span>，现在要将这个basis变成orthogonal basis，要做的是： <span class="math display">\[
\begin{align}
v_1 &amp; = u_1 \\
v_2 &amp; = u_2 - \frac{u_2 \cdot v_1}{\|v_1\|^2}v_1 \\
v_3 &amp; = u_3 - \frac{u_3 \cdot v_2}{\|v_2\|^2}v_2 - \frac{u_3 \cdot v_1}{\|v_1\|^2}v_1 \\
&amp; \vdots \\
v_n &amp; = u_n - \frac{u_n \cdot v_{n-1}}{\|v_{n-1}\|^2}v_{n-1} - \cdots - \frac{u_n \cdot v_1}{\|v_1\|^2}v_1
\end{align}
\]</span></p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>线性代数 10</title>
    <url>/linear_algebra_step10/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>在讲矩阵可对角化前，先引入一个概念，矩阵相似。如果存在方阵<span class="math inline">\(A，B\)</span>，一个可逆矩阵<span class="math inline">\(P\)</span>，使得<span class="math inline">\(P^{-1} A P = B\)</span>，那么我们称<span class="math inline">\(A\)</span>和<span class="math inline">\(B\)</span>是相似的。那么如果现在<span class="math inline">\(B\)</span>是一个对角矩阵的话，那么我们就称<span class="math inline">\(A\)</span>是可对角化的（diagonalizable）。一般而言，这里会用<span class="math inline">\(D\)</span>来表示对角矩阵。</p>
<span id="more"></span>
<p>那么对角化有什么意义呢？我们从公式出发看一下，将<span class="math inline">\(P\)</span>表示为<span class="math inline">\([p_1 \; \cdots \; p_n]\)</span>，将<span class="math inline">\(D\)</span>表示为<span class="math inline">\(\begin{bmatrix} d_1 &amp; \cdots &amp; 0 \\ \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp; d_n \end{bmatrix}\)</span>。我们之前的公式是<span class="math inline">\(P^{-1} A P = D\)</span>，所以<span class="math inline">\(AP = PD\)</span>。</p>
<p>先看左边，<span class="math inline">\(AP = [Ap_1 \; \cdots \; Ap_n]\)</span>，再看右边<span class="math inline">\(PD = P[d_1 e_1 \; \cdots \; d_n e_n] = [P d_1 e_1 \; \cdots \; P d_n e_n] = [d_1 P e_1 \; \cdots \; d_n P e_n] = [d_1 p_1 \; \cdots \; d_n p_n]\)</span>。这不就是特征根么。</p>
<p>所以我们就看到<span class="math inline">\(A\)</span>的特征向量可以组成一个向量空间<span class="math inline">\(\mathbb{R}^n\)</span>。</p>
<p>那么如何对角化呢，只要找到n个线性无关的向量<span class="math inline">\(p_i\)</span>，然后将这些向量组成一个矩阵，就可以得到可逆矩阵<span class="math inline">\(P\)</span>。然后特征根只要按对角线排列就是<span class="math inline">\(D\)</span>。</p>
<p>解法就是计算<span class="math inline">\(\det(A - tI) = (t-\lambda_1)^{m_1} (t-\lambda_2)^{m_2} \cdots\)</span>。那么因为每个<span class="math inline">\(\lambda\)</span>对应能有的eigenvector数量是小于等于指数<span class="math inline">\(m\)</span>的，只要每一个指数<span class="math inline">\(m\)</span>都等于eigenspace，那么我们就说<span class="math inline">\(A\)</span>可以对角化。</p>
<p>比如矩阵<span class="math inline">\(A = \begin{bmatrix} -1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 2 \\ 0 &amp; 2 &amp; 1 \end{bmatrix}\)</span>，那么<span class="math inline">\(A\)</span>的因式分解是<span class="math inline">\(-(t+1)^2 (t-3)\)</span>。所以特征根是3和-1。而对应的特征向量就是<span class="math inline">\(\begin{bmatrix}0 \\ 1 \\ 1 \end{bmatrix} \; \begin{bmatrix}1 \\ 0 \\ 0 \end{bmatrix} \; \begin{bmatrix}0 \\ 1 \\ -1 \end{bmatrix}\)</span>。这样我们就完成了对角化。</p>
<p>矩阵对角化的好处是如果要做连乘的时候，对角矩阵的连乘是非常简单的，这样就可以极大减少计算开销。也就是说<span class="math inline">\(A^m = P^{-1} D^m P\)</span>。</p>
<p>最后其实回想一下之前的坐标系变换，对角化的过程其实就是一次坐标系的变换过程。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>线性代数 12</title>
    <url>/linear_algebra_step12/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>稍微介绍一下两种特殊的矩阵，orthogonal matrix和symmetric matrix。</p>
<span id="more"></span>
<p>orthogonal matrix其实就是矩阵里面每个向量相互独立的矩阵，如果是orthonormal的话，这些矩阵里的向量都是单位向量。比如说<span class="math inline">\(\begin{bmatrix}\frac{1}{\sqrt{2}} &amp; 0 &amp; \frac{1}{\sqrt{2}} \\ \frac{1}{\sqrt{2}} &amp; 0 &amp; \frac{-1}{\sqrt{2}} \\ 0 &amp; 1 &amp; 0 \end{bmatrix}\)</span>。</p>
<p>这样的矩阵有一些特性，首先，orthogonal matrix <span class="math inline">\(Q\)</span>的transpose和inverse相等。也就是<span class="math inline">\(Q^{\top} = Q^{-1}，且这两个矩阵都是orthogonal的\)</span>，另外，<span class="math inline">\(\det(Q) = \pm 1\)</span>。最后，orthogonal matrix和orthogonal matrix叉乘之后还是orthogonal matrix。</p>
<p>orthogonal matrix还有一个很特殊的特性，就是向量和orthogonal matrix相乘以后，向量的norm不变。</p>
<p>另一种特殊矩阵是symmetric matrix，也就是类似<span class="math inline">\(\begin{bmatrix}a &amp; b \\ b &amp; c \end{bmatrix}\)</span>。首先，symmetric matrix一定有实特征根。其次，symmetric matrix一定有orthogonal eigenvectors。最后，symmetric matrix一定是diagonalizable的。这里存在一个等价关系<span class="math inline">\(A \text{ is symmetric等价于} P^{\top}AP = D 或 A = PDP^{\top}\)</span>。而这的<span class="math inline">\(P\)</span>包含<span class="math inline">\(A\)</span>的特征向量，<span class="math inline">\(D\)</span>是<span class="math inline">\(A\)</span>的特征根组成的对角矩阵。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>LeetCode 刷着玩</title>
    <url>/leetcode/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>作为一个技术渣，每天刷2-3道LeetCode自虐。233</p>
<span id="more"></span>
<p>我就是吃饱了撑的找死。 solved list：</p>
<ul>
<li><p>No.1 <a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/sp/leetcode/Two_Sum.py">Two Sum</a></p></li>
<li><p>No.2 <a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/sp/leetcode/Add_Two_Numbers.py">Add Two Numbers</a></p></li>
<li><p>No.3 <a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/sp/leetcode/Longest_Substring_Without_Repeating_Characters.py">Longest Substring Without Repeating Characters</a></p></li>
<li><p>No.7 <a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/sp/leetcode/Reverse_Integer.py">Reverse Integer</a></p></li>
<li><p>No.9 <a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/sp/leetcode/Palindrome_Number.py">Palindrom Number</a></p></li>
<li><p>No.12 <a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/sp/leetcode/Roman_to_Int.py">Roman to Int</a></p></li>
<li><p>No.13 <a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/sp/leetcode/Int_to_Roman.py">Int to Roman</a></p></li>
<li><p>No.14 <a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/sp/leetcode/Longest_Common_Prefix.py">Longest Common Prefix</a></p></li>
<li><p>No.15 <a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/sp/leetcode/3Sum.py">3Sum</a></p></li>
<li><p>No.20 <a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/sp/leetcode/Valid_Parentheses.py">Valid Parentheses</a></p></li>
<li><p>No.26 <a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/sp/leetcode/Remove_Duplicates_from_Sorted_Array.py">Remove Duplicates from Sorted Array</a></p></li>
<li><p>No.27 <a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/sp/leetcode/Remove_Element.py">Remove Element</a></p></li>
<li><p>No.29 <a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/sp/leetcode/Divide_Two_Integers.py">Divide Two Integers</a></p></li>
<li><p>No.35 <a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/sp/leetcode/Search_Insert_Position.py">Search Insert Position</a></p></li>
<li><p>No.38 <a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/sp/leetcode/Count_and_Say.py">Count and Say</a></p></li>
<li><p>No.53 <a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/sp/leetcode/Maximum_Subarray.py">Maximum Subarray</a></p></li>
<li><p>No.58 <a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/sp/leetcode/Length_of_Last_Word.py">Length of Last Word</a></p></li>
<li><p>No.66 <a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/sp/leetcode/Plus_One.py">Plus One</a></p></li>
<li><p>No.67 <a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/sp/leetcode/Add_Binary.py">Add Binary</a></p></li>
<li><p>No.88 <a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/sp/leetcode/Merge_Sorted_Array.py">Merge Sorted Array</a></p></li>
<li><p>No.191 <a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/sp/leetcode/Number_of_1_Bits.py">Number of 1 Bits</a></p></li>
</ul>
]]></content>
      <categories>
        <category>LeetCode</category>
      </categories>
  </entry>
  <entry>
    <title>线性代数 02</title>
    <url>/linear_algebra_step2/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>关于行列式是否有解，以及如何求解</p>
<span id="more"></span>
<p>要判断一个行列式是否有解，我们先从一个简单的行列式看起。假设我们有一个方程式： <span class="math display">\[
\begin{align}
&amp; a_{11} x_1 + a_{12} x_2 = b_1 \\
&amp; a_{21} x_1 + a_{22} x_2 = b_2
\end{align}
\]</span> 我们可以这样认为，一个方程代表的是一条直线，所以这样的方程组代表了两条直线。在一个平面上，两条直线的关系可以是完全重合、平行、以及相交。因此，很直观可以看到，当两条直线平行或者重合的时候，有无穷多的点可以满足方程等式。而当两条直线相交的时候，就很明显有且仅有一个解。</p>
<p>接下来看第二个方程组： <span class="math display">\[
\begin{align}
&amp; a_{11} x_1 + a_{12} x_2 + a_{13} x_3 = b_1 \\
&amp; a_{21} x_1 + a_{22} x_2 + a_{23} x_3 = b_2
\end{align}
\]</span> 因为是三个未知数，这样的方程确定的是一个平面。现在这里提供了两个平面，因此，不论两个平面是平行还是相交，我们都会得到无穷多的解。</p>
<p>如果现在我们的方程组是 <span class="math display">\[
\begin{align}
&amp; a_{11} x_1 + a_{12} x_2 + a_{13} x_3 = b_1 \\
&amp; a_{21} x_1 + a_{22} x_2 + a_{23} x_3 = b_2 \\
&amp; a_{31} x_1 + a_{32} x_2 + a_{33} x_3 = b_3
\end{align}
\]</span> 那么现在我们有了三个平面，我们可以想到，三个平面可以相交于一个点，也可以相交于一条直线，也可以完全重叠，或者完全平行，也可以两个平面平行与另一个平面相交于两条直线等等。</p>
<p>因此我们一样会发现，这样的方程组可能有一个解，或者无穷多解。</p>
<p>如果现在是四维的数据，我们就很难用图形来表示了，那么怎么判断是否有解。我们可以引入线性组合（linear combination）的概念。</p>
<p>我们将线性组合定义为：<span class="math inline">\(v = a_1 u_1 + a_2 u_2 + \cdots a_n u_n\)</span>。这里的<span class="math inline">\(u\)</span>和<span class="math inline">\(v\)</span>都是向量。当我们发现一个向量是另外一个向量的线性组合的时候，例如<span class="math inline">\(v = c u\)</span>，那么其实类比到直线的话，这两个向量就是平行的。</p>
<p>然后我们可以将上面的方程组用矩阵的形式表示为<span class="math inline">\(Ax = b\)</span>，这里<span class="math inline">\(b\)</span>就可以看做是<span class="math inline">\(A\)</span>的线性组合。</p>
<p>这里再引入另一个概念span。span我们可以认为是一个向量的大集合，但是这些向量都是由一些基础向量通过线性组合得到的。一般可以记做是<span class="math inline">\(\{u_1, u_2, \cdots, u_k \}\)</span>或者是<span class="math inline">\(S=\{c_1 u_1 + c_2 u_2 + \cdots\}\)</span>。这里需要注意，对于一个span而言，所谓基础向量，就是不平行的向量。例如<span class="math inline">\(S_2\)</span>就必须有两个不平行的二维向量才能确定。同理，其他维度的可以推理得到。</p>
<p>那么这个span跟上面的线性组合有什么关系呢？其实如果<span class="math inline">\(b\)</span>是<span class="math inline">\(A\)</span>的线性组合，那么<span class="math inline">\(b\)</span>就是<span class="math inline">\(A\)</span>的span。所以当方程式满足这样的条件的时候，就有解。</p>
<p>然后我们回想，刚刚提到了，有时候虽然有解，但是可能有无穷多的解。那么现在的问题就是，如果有解，那么有多少解？</p>
<p>这里先直接抛出结论，当<span class="math inline">\(A\)</span>线性无关（independent）时候有唯一解，线性相关（depend）时有无穷多解。</p>
<p>那么什么是线性相关。线性相关就是给定一组向量<span class="math inline">\(\{a_1, a_2, \cdots \}\)</span>，当存在一组标量<span class="math inline">\(\{x_1, x_2 \cdots \}\)</span>，且<span class="math inline">\(x_i\)</span>不全为0，那么只要能让<span class="math inline">\(x_1 a_1 + x_2 a_2 + \cdots = 0\)</span>就是线性相关。而当且仅当所有的<span class="math inline">\(x\)</span>都是0，则说明这是线性无关的。</p>
<p>最后引入一个概念秩（rank）。秩是表示矩阵内所有线性无关的向量的数量。比如说满秩就说明矩阵的column全都是线性无关的。</p>
<p>判断完是否有解后，一般采用高斯消元法（Gaussian Elimination）。就是下图的过程： <img data-src='https://i.imgur.com/1wVVjhQ.png'> 这里提一下的就是augment matrix就是将<span class="math inline">\(b\)</span>拼到<span class="math inline">\(A\)</span>后面，叫做增广矩阵。这样高斯消元法得到的就是最后的解。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>线性代数 13</title>
    <url>/linear_algebra_step13/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>两个线性代数里面在机器学习领域大放异彩的算法，SVD和PageRank。</p>
<span id="more"></span>
<p>之前对方阵有对角化分解，而且不是所有的方阵都可以对角化，但是SVD是所有矩阵都可以进行分解的。SVD分解的过程如下：</p>
<p><img data-src='https://i.imgur.com/ZMo1wcb.png'></p>
<p>这个公式会有什么特性呢？我们假设<span class="math inline">\(U = \{u_1, u_2, \cdots, u_m\}\)</span>，<span class="math inline">\(V = \{v_1, v_2, \cdots, v_n \}\)</span>，<span class="math inline">\(\Sigma\)</span>是常数<span class="math inline">\(\{\sigma_1, \sigma_2, \cdots, \sigma_k\}\)</span>的对角矩阵。这里有一个点要注意的就是<span class="math inline">\(\Sigma\)</span>的样子大体上会是左上角一个对角矩阵，其余部分都是零的<span class="math inline">\(m \times n\)</span>的矩阵。这些<span class="math inline">\(\sigma\)</span>称为奇异值，这些奇异值会等于<span class="math inline">\(A^{\top}A\)</span>的特征根的平方根。</p>
<p>那么如果矩阵经过SVD分解以后，一定会得到<span class="math inline">\(Av_i = \begin{cases}\sigma_i u_i &amp; \text{if } 1 \le i \le k \\ 0 &amp; \text{if } i &gt; k \end{cases}\)</span>，<span class="math inline">\(A^{\top}u_i = \begin{cases}\sigma_i v_i &amp; \text{if } 1 \le i \le k \\ 0 &amp; \text{if } i &gt; k \end{cases}\)</span>。</p>
<p>现在问题来了，如果给一个矩阵，要怎么计算奇异值？假设有一个矩阵<span class="math inline">\(A = \begin{bmatrix} 0 &amp; 1 &amp; 2 \\ 1 &amp; 0 &amp; 1 \end{bmatrix}\)</span>，可以直观看到，这个矩阵是<span class="math inline">\(3 \times 2\)</span>的矩阵，因此需要在<span class="math inline">\(\mathbb{R}^3\)</span>和<span class="math inline">\(\mathbb{R}^2\)</span>都要有orthogonal matrix。所以先构建矩阵<span class="math inline">\(A^{\top}A = \begin{bmatrix}1 &amp; 0 &amp; 1 \\ 0 &amp; 1 &amp; 2 \\ 1 &amp; 2 &amp; 5 \end{bmatrix}\)</span>。那么做一个<span class="math inline">\(\mathbb{R}^3\)</span>上面的orthogonal matrix，按照这个<a href="https://samaelchen.github.io/linear_algebra_step11/">博客</a>最后的正交化方法，我们可以将矩阵正交化为<span class="math inline">\(v_1 = \frac{1}{\sqrt{30}} \begin{bmatrix} 1 \\ 2 \\ 5 \end{bmatrix}\)</span>，<span class="math inline">\(v_2 = \frac{1}{\sqrt{5}} \begin{bmatrix} 2 \\ -1 \\ 0 \end{bmatrix}\)</span>，<span class="math inline">\(v_3 = \frac{1}{\sqrt{6}} \begin{bmatrix} 1 \\ 2 \\ -1 \end{bmatrix}\)</span>。然后求一下<span class="math inline">\(A^{\top}A\)</span>的特征根分别是6，1和0。所以奇异值就是<span class="math inline">\(\sqrt{6}\)</span>和1。然后就可以按照上面的公式算出<span class="math inline">\(u_1 = \frac{1}{\sqrt{5}} \begin{bmatrix} 2 \\ 1 \end{bmatrix}\)</span>，<span class="math inline">\(u_1 = \frac{1}{\sqrt{5}} \begin{bmatrix} -1 \\ 2 \end{bmatrix}\)</span>。</p>
<p>那么事实上算到这里，只要将上面的向量集合和奇异值排列好，就完成了矩阵<span class="math inline">\(A\)</span>的SVD分解。也就是<span class="math display">\[
A = \begin{bmatrix} \frac{2}{\sqrt{5}} &amp; \frac{-1}{\sqrt{5}} \\ \frac{1}{\sqrt{5}} &amp; \frac{2}{\sqrt{5}} \end{bmatrix} \begin{bmatrix} \sqrt{6} &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \end{bmatrix} \begin{bmatrix} \frac{1}{\sqrt{30}} &amp; \frac{2}{\sqrt{5}} &amp; \frac{1}{\sqrt{6}} \\ \frac{2}{\sqrt{30}} &amp; \frac{-1}{\sqrt{5}} &amp; \frac{2}{\sqrt{6}} \\ \frac{5}{\sqrt{30}} &amp; 0 &amp; \frac{-1}{\sqrt{6}} \end{bmatrix}^{\top}
\]</span></p>
<p>那么SVD跟PCA是有非常多相似的地方的，如果我们使用了全部的奇异值，那么我们就可以还原原来的矩阵，但是如果我们只取了前面的一部分奇异值，我们得到的就是一个损失了一部分信息的矩阵。SVD在机器学习的领域有非常多的应用，最常用的一个地方就是用在推荐算法里面，另外就是降维。此外，还有一个矩阵分解方法是NMF，解释性会更强一些。这个在之前机器学习的博客里面也有提到。</p>
<p>然后是PageRank。这个算法缔造了今天的谷歌，也被称作是最贵的eigen value。PageRank实际上是一个蛮复杂的模型，这里讲一个最简单的情况，后面找机会再认真学习一下。所以这里有一些矩阵分析里面的定理（虽然我也不是很懂）就直接记结论，证明过程以后再学吧。</p>
<p>首先我们假设这个世界上只有四个网页，他们的关系如下：</p>
<p><img data-src='https://i.imgur.com/kpsOeB5.png'></p>
<p>现在假设有一个人随机浏览网页，他到每一个网站的可能性都是一样的，那么根据上图的结果我们可以得到： <span class="math display">\[
\begin{align}
x_1 &amp; = x_3 + \frac{1}{2} x_4 \\
x_2 &amp; = \frac{1}{3} x_1 \\
x_3 &amp; = \frac{1}{3} x_1 + \frac{1}{2} x_2 + \frac{1}{2} x_4 \\
x_4 &amp; = \frac{1}{3} x_1 + \frac{1}{2} x_2
\end{align}
\]</span></p>
<p>所以我们就可以很简单得到这样一个矩阵<span class="math inline">\(A = \begin{bmatrix}0 &amp; 0 &amp; 1 &amp; \frac{1}{2} \\ \frac{1}{3} &amp; 0 &amp; 0 &amp; 0 \\ \frac{1}{3} &amp; \frac{1}{2} &amp; 0 &amp; \frac{1}{2} \\ \frac{1}{3} &amp; \frac{1}{2} &amp; 0 &amp; 0\end{bmatrix}\)</span>。这样的矩阵我们叫做马尔科夫矩阵，或者叫转移矩阵。这种矩阵的特点是每一个行的和为1，或者每一个列的和为1。根据<a href="https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem">Perron-Frobenius theorem</a>这样的矩阵必然有一个特征值为1。</p>
<p>所以PageRank实际上在做的事情就是计算<span class="math inline">\(A\)</span>的特征根为1时候的特征向量。这个特征向量最后就是我们的网页排名。</p>
<p>如果想要对PageRank有多一点的了解可以上Wikipedia看一下<a href="https://en.wikipedia.org/wiki/PageRank">PageRank的页面</a>，也可以直接看原来的<a href="http://ilpubs.stanford.edu:8090/422/1/1999-66.pdf">论文</a>。另外有中文的这篇博客<a href="http://blog.codinglabs.org/articles/intro-to-pagerank.html">http://blog.codinglabs.org/articles/intro-to-pagerank.html</a>，介绍比较全面，不过基本上没有数学证明过程。看看以后有没有空自己推导一遍，顺便Python实现一下。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>线性代数 04</title>
    <url>/linear_algebra_step4/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>矩阵乘法</p>
<span id="more"></span>
<p>矩阵的叉乘运算是高中内容，比较简单： <span class="math display">\[
C = AB \\
c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + \cdots + a_{in}b_{nj}
\]</span> 所以这里我们不去花太多时间讨论计算的事情，这里回顾一下矩阵乘法的意义。另外一般而言，叉乘省略乘号。如果是element-wise multiplication一般会用<span class="math inline">\(\odot\)</span>。</p>
<p>矩阵可以看作是一个线性系统，因此，一种看法，我们可以把矩阵乘法看作是一组向量通过一个线性系统变换，得到另一组向量。</p>
<p>另外一种视角是，我们将一个矩阵看作是一个线性变换的函数，那么两个矩阵相乘就可以看做是一个线性变换的组合，或者说是函数的组合。但是这里要注意一点，矩阵相乘前后顺序不一致，得到的结果不一样。</p>
<p>矩阵乘法有一些性质：</p>
<ol type="1">
<li><span class="math inline">\(s(AC) = (sA)C = A(sC)\)</span></li>
<li><span class="math inline">\((A + B)C = AC + BC\)</span></li>
<li><span class="math inline">\(C(P + Q) = CP + CQ\)</span></li>
<li><span class="math inline">\(IA = A = AI\)</span></li>
<li><span class="math inline">\(A^k = AAA \cdots A(\text{k times})\)</span></li>
<li><span class="math inline">\((AC)^{\top} = C^{\top}A^{\top}\)</span></li>
</ol>
<p>另外矩阵可以做增广，也可以做分块。增广很好理解，跟之前线性方程组做增广矩阵非常像，只要两个矩阵的row相等，就可以拼在一起<span class="math inline">\([ A \ B ]\)</span></p>
<p>矩阵分块也很好理解，就是将一个很大的矩阵分割成好几个小矩阵。实际上，做partition这个事情的好处是，我们可以在一定程度上减少运算量。如下图：</p>
<p><img data-src='https://i.imgur.com/X508XpX.png'></p>
<p>矩阵的乘法其实并不难，而且现在都可以用机器来计算，一般来说GPU比CPU更擅长算这个，这也是为什么深度学习需要用GPU来加速的原因。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>线性代数 03</title>
    <url>/linear_algebra_step3/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Reduced row echelon form</p>
<span id="more"></span>
<p>在求解线性方程组的时候，RREF可以看做是原来矩阵的等价，而且更易于求解。定义RREF前，先定义leading entry。我们称每一行自左往右首个非零元素为leading entry。那么一个row echelon form matrix满足：</p>
<pre><code>1. 每一个非零行都在全零行的上方
2. 下一行的leading entry严格比上一行的leading entry靠右
3. 每一个包含leading entry的列，自leading entry往下都是0</code></pre>
<p>而如果这个矩阵额外满足另外两个条件：</p>
<pre><code>4. 如果包含leading entry的列，除了leading entry之外，其余元素全为0。且只有leading entry一个非零元素
5. leading entry是1</code></pre>
<p>那么，在这样的情况下，我们叫这样的矩阵是RREF。</p>
<p>实际上，当我们完成高斯消元法后，我们得到的最后matrix就是RREF的。</p>
<p>那我们从RREF可以学到什么？</p>
<p>首选，不论我们如何进行行之间的变化，列之间的关系不会发生变化。比如：</p>
<p><span class="math display">\[
A = \begin{bmatrix}1 &amp;2 &amp;-1 &amp;2 &amp;1 &amp;2 \\
                    -1 &amp;-2 &amp;1 &amp;2 &amp;3 &amp;6 \\
                    2 &amp;4 &amp;-3 &amp;2 &amp;0 &amp;3 \\
                    -3 &amp;-6 &amp;2 &amp;0 &amp; 3 &amp;9 \end{bmatrix} \\
                    \ \\
R = \begin{bmatrix}1 &amp;2 &amp;0 &amp;0 &amp;-1 &amp;-5 \\
                    0 &amp;0 &amp;1 &amp;0 &amp;0 &amp;-3 \\
                    0 &amp;0 &amp;0 &amp;1 &amp;1 &amp;2 \\
                    0 &amp;0 &amp;0 &amp;0 &amp;0 &amp;0 \end{bmatrix}
\]</span></p>
<p>如果我们现在定义<span class="math inline">\(R\)</span>是<span class="math inline">\(A\)</span>的RREF，我们可以很直观看到矩阵<span class="math inline">\(A\)</span>的第二列是第一列的两倍，而<span class="math inline">\(R\)</span>也是这样的。其余的列之间的线性关系也一样得到了继承。</p>
<p>我们将每一列仅包含一个非零元素，且这个非零元素是1的列叫做pivot columns。然后我们可以很自然看到，pivot columns一定是independent的。</p>
<p>我们会发现，如果我们现在得到的是一个方阵，那么转成RREF以后，如果每一个column都是independent的，那么最后得到的一定是identity matrix。</p>
<p>如果现在是一个瘦长型的矩阵，也就是行数大于列数的矩阵，那么转成RREF一定是一部分为identity matrix，另一部分是zero matrix。且，identity matrix一定在zero matrix上方。</p>
<p>而现在如果是一个矮胖的矩阵，也就是列比行多的矩阵，那么转成RREF一定不会是linear independent的。</p>
<p>那么我们RREF的non-zero row数目跟上一篇的Rank有什么关系呢。实际上这两个数目是刚好相等的。这事情其实回想一下Rank的定义就可以了。进一步观察就会发现，其实也跟pivot columns一样多。</p>
<p>RREF其实大部分的内容也就这么多。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>线性代数 05</title>
    <url>/linear_algebra_step5/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>逆矩阵</p>
<span id="more"></span>
<p>假设，现在有一个向量<span class="math inline">\(v\)</span>，经过矩阵<span class="math inline">\(A\)</span>的变换，再经过矩阵<span class="math inline">\(B\)</span>的变换，向量不变。此外，先经过<span class="math inline">\(B\)</span>再经过<span class="math inline">\(A\)</span>，仍然得到<span class="math inline">\(v\)</span>。在这种情况下，我们就可以说<span class="math inline">\(A\)</span>和<span class="math inline">\(B\)</span>互逆。</p>
<p>那么严格一点，如果<span class="math inline">\(n \times n\)</span>的矩阵<span class="math inline">\(A\)</span>是可逆的，那么会存在一个<span class="math inline">\(n \times n\)</span>的矩阵<span class="math inline">\(B\)</span>使得<span class="math inline">\(AB = BA = I\)</span>。在这种情况下，我们称<span class="math inline">\(B\)</span>是<span class="math inline">\(A\)</span>的一个逆矩阵，可以记为<span class="math inline">\(A^{-1}\)</span>。</p>
<p>所以有一个很明显的点，当一个矩阵不是方阵的时候，必定是没有逆矩阵的。不过实际上在矩阵分析里面，这样的矩阵也可以求逆，叫做伪逆。</p>
<p>此外，一个矩阵如果有逆矩阵，那么一定只有唯一的一个逆矩阵。这个其实很好证明： <span class="math display">\[
AB = BA = I，AC = CA = I \\
B = BI = B(AC) = (BA)C = IC = C
\]</span></p>
<p>那逆矩阵有什么用呢，用逆矩阵可以解之前的线性方程组，直接得到经过高斯消元法之后的结果。那实际上，逆矩阵求解对机器来说是没效率的，因为机器求逆矩阵就用了RREF。</p>
<p>现在考虑一下，如果我们对矩阵的乘做逆运算会怎么样。也就是<span class="math inline">\((AB)^{-1}\)</span>。那么我们可以得到的是<span class="math inline">\((AB)^{-1} = B^{-1}A^{-1}\)</span>，这里要求两个矩阵都是可逆的。由此我们可以推广，<span class="math inline">\((A_1 A_2 \cdots A_k)^{-1} = A_k^{-1} \cdots A_2^{-1} A_1^{-1}\)</span>。</p>
<p>那如果<span class="math inline">\(A\)</span>是可逆的，<span class="math inline">\(A^{\top}\)</span>是不是也是可逆呢？答案很明显是可逆的，可以求一下，<span class="math inline">\(AA^{-1} = (AA^{-1})^{\top} = (A^{-1})^{\top} A^{\top} = I\)</span>，然后反过来<span class="math inline">\(A^{-1}A = (A^{-1}A)^{\top} = A^{\top} (A^{-1})^{\top} = I\)</span>。所以我们就知道，<span class="math inline">\((A^{\top})^{-1} = (A^{-1})^{\top}\)</span>。</p>
<p>那么如何判断一个矩阵是不是可逆的？在Elementary Linear Algebra里面，提供了十几种判断依据：</p>
<blockquote>
<ol type="a">
<li><span class="math inline">\(A\)</span> is invertible.</li>
<li>The reduced row echelon form of <span class="math inline">\(A\)</span> is <span class="math inline">\(I_n\)</span>.</li>
<li>The rank of <span class="math inline">\(A\)</span> equals <span class="math inline">\(n\)</span>.</li>
<li>The span of the columns of <span class="math inline">\(A\)</span> is <span class="math inline">\(R_n\)</span>.</li>
<li>The equation <span class="math inline">\(Ax = b\)</span> is consistent for every <span class="math inline">\(b\)</span> in <span class="math inline">\(R_n\)</span>.</li>
<li>The nullity of <span class="math inline">\(A\)</span> equals zero.</li>
<li>The columns of <span class="math inline">\(A\)</span> are linearly independent.</li>
<li>The only solution of <span class="math inline">\(Ax = 0\)</span> is <span class="math inline">\(\mathbf{0}\)</span>.</li>
<li>There exists an <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(B\)</span> such that <span class="math inline">\(BA = I_n\)</span>.</li>
<li>There exists an <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(C\)</span> such that <span class="math inline">\(AC = I_n\)</span>.</li>
<li><span class="math inline">\(A\)</span> is a product of elementary matrices.</li>
</ol>
</blockquote>
<p>也就是说，当一个矩阵<span class="math inline">\(A\)</span>是方阵时，如果<span class="math inline">\(A\)</span>可逆，上述的条件都是等价的。</p>
<p>判断完是否可逆之后，如何计算逆矩阵呢？</p>
<p>这里引入一个概念，叫做elementary matrix，比如说<span class="math inline">\(E = \begin{bmatrix} 1 &amp;0 &amp;0 \\ 0 &amp;0 &amp;1 \\ 0 &amp;1 &amp;0 \end{bmatrix}\)</span>。那这个变换矩阵的作用就是交换第二行和第三行。我们回顾一下，之前说的RREF其实做的事情就是多次的elementary matrix乘原来的矩阵。那么如果我们的RREF是单位矩阵，那么其实<span class="math inline">\(A\)</span>的逆矩阵就是这么多elementary matrix的乘积，不过这里要注意的是elementary matrix <span class="math inline">\(E\)</span>的顺序。</p>
<p>矩阵的逆大概就是这么多内容，简单一点判断，如果矩阵是方阵，且满秩，就可逆。计算方法就用高斯消元法的那一套来。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>线性代数 06</title>
    <url>/linear_algebra_step6/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>矩阵的行列式</p>
<span id="more"></span>
<p>矩阵的行列式是非常重要的一个概念，包括到后面的矩阵求导什么的，都有用到。如果上过吴恩达的machine learning课程就会发现这一点。一般来说我们用<span class="math inline">\(\det(A)\)</span>表示行列式。</p>
<p>二阶方阵还有三阶方阵的行列式计算方法都是高中的内容，不多说，回顾一下行列式的一些特性：</p>
<blockquote>
<ol type="a">
<li>单位矩阵的行列式一定为1</li>
<li>交换矩阵两个行的位置一次，行列式乘一次<span class="math inline">\(-1\)</span></li>
<li>如果矩阵线性相关，行列式为0</li>
<li><span class="math inline">\(\det(\begin{bmatrix} ta &amp;tb \\ c &amp;d \end{bmatrix}) = t \det(\begin{bmatrix} a &amp;b \\ c &amp;d \end{bmatrix})\)</span></li>
<li><span class="math inline">\(\det(\begin{bmatrix} a + a&#39; &amp;b + b&#39; \\ c &amp;d \end{bmatrix}) = \det(\begin{bmatrix} a &amp;b \\ c &amp;d \end{bmatrix}) + \det(\begin{bmatrix} a&#39; &amp;b&#39; \\ c &amp;d \end{bmatrix})\)</span></li>
<li><span class="math inline">\(\det(AB) = \det(A) \det(B)\)</span></li>
<li><span class="math inline">\(\det(A^{\top}) = \det(A)\)</span></li>
</ol>
</blockquote>
<p>那行列式和前面的矩阵可逆之间有什么关系呢？很直觉的，如果矩阵的行列式为0，那么这个矩阵就不可逆，如果行列式不为0，那么这个矩阵就可逆。</p>
<p>大部分人应该都忘了大学线代教的高阶矩阵如何计算行列式，所以这里复习一下。我们引入一个定义<span class="math inline">\(A_{ij}\)</span>表示的是原来的矩阵<span class="math inline">\(A\)</span>不包含<span class="math inline">\(i\)</span>行<span class="math inline">\(j\)</span>列的所有元素，也可以记做<span class="math inline">\(c_{ij}\)</span>。这个英文是cofactor，就是代数余子式，所以取了首字母。然后我们可以这样定义行列式： <span class="math display">\[
\det(A) = a_{11} c_{11} + a_{12} c_{12} + \cdots + a_{1n} c_{1n}
\]</span></p>
<p>其中<span class="math inline">\(c_{ij} = (-1)^{i+j} \det A_{ij}\)</span>。</p>
<p>当然，除了用一行的元素，也可以用一列的元素。</p>
<p>那这样一来，不论是多么高阶的矩阵，我们都可以不断拆解，直到一直拆到二阶矩阵求行列式。</p>
<p>行列式的计算大概就这么一回事，计算量比较大，原理不难。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>线性代数 07</title>
    <url>/linear_algebra_step7/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>subspace &amp; basis</p>
<span id="more"></span>
<p>subspace不严格的讲法可以当做是一个向量空间的子集。严格一点，我们的定义是这样的。一个向量子空间是一组向量，满足三个特征：</p>
<ol type="1">
<li>零向量属于这个向量组</li>
<li>该向量组内的两个向量<span class="math inline">\(u\)</span>和<span class="math inline">\(v\)</span>的线性组合也属于改向量组。</li>
<li>该向量组内的向量<span class="math inline">\(u\)</span>乘以一个常数<span class="math inline">\(c\)</span>，<span class="math inline">\(cu\)</span>也属于该向量组。</li>
</ol>
<p>举个例子，比如我们现在有一个向量组<span class="math inline">\(W\)</span>属于<span class="math inline">\(\mathbb{R}^3\)</span>，且满足<span class="math inline">\(6w_1 - 5w_2 + 4w_3 = 0\)</span>。那么<span class="math inline">\(W\)</span>是不是<span class="math inline">\(\mathbb{R}^3\)</span>的一个向量子空间呢？我们按照三个特征一一过一下。</p>
<p>首先，零向量是不是属于这个向量组？很明显是的，让<span class="math inline">\(w_1 = w_2 = w_3 = 0\)</span>等式成立。</p>
<p>然后我们看，如果现在有<span class="math inline">\(u\)</span>，<span class="math inline">\(v\)</span>属于<span class="math inline">\(W\)</span>，那么<span class="math inline">\(u+v = \begin{bmatrix} u_1 + v_1 &amp;u_2 + v_2 &amp;u_3 + v_3 \end{bmatrix} ^\top\)</span>。那么实际上<span class="math inline">\(6(u_1 + v_1) - 5(u_2+v_2) + 4(u_3 + v_3) = 0\)</span>，因此第二条也满足。</p>
<p>最后一条，其实也非常显而易见，把<span class="math inline">\(c\)</span>提出来就可以了。</p>
<p>那subspace里面有一个比较特殊的类型，叫做null space。null space其实就是矩阵<span class="math inline">\(A\)</span>关于<span class="math inline">\(Ax = 0\)</span>的解。对于一个<span class="math inline">\(m \times n\)</span>的矩阵<span class="math inline">\(A\)</span>，一般我们会记做：<span class="math inline">\(\text{Null} \; A = \{v \in \mathbb{R}^n ： Av=0\}\)</span>。那null space一定会是<span class="math inline">\(\mathbb{R}^n\)</span>的向量子空间。</p>
<p>而basis，也就是基也是特殊的一种subspace。基有三个特性：</p>
<ol type="1">
<li>基是最小的生成集</li>
<li>基是最大的线性无关向量集合</li>
<li>向量空间中的向量都按唯一的表达为基的线性组合。也就是说，通过基的线性组合，可以表达向量空间中所有的向量，且都是唯一的表达。</li>
</ol>
<p>第一个特性是很直觉的一件事情，假设我们现在有一个向量子空间<span class="math inline">\(V\)</span>，而<span class="math inline">\(S = \{u_1, u_2, \cdots, u_k \}\)</span>是<span class="math inline">\(V\)</span>的一个generation set。现在假设<span class="math inline">\(A = [u_1, u_2, \cdots, u_k]\)</span>，而<span class="math inline">\(\text{Col }A = \text{Span} \{u_1, u_2, \cdots, u_k \}\)</span>，因为<span class="math inline">\(A\)</span>的pivot column会是<span class="math inline">\(\text{Col }A\)</span>的basis，而且很显然会是<span class="math inline">\(V\)</span>的basis，也会是<span class="math inline">\(S\)</span>的subset。</p>
<p>第二个特性也是一个很直觉的事情，假设有个subspace <span class="math inline">\(V\)</span>，现在给定一个线性无关的集合<span class="math inline">\(S\)</span>，那么就存在两种情况，第一种，<span class="math inline">\(\text{Span }S\)</span>就是<span class="math inline">\(V\)</span>，那么<span class="math inline">\(S\)</span>就是<span class="math inline">\(V\)</span>的basis。第二种，存在一个向量<span class="math inline">\(v_1\)</span>不在<span class="math inline">\(\text{Span }S\)</span>里面，那么<span class="math inline">\(S \cup \{v_1\}\)</span>就是一个新的线性无关集合，然后又回到前面的两种情况。如此不断循环下去，最后就会发现，basis刚好就是最大的线性无关集合。</p>
<p>第三个特性就更加直觉了，basis我们可以认为是dimension，那就非常直觉了，再一个坐标系内，每一个向量都可以用坐标来唯一表示。</p>
<p>回顾一下，向量空间（vector space），列空间（column space），零空间（null space），行空间（row space），子空间（subspace）等等这些概念之间是什么关系？</p>
<p>首先是向量空间，顾名思义，就是向量所在的空间。向量的线性组合，数乘都在这个空间内。</p>
<p>而向量子空间还要包含零向量，其余和向量空间一样。也就是说，向量子空间是属于向量空间的。</p>
<p>然后是列空间，矩阵中所有列组成的向量空间就叫做列空间。相应的，行空间就是所有行组成的空间。</p>
<p>零空间是<span class="math inline">\(Ax=0\)</span>的所有解的集合。也叫作核。</p>
<p>现在我们看这些概念之间的关系。其实都是一些非常直觉的关系。</p>
<ol type="1">
<li>矩阵<span class="math inline">\(A\)</span>的列空间<span class="math inline">\(\text{Col } A\)</span>的维度就是<span class="math inline">\(A\)</span>的pivot column，也就是<span class="math inline">\(\text{rank }A\)</span>。</li>
<li><span class="math inline">\(\text{Dim}(\text{Null } A) = n - \text{rank }A\)</span></li>
<li><span class="math inline">\(\text{Dim}(\text{Row } A) = \text{rank }A\)</span>，也就是<span class="math inline">\(A\)</span>的RREF下，非零的行</li>
</ol>
<p>那上面的这些特性，又会让我们知道<span class="math inline">\(\text{rank }A = \text{rank } A^{\top}\)</span>。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>线性代数 08</title>
    <url>/linear_algebra_step8/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Coordinate system，就是坐标系。其实就是一组vector。</p>
<span id="more"></span>
<p>能够拿来做坐标系的vector set，很显然，按照上一篇博客里的内容，我们可以很自然想到，一个vector set必须符合两个条件：</p>
<ol type="1">
<li>这个vector set是<span class="math inline">\(\mathbb{R}^n\)</span>的span</li>
<li>这个vector set里面的vector是independent的</li>
</ol>
<p>那其实这个vector set就是basis。</p>
<p>如果现在我们的basis刚刚好每个vector都是相互垂直的单位向量，那么我们就会把这个坐标系叫做直角坐标系。</p>
<p>那么从这个角度来看，其实，我们就可以将矩阵乘法看作是坐标系转换。而且坐标系转换，矩阵一定是可逆的。</p>
<p>所以如果我们要做任意的坐标系和直角坐标系之间的转换，我们遵守如下的公式：</p>
<p>从<span class="math inline">\(v_{B}\)</span>到<span class="math inline">\(v\)</span>就是<span class="math inline">\(v = B v_{B}\)</span>，反过来就是<span class="math inline">\(v_{B} = B^{-1} v\)</span>。</p>
<p>事实上，坐标系转换，或者说线性变换是机器学习里面非常常见的一种情况。比如说PCA就是这样的一种变换。PCA有一点像是在找basis。另外如果了解NMF的话，NMF看上去更像是将一组数据的basis找出来。不过要注意的是，仅仅是看上去很像而已。</p>
<p>线性变换具有很显著的意义，将一个在原来坐标系下面很复杂的函数，通过线性变换以后就可能得到一个非常简单的函数。</p>
<p>如下图：</p>
<p><img data-src='https://i.imgur.com/HJTQBeg.png'></p>
<p>我们要做一个关于直线<span class="math inline">\(y = \frac{1}{2} x\)</span>的映射关系。如果这个映射在直角坐标系下面，那么我们的变换矩阵是<span class="math inline">\(\begin{bmatrix} 0.6 &amp;0.8 \\ 0.8 &amp;-0.6 \end{bmatrix}\)</span>。但是如果我们用这条直线作为横轴，垂直于这条直线的向量为纵轴，就会发现，其实在这个坐标系<span class="math inline">\(\begin{bmatrix} 2 &amp;-1 \\ 1 &amp;2 \end{bmatrix}\)</span>内，变换只是<span class="math inline">\(\begin{bmatrix} 1 &amp;0 \\ 0 &amp;-1 \end{bmatrix}\)</span>。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>线性代数 09</title>
    <url>/linear_algebra_step9/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>矩阵的特征根与特征向量。这两个是线性代数里面非常重要的概念。</p>
<span id="more"></span>
<p>我们对这两个概念的定义是这样的，如果存在一个矩阵<span class="math inline">\(A\)</span>可以使得常数<span class="math inline">\(\lambda\)</span>和向量<span class="math inline">\(v\)</span>满足： <span class="math display">\[
Av = \lambda v
\]</span> 那么<span class="math inline">\(\lambda\)</span>就是矩阵的特征根，而<span class="math inline">\(v\)</span>就是特征向量。但是这里需要注意的是，<span class="math inline">\(A\)</span>一定是方阵。举个例子： <span class="math display">\[
\begin{bmatrix}5 &amp; 2 &amp; 1 \\ -2 &amp; 1 &amp; -1 \\ 2 &amp; 2 &amp; 4\end{bmatrix} \begin{bmatrix}1 \\ -1 \\ 1\end{bmatrix} = \begin{bmatrix}4 \\ -4 \\ 4\end{bmatrix} = 4\begin{bmatrix}1 \\ -1 \\ 1\end{bmatrix}
\]</span></p>
<p>不过要注意的是，零向量不能作为特征向量。</p>
<p>所以直观的感受上，特征向量就是经过线性变换以后，只是改变向量长度或者是变成相反的方向。</p>
<p>那么在特征根特征向量在图像中有什么意义呢？下面举四个例子。</p>
<p>首先是扭曲，如下图：</p>
<p><img data-src='https://i.imgur.com/4z9YFVw.png'></p>
<p>那么这样的变换过程中，落在横轴上的向量是没有发生任何变换的，因此图片中蓝色的向量就是特征向量，特征根是1。</p>
<p>第二个变换是映射，如下：</p>
<p><img data-src='https://i.imgur.com/KFeF5NV.png'></p>
<p>在这个变换中，<span class="math inline">\(b_1\)</span>是没有变化的，所以是一个特征根为1的特征向量。而<span class="math inline">\(b_2\)</span>则是刚好反了一个方向，因此是一个特征根为-1的特征向量。</p>
<p>第三种变换是缩放，如下：</p>
<p><img data-src='https://i.imgur.com/hSWgUwf.png'></p>
<p>在这种情况下，图片的所有向量都是特征向量，特征根就是缩放倍数。</p>
<p>第四种是旋转，如下：</p>
<p><img data-src='https://i.imgur.com/03ETOyW.png'></p>
<p>因为上面的旋转过程中，没有一个向量保持了原来的方向，或者转到完全相反的方向，因此这样的变换过程中，没有特征向量。</p>
<p>从上面的四个例子我们还可以发现一个很重要的事情，就是一个特征向量只有唯一对应一个特征根，但是一个特征根可以有多个特征向量。然后，我们就可以顺势定义一个新的概念，eigenspace。也就是<span class="math inline">\(\lambda\)</span>对应的所有特征向量加上零向量构成的subspace。</p>
<p>那么我们要怎么去找到特征向量和特征根呢？</p>
<p>首先我们回顾一下之前的公式： <span class="math display">\[
Av = \lambda v = \lambda I v
\]</span> 所以<span class="math inline">\((A-\lambda I) v = 0\)</span>。这样一来，我们就知道，当我们知道<span class="math inline">\(\lambda\)</span>的时候，只要找到上面那个等式的非零解，就是我们的特征根。</p>
<p>那么如果现在要判断一个常数是不是特征根，我们依照上面那个等式一步步向下推理，因为<span class="math inline">\((A-\lambda I)v = 0\)</span>有多个解，因此我们可以知道<span class="math inline">\(\text{Rank} (A - \lambda I) &lt; n\)</span>，所以<span class="math inline">\(A - \lambda I\)</span>不可逆，也就是说它的行列式为0。</p>
<p>比如说矩阵<span class="math inline">\(A = \begin{bmatrix}-4 &amp; 3 \\ 3 &amp; 6 \end{bmatrix}\)</span>，我们计算行列式<span class="math inline">\(\begin{bmatrix}-4-\lambda &amp; 3 \\ 3 &amp; 6-\lambda \end{bmatrix} = 0\)</span>。也就是说<span class="math inline">\((-4-\lambda)(6-\lambda) - 9 = 0\)</span>。所以我们就可以求出来，<span class="math inline">\(\lambda = -3\)</span>或<span class="math inline">\(\lambda = 5\)</span>。</p>
<p>那么特征根有一些特性。首先，一般来说，一个矩阵跟它的RREF的特征根是不一样的。如果是两个矩阵的因式分解一样，那么就有一样的特征根。</p>
<p>假设现在有个矩阵<span class="math inline">\(A\)</span>有<span class="math inline">\(n\)</span>个特征根（这里只考虑实数根），那么特征根的和刚好就是<span class="math inline">\(\text{trace } A\)</span>，也就是<span class="math inline">\(A\)</span>的对角线元素的和；特征根的乘积刚好就是<span class="math inline">\(\det A\)</span>。</p>
<p>实际上，理解一下特征根和特征向量在图像中的意义，然后知道特征根的解法是<span class="math inline">\(\det(A - \lambda I)\)</span>，特征向量的解法是<span class="math inline">\((A - \lambda I)v = 0\)</span>就好了。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>Ubuntu 装机后必搞的一些软件</title>
    <url>/linux_software/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>记录一下装机后必搞的一些软件，以后装机简单一点。</p>
<span id="more"></span>
<p>每次装机后都要装好多软件，这里记录一下，以后就简单多了。</p>
<p>添加PPA：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:hzwhuang/ss-qt5</span><br><span class="line">sudo add-apt-repository ppa:webupd8team/java</span><br><span class="line">sudo add-apt-repository ppa:jfi/ppa</span><br><span class="line">sudo add-apt-repository ppa:indicator-multiload/stable-daily</span><br><span class="line">sudo add-apt-repository ppa:noobslab/themes</span><br><span class="line">sudo add-apt-repository ppa:noobslab/icons</span><br><span class="line"></span><br><span class="line">wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add</span><br><span class="line">sudo sh -c <span class="string">&#x27;echo &quot;deb http://dl.google.com/linux/chrome/deb/ stable main&quot; &gt;&gt; /etc/apt/sources.list.d/google-chrome.list&#x27;</span></span><br><span class="line"></span><br><span class="line">curl -sL https://packagecloud.io/AtomEditor/atom/gpgkey | sudo apt-key add -</span><br><span class="line">sudo sh -c <span class="string">&#x27;echo &quot;deb [arch=amd64] https://packagecloud.io/AtomEditor/atom/any/ any main&quot; &gt; /etc/apt/sources.list.d/atom.list&#x27;</span></span><br><span class="line"></span><br><span class="line">wget -q -O - http://archive.getdeb.net/getdeb-archive.key | sudo apt-key add -</span><br><span class="line">sudo sh -c <span class="string">&#x27;echo &quot;deb http://archive.getdeb.net/ubuntu xenial-getdeb apps&quot; &gt;&gt; /etc/apt/sources.list.d/getdeb.list&#x27;</span></span><br></pre></td></tr></table></figure>
<p>然后更新安装：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt update</span><br><span class="line"></span><br><span class="line">sudo apt install git terminator guake shadowsocks-qt5 nethogs dpkg oracle-java8-installer atom screenfetch sensord lm-sensors hddtemp psensor indicator-multiload shutter kazam vlc okular ubuntu-tweak flatabulous-theme ultra-flat-icons tsocks vim google-chrome</span><br></pre></td></tr></table></figure>
<p>装完小飞机后修改tsocks的配置文件 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo vim /etc/tsocks.conf</span><br></pre></td></tr></table></figure> 找到 server 改成 server = 127.0.0.1。</p>
<p>然后是百度搜狗输入法，网易云音乐安装包。用dpkg安装就好了。</p>
<p>然后是搜cuda，按照官方的步骤一步步安装。另外就是安装anaconda。这个也不赘述。</p>
<p>接着是一些骚兮兮的美化工作。用Ubuntu-tweak把系统主题改成flatabulous，图标改成ultra-flat-icons。不过可惜啊，flatabulous的作者不再继续支持这个主题了，感觉以后要用macbuntu之类的了（这两天逛gnome-look.org发现另一个挺好看的主题，ant themes，都是黑色的主题，简直本命）。</p>
<p>接着是设置一下psensor，在toolbar显示CPU和GPU的温度。</p>
<p>这些弄完，配置一下atom作为Python的ide： <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">apm install linter hydrogen markdown-preview-enhanced atom-beautify language-markdown language-latex atom-language-r project-manager</span><br><span class="line">pip install flake8 flake8-docstrings</span><br><span class="line">apm install linter-flake8</span><br><span class="line">pip install autopep8</span><br></pre></td></tr></table></figure></p>
<p>然后是在preference里面把tab length改成4个spaces。另外为了避免enter自动补全，而只是换行，修改keymap，添加： <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Disable Enter key for confirming an autocomplete suggestion</span><br><span class="line">&#x27;atom-text-editor:not(mini).autocomplete-active&#x27;:</span><br><span class="line">  &#x27;enter&#x27;: &#x27;editor:newline&#x27;</span><br></pre></td></tr></table></figure></p>
<p>另外atom-beautify的快捷键是ctrl+alt+B，跟fcitx的软键盘快捷键冲突了，吧fcitx开启软键盘的快捷键改掉就好了。</p>
<p>然后是安装zsh，配置一个骚气的终端。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install zsh</span><br><span class="line">chsh -s /bin/zsh</span><br></pre></td></tr></table></figure>
<p>接着安装oh-my-zsh</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sh -c <span class="string">&quot;<span class="subst">$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)</span>&quot;</span></span><br></pre></td></tr></table></figure>
<p>配置自动跳转 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install autojump</span><br><span class="line">vim .zshrc</span><br><span class="line"><span class="comment">#在最后一行加入，注意点后面是一个空格</span></span><br><span class="line">. /usr/share/autojump/autojump.sh</span><br></pre></td></tr></table></figure></p>
<p>配置语法高亮 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/zsh-users/zsh-syntax-highlighting.git</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;source <span class="variable">$&#123;(q-)PWD&#125;</span>/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh&quot;</span> &gt;&gt; <span class="variable">$&#123;ZDOTDIR:-<span class="variable">$HOME</span>&#125;</span>/.zshrc</span><br></pre></td></tr></table></figure></p>
<p>配置语法历史记录 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> git://github.com/zsh-users/zsh-autosuggestions <span class="variable">$ZSH_CUSTOM</span>/plugins/zsh-autosuggestions</span><br></pre></td></tr></table></figure> 然后修改.zshrc，改成 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">plugins=(</span><br><span class="line">  git</span><br><span class="line">  zsh-autosuggestions</span><br><span class="line">)</span><br></pre></td></tr></table></figure> 然后在最后一行加入 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">source $ZSH_CUSTOM/plugins/zsh-autosuggestions/zsh-autosuggestions.zsh</span><br></pre></td></tr></table></figure></p>
<p>最后是配置一个骚气的主题，可以看<a href="https://github.com/robbyrussell/oh-my-zsh/wiki/External-themes">https://github.com/robbyrussell/oh-my-zsh/wiki/External-themes</a>里面的主题，这里我用的是agnosterzak。</p>
<p>先安装powerline： <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install fonts-powerline</span><br></pre></td></tr></table></figure> 然后将主题放到~/.oh-my-zsh/themes下面： <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget http://raw.github.com/zakaziko99/agnosterzak-ohmyzsh-theme/master/agnosterzak.zsh-theme -P ~/.oh-my-zsh/themes</span><br></pre></td></tr></table></figure> 然后修改 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ZSH_THEME=&quot;agnosterzak&quot;</span><br></pre></td></tr></table></figure> 最后将.bashrc底下一些新加入的export啊，alias啊什么的复制到.zshrc底下就好了。</p>
<p>退出终端重新进入，骚气的zsh就配置好了。</p>
<p>然后是电池电量不显示的问题，安装acpi就可以显示。</p>
<p>如果想卸载的话，执行： <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo sh -c <span class="string">&quot;<span class="subst">$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/uninstall.sh)</span>&quot;</span></span><br></pre></td></tr></table></figure></p>
<p>然后将/etc/passwd里面找到自己用户名那一行，把/usr/bin/zsh改成/bin/bash/就可以了。</p>
<p>还有就是配置一个自我发泄的自动对命令行纠错的插件，thefuck，前面配好anaconda以后，只要pip install thefuck就行。然后在.zshrc最后加上eval $(thefuck --alias)。后面如果敲错代码，只要输入fuck，就会自动纠错。</p>
<p>另外，作为Unity的死忠粉，如果装的是Ubuntu 18.04，那么更新完软件以后，要做的事情就是： <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install ubuntu-unity-desktop</span><br></pre></td></tr></table></figure> 记着将display manager改成lightdm就好了。</p>
<p>目前大概就是这样吧，以后想到有什么更新再说。</p>
]]></content>
      <categories>
        <category>搞着玩</category>
      </categories>
  </entry>
  <entry>
    <title>Linux 日常使用软件的日经吐槽贴</title>
    <url>/linux_usage_daily/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>常年把Linux作为desktop的日经吐槽贴，买MacBook的小钱钱攒够就关了。每个日经贴都是买MacBook的一个理由啊。别问为什么不用Windows，用Python写machine learning的应该知道Windows下面有多折腾。</p>
<p>等买MacBook的小钱钱攒够了就关掉此贴（大概此生无望了/(ㄒoㄒ)/~~）。</p>
<span id="more"></span>
<p>说一下用过的发行版，Ubuntu启蒙，目前用的是Ubuntu，另外就是国产的Deepin。不过说起来，Ubuntu放弃了Unity，Deepin创始人离开，感觉Linux没什么可以留恋的了。哪天Unity彻底死掉，就弃坑了。</p>
<p>在国内能够愉快用Linux不得不感谢Deepin的付出。首先是国内绕不开的QQ和微信，Deepin wine都有很好的支持。此外，Deepin共同开发了搜狗输入法（其实是Deepin最早开始的，后来优麒麟不要脸抢走了的样子），网易云音乐和有道词典。</p>
<h1 id="日经">2018.06.22 日经</h1>
<p>这次日经贴就是吐槽词典的，Deepin毕竟人手有限，有道词典有个可怕的bug，就是内存泄露，这个程序已经3年没更新了，所以这个bug就一直在。</p>
<p>那为了愉快使用词典，我寻求了古老但坚挺的GoldenDict。在Ubuntu下面，老将还是非常稳定的。但是在最新的Deepin 15.6下面，GoldenDict会莫名其妙一直占满一个线程（WTF）。</p>
<p>于是只能转寻命令行查词的工具了。一开始用的是dictd，非常好用，但是不支持扩展星际译王的词典，所以放弃了。</p>
<p>然后用的是sdcv，星际译王的终端版本，支持扩展词典，效果棒棒哒。目前在用，效果如图：</p>
<p><img data-src='https://i.imgur.com/EQg93QK.jpg'></p>
<p>安装方法很简单： <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install sdcv</span><br></pre></td></tr></table></figure> 然后将自己下载好的辞书放到～/.stardict/dic路径下就可以了。嗯，不折腾买MacBook理由+1。</p>
<h1 id="日经-1">2018.07.09 日经</h1>
<p>这个感觉也不是Linux的问题，主要是markdown底下碰到了要打中文间隔号的时候，发现诶，好像不是很好用啊。如果跟我一样用的是搜狗输入法，那么可以输入yd，然后就有这个符号的候选了。感谢搜狗。</p>
<h1 id="日经-2">2018.08.15 日经</h1>
<p>写hexo用next主题的时候有个问题，就是有时候网页上的icon会不显示，比如home上面那个小房子。其实就是push以后，next文件夹底下的source/lib里少了font-awesome和ua-parser-js两个文件夹。到github的next源码那里复制一份拷下来就好了。这俩文件夹被github屏蔽了难道？导致每次都push不上去？！</p>
<h1 id="日经-3">2018.08.22 日经</h1>
<p>atom使用的时候避免回车变成候选词，修改一下keymap：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># Disable Enter key for confirming an autocomplete suggestion</span><br><span class="line">&#x27;atom-text-editor:not(mini).autocomplete-active&#x27;:</span><br><span class="line">  &#x27;enter&#x27;: &#x27;editor:newline&#x27;</span><br></pre></td></tr></table></figure>
<p>另外就是github的二次验证问题。</p>
<p>首先让git记住你的秘钥</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git config --global credential.helper store</span><br><span class="line">git config --global user.email <span class="string">&#x27;XXX@xxx.com&#x27;</span></span><br><span class="line">git config --global user.name <span class="string">&#x27;XXX&#x27;</span></span><br></pre></td></tr></table></figure>
<p>接着去github上面拿一个personal token</p>
<p>然后push的时候密码用这个token就可以了。</p>
<h1 id="日经-4">2018.10.08 日经</h1>
<p>突然发现不蒜子不能统计数据了，翻了一下发现是作者的七牛云过期了，只要修改一下next/layout/_third_party_analytics/busuanzi-counter.swig里面的js路径就好了。新路径看作者的网站：<a href="&#39;https://busuanzi.ibruce.info/&#39;">https://busuanzi.ibruce.info/</a></p>
]]></content>
      <categories>
        <category>搞着玩</category>
      </categories>
  </entry>
  <entry>
    <title>台大李宏毅机器学习作业——梯度下降</title>
    <url>/machine_learning_hw01/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>机器学习课程的课程作业。嗯，突然发现一直上理论没有实践，机器学习这样一门实践科学怎么能不实践。</p>
<span id="more"></span>
<p>课程第一次作业在<a href="https://docs.google.com/presentation/d/1L1LwpKm5DxhHndiyyiZ3wJA2mKOJTQ2heKo45Me5yVg/edit#slide=id.g1ebd1c9f8d_0_0">这里</a></p>
<p>课程用的是kaggle玩的一个数据，预测PM2.5，不过因为不是班上的学生，所以我没法提交，就不用这个数据了。可以用kaggle上面的练手数据来搞。这里我就用kaggle上面的Pokemon这个数据来练手。反正就是搞着玩的。</p>
<p>课程的要求是自己用梯度下降实现一个线性回归，不能用现成的框架，比如Python必备的sklearn，当然同理也不能用MXNet或者TF这样的重武器了。</p>
<p>用梯度下降来实现的话，其实有一个很简单的，重点就是先实现损失函数和梯度下降。秉持写代码就是先写糙活，再做优化的原则，先开始写一个最直觉的函数。</p>
<p>首先我们先算一下梯度下降的公式。我们用最简单的MSE作为损失函数。那么公式上就是<span class="math inline">\(MSE = \frac{1}{N} \sum_i(\hat{y}_i - y_i)^2 = \frac{1}{N} \sum_i (\sum_j w_j \cdot x_{i,j} - y_i)^2\)</span>。</p>
<p>那么我们做梯度下降的时候就是求<span class="math inline">\(\frac{\partial L}{\partial w}\)</span>。出于简单理解考虑，假设我们现在只有一个<span class="math inline">\(w\)</span>，因为多个<span class="math inline">\(w\)</span>的话我们假设每个feature是相互独立的，求偏导的时候跟单个求导数没啥差别。那我们现在假设只有一个<span class="math inline">\(w\)</span>，那么我们现在可以发现一个样本进来的时候，误差是<span class="math inline">\((w \cdot x - y)^2\)</span>，那么我们的梯度就是<span class="math inline">\(2(w \cdot x -y) x\)</span>，那我们可以发现，其实<span class="math inline">\(w \cdot x - y\)</span>就是残差，所以这样一来，我们要实现SGD就很简单了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span>(<span class="params">X, y_true, w, eta=<span class="number">0.1</span>, epoch=<span class="number">10</span></span>):</span></span><br><span class="line">    rounds = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> rounds &lt; epoch:</span><br><span class="line">        sum_error = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(X)):</span><br><span class="line">            error = <span class="built_in">sum</span>(X.iloc[i, :] * w) - y_true[i]</span><br><span class="line">            <span class="comment"># 根据梯度更新每个参数</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">1</span>]):</span><br><span class="line">                w[j] -= (<span class="number">1</span> / X.shape[<span class="number">0</span>]) * eta * error * X.iloc[i, j]</span><br><span class="line">            sum_error += error ** <span class="number">2</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch: &#x27;</span> + <span class="built_in">str</span>(rounds) + <span class="string">&#x27;  weight: &#x27;</span> + <span class="built_in">str</span>(w) + <span class="string">&#x27;  total error: &#x27;</span> + <span class="built_in">str</span>(sum_error))</span><br><span class="line">        rounds += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span>(w)</span><br></pre></td></tr></table></figure>
<p>这个地方我在梯度的位置加了一个<span class="math inline">\(\frac{1}{N}\)</span>的系数，就是为了让learning rate设置的时候稍微大一点而已，调整参数的时候稍微简单一点点。不过原始SGD调learning rate就很麻烦。</p>
<p>Pokemon的数据长这样：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml087.png></p>
<p>这个数据里面Total等于后面所有属性的和。所以我们可以做这么一个简单的function来试试看我们的梯度下降能不能找出来。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy <span class="keyword">as</span> sp</span><br><span class="line"></span><br><span class="line">data = pd.read_csv(<span class="string">&#x27;Pokemon.csv&#x27;</span>)</span><br><span class="line"></span><br><span class="line">X = data.loc[:<span class="number">560</span>, [<span class="string">&#x27;Total&#x27;</span>, <span class="string">&#x27;HP&#x27;</span>, <span class="string">&#x27;Defense&#x27;</span>, <span class="string">&#x27;Sp. Atk&#x27;</span>, <span class="string">&#x27;Sp. Def&#x27;</span>, <span class="string">&#x27;Speed&#x27;</span>]]</span><br><span class="line">y = np.array(data.loc[:<span class="number">560</span>, <span class="string">&#x27;Attack&#x27;</span>])</span><br><span class="line"></span><br><span class="line">X[<span class="string">&#x27;constant&#x27;</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span>(<span class="params">X, y_true, w, eta=<span class="number">0.1</span>, epoch=<span class="number">10</span></span>):</span></span><br><span class="line">    rounds = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> rounds &lt; epoch:</span><br><span class="line">        sum_error = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(X)):</span><br><span class="line">            error = <span class="built_in">sum</span>(X.iloc[i, :] * w) - y_true[i]</span><br><span class="line">            sum_error += error ** <span class="number">2</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">1</span>]):</span><br><span class="line">                w[j] -= (<span class="number">1</span> / X.shape[<span class="number">0</span>]) * eta * error * X.iloc[i, j]</span><br><span class="line">        rounds += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch: &#x27;</span> + <span class="built_in">str</span>(rounds) + <span class="string">&#x27;  weight: &#x27;</span> + <span class="built_in">str</span>(w) + <span class="string">&#x27;   error: &#x27;</span> + <span class="built_in">str</span>(sum_error))</span><br><span class="line">    <span class="keyword">return</span>(w)</span><br><span class="line"></span><br><span class="line">w = np.random.rand(X.shape[<span class="number">1</span>])</span><br><span class="line"><span class="built_in">print</span>(w)</span><br><span class="line">sgd(X, y, w, <span class="number">0.0001</span>, <span class="number">1000</span>)</span><br></pre></td></tr></table></figure>
<p>这里我在最后加了一个常数进来，就是一般书上的bias，参数是<span class="math inline">\(w_0\)</span>。如果顺利的话，我们应该看到的是<span class="math inline">\(w = [1, -1, -1, -1, -1, -1, 0]\)</span>或者是这附近的权重向量。</p>
<p>训练1000轮之后的效果是：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml088.png></p>
<p>可以看到最后的weight其实还是挺接近正确答案的，只是常数项没有被消掉。那我们试试看训练5000轮的效果。</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml089.png></p>
<p>这一次看上去比上一次的好很多。那么是不是真的越多就一定越好呢？回顾了一下自己的代码，发现这里有一个问题，我的<span class="math inline">\(w\)</span>是随机initialize的，那很可能这个也有影响。所以重新做个实验。我们把随机初始化改成初始全部为0，分别跑1000次和5000次，看看是什么效果。（PS：这里提醒一下，因为numpy恶心的一点，所以我们要用float的类型而不能用int，也就是0要表示为0.。否则的话weight会一直保持在0。）</p>
<p>1000个epoch的效果：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml090.png></p>
<p>5000个epoch的效果：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml091.png></p>
<p>我们可以看到的是，其实5000轮没比1000轮好出多少。但是相比之前在0-1之间随机初始化的要好出不少，1000轮的结果就比上一次5000轮的好。这也是为什么很多时候机器学习的权重初始化会设计在0附近，或者干脆全部设计为0。</p>
<p>但是这里有个问题，明明简单线性回归的损失函数是有一个最优解的，而且只有一个最优解，那为什么我们就到不了呢？其实也很好理解，因为快到碗底的时候速度会非常非常的慢，这里的梯度我们类比为速度，那么分解到水平方向的速度就很小。所以这就会有一个非常尴尬的事情，就是说实践上，别说是全局最优了，我们连局部最优都到不了。如果损失函数再复杂一点，我们连saddle point都到不了。那如果我们把步长设得很大呢？如果这样，我们很可能一步跨到对面山上，然后就收敛不了了。</p>
<p>那现在如果我们做个regularization会怎么样呢？我们这里实现一个<span class="math inline">\(L_2\)</span>，那么其实我们这里的梯度下降就变成了<span class="math inline">\(\frac{2}{N}(w \cdot x -y) x + 2 \lambda w\)</span>，那么我们可以把梯度下降改一下。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd</span>(<span class="params">X, y_true, w, eta=<span class="number">0.1</span>, epoch=<span class="number">10</span>, penalty=<span class="number">0.1</span></span>):</span></span><br><span class="line">    rounds = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> rounds &lt; epoch:</span><br><span class="line">        sum_error = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(X)):</span><br><span class="line">            error = <span class="built_in">sum</span>(X.iloc[i, :] * w) - y_true[i]</span><br><span class="line">            sum_error += error ** <span class="number">2</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">1</span>]):</span><br><span class="line">                w[j] -= (<span class="number">1</span> / X.shape[<span class="number">0</span>]) * eta * error * X.iloc[i, j] + penalty * w[j]</span><br><span class="line">        rounds += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch: &#x27;</span> + <span class="built_in">str</span>(rounds) + <span class="string">&#x27;  weight: &#x27;</span> + <span class="built_in">str</span>(w) + <span class="string">&#x27;   error: &#x27;</span> + <span class="built_in">str</span>(sum_error))</span><br><span class="line">    <span class="keyword">return</span>(w)</span><br></pre></td></tr></table></figure>
<p>那这里要注意一点，就是说如果我们的penalty设的太大，模型会趋向于保守，换句话说就是权重的更新会比较小，收敛起来会非常非常非常慢。上面的梯度下降里面我们把常数项也做了regularization，那weight初始化全是0，迭代1000轮的效果如下：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml092.png></p>
<p>可以看到的是，这边的weight收敛非常慢。现在我们再试一下不对常数项做regularization会怎么样：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml093.png></p>
<p>可以看到的是，其实常数项这边加不加regularization对其他的参数影响是不太大的。所以本质上我们没有必要去对bias做regularization。</p>
<p>既然SGD都实现了，我们干脆把adagrad也实现一下。adagrad其实很容易做，就是在learning rate那里做动作，加上一个系数。所以我们的梯度下降就可以写作：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adagrad</span>(<span class="params">X, y_true, w, eta=<span class="number">0.1</span>, epoch=<span class="number">10</span></span>):</span></span><br><span class="line">    rounds = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> rounds &lt; epoch:</span><br><span class="line">        sum_error = <span class="number">0</span></span><br><span class="line">        grad = np.array([<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(X)):</span><br><span class="line">            error = <span class="built_in">sum</span>(X.iloc[i, :] * w) - y_true[i]</span><br><span class="line">            sum_error += error ** <span class="number">2</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(X.shape[<span class="number">1</span>]):</span><br><span class="line">                grad[j] += (error * X.iloc[i, j]) ** <span class="number">2</span></span><br><span class="line">                w[j] -= (<span class="number">1</span> / X.shape[<span class="number">0</span>]) * (eta / np.sqrt(grad[j])) * error * X.iloc[i, j]</span><br><span class="line">        rounds += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch: &#x27;</span> + <span class="built_in">str</span>(rounds) + <span class="string">&#x27;  weight: &#x27;</span> + <span class="built_in">str</span>(w) + <span class="string">&#x27;   error: &#x27;</span> + <span class="built_in">str</span>(sum_error))</span><br><span class="line">    <span class="keyword">return</span>(w)</span><br></pre></td></tr></table></figure>
<p>利用adagrad，我们可以一开始就把eta设大一点，我这里设到10，然后迭代100轮就得到了：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml094.png></p>
<p>使用adagrad这样的算法好处就是learning rate比较好调，一开始给一个大一点的，然后迭代次数多一点就好了。原始SGD其实learning rate不是那么好调的。</p>
<p>框架有了，其实后面要试增加样本量，去掉常数项啥的就很方便了。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>台大李宏毅机器学习作业——分类算法</title>
    <url>/machine_learning_hw02/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>好久没更，李宏毅第二次课程实操</p>
<span id="more"></span>
<p>第二次作业主要做的是分类算法。分类算法分为Generative和Discriminate两种。Generative常见的比如朴素贝叶斯，Discriminator就多了，逻辑回归就是。这里实现两个不同的分类算法。</p>
<p>首先是probabilistic generative model。概率生成模型其实就是根据数据分布的情况，拟合一种分布，计算条件概率，判断样本的类别。基本公式就是<span class="math inline">\(P(C_i | x) = \frac{P(x | C_i) P(C_i)}{\sum(P(x | C_i) P(C_i))}\)</span>。那需要用到概率分布的地方就是<span class="math inline">\(P(x | C_i)\)</span>这里，我们需要假设一个分布来拟合这个概率。一般而言，我们用正态分布比较多。之所以用正态分布多，可以回顾一下大数定律和中心极限定理。当数据量到了一定程度，就会呈现出正态分布。</p>
<p>大概的原理可以看这篇<a href="https://samaelchen.github.io/machine_learning_step4/">博客</a>，不是一个很复杂的模型。这里用到的数据就是<a href="https://ntumlta.github.io/2017fall-ml-hw2/">作业2</a>的示例数据，是一个二分类的数据。因为模型的公式已经很明显了，这里就直接将公式转化为代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_model</span>(<span class="params">X_test, X_train, threshold=<span class="number">0.5</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Generative model&quot;&quot;&quot;</span></span><br><span class="line">    D = X_train.shape[<span class="number">1</span>] / <span class="number">2</span></span><br><span class="line">    a = X_train.loc[X_train[<span class="string">&#x27;y&#x27;</span>] == <span class="number">0</span>, :].drop(<span class="string">&#x27;y&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">    mu1 = a.mean()</span><br><span class="line">    tmp1 = pd.DataFrame(X_test - mu1)</span><br><span class="line">    b = X_train.loc[X_train[<span class="string">&#x27;y&#x27;</span>] != <span class="number">0</span>, :].drop(<span class="string">&#x27;y&#x27;</span>, axis=<span class="number">1</span>)</span><br><span class="line">    mu2 = b.mean()</span><br><span class="line">    tmp2 = pd.DataFrame(X_test - mu2)</span><br><span class="line">    <span class="comment"># 这里的协方差矩阵是两个类别的协方差矩阵的加权平均。通常来说用加权平均的效果比较好。</span></span><br><span class="line">    sigma = a.shape[<span class="number">0</span>] / <span class="built_in">len</span>(X) * np.cov(a.T, bias=<span class="literal">True</span>) + b.shape[<span class="number">0</span>] / <span class="built_in">len</span>(X_train) * np.cov(b.T, bias=<span class="literal">True</span>)</span><br><span class="line">    sigma_det = <span class="number">1</span> / np.sqrt(np.linalg.det(sigma))</span><br><span class="line">    <span class="comment"># 这里用伪逆，实际上不用伪逆问题也不大</span></span><br><span class="line">    sigma_pinv = np.linalg.pinv(sigma)</span><br><span class="line">    <span class="comment"># 这里要注意，我直接算的是矩阵，而不是一行一行的算。那么直接计算矩阵的时候其实就只要拿对角线元素就可以了。为什么直接取对角线元素，可以回顾一下MIT的线性代数课程。</span></span><br><span class="line">    array1 = np.diag(np.dot(tmp1, sigma_pinv).dot(tmp1.T))</span><br><span class="line">    array2 = np.diag(np.dot(tmp2, sigma_pinv).dot(tmp2.T))</span><br><span class="line">    gauss1 = np.power(<span class="number">2</span> * np.pi, -D) * sigma_det * np.exp(-<span class="number">0.5</span> * array1)</span><br><span class="line">    gauss2 = np.power(<span class="number">2</span> * np.pi, -D) * sigma_det * np.exp(-<span class="number">0.5</span> * array2)</span><br><span class="line">    prob1 = gauss1 * a.shape[<span class="number">0</span>] / <span class="built_in">len</span>(X)</span><br><span class="line">    prob2 = gauss2 * b.shape[<span class="number">0</span>] / <span class="built_in">len</span>(X)</span><br><span class="line">    prob = prob1 / (prob1 + prob2)</span><br><span class="line">    prob[np.isnan(prob)] = <span class="number">0</span></span><br><span class="line">    <span class="comment"># 这里用了一个阈值，其实也可以按照每个类别的先验概率来分类。个人以为比较合理，但是实际上未必效果最好。五五开也是一种常见的方法。</span></span><br><span class="line">    prob[prob &gt;= threshold] = <span class="number">0</span></span><br><span class="line">    prob[prob != <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> prob</span><br></pre></td></tr></table></figure>
<p>那实际上这样概率生成模型就结束了。从某种程度而言，概率生成模型是不存在训练测试的。</p>
<p>这里顺带提一下用的几个包：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"><span class="keyword">import</span> time</span><br></pre></td></tr></table></figure>
<p>其实sklearn的几个metric可以自己写，但是这里偷懒了。原则上还是自己实现核心算法。</p>
<p>相对而言，逻辑回归就比较简单了，大概过程跟线性回归是一样的，用梯度下降去找<span class="math inline">\(w\)</span>。原理看这篇<a href="https://samaelchen.github.io/machine_learning_step5/">博客</a>。所以实现也很简单：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigma</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">minibatch_sgd</span>(<span class="params">X, y_true, w, eta=<span class="number">0.1</span>, epoch=<span class="number">10</span>, batch_size=<span class="number">100</span></span>):</span></span><br><span class="line">    rounds = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> rounds &lt; epoch:</span><br><span class="line">        start = time.time()</span><br><span class="line">        start_index = <span class="number">0</span></span><br><span class="line">        sum_error = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> start_index &lt; (X.shape[<span class="number">0</span>] - <span class="number">1</span> - batch_size):</span><br><span class="line">            end_index = start_index + batch_size</span><br><span class="line">            <span class="keyword">if</span> end_index &gt; X.shape[<span class="number">0</span>]:</span><br><span class="line">                end_index = X.shape[<span class="number">0</span>] - <span class="number">1</span></span><br><span class="line">            X_batch = X.iloc[start_index:end_index, :]</span><br><span class="line">            error = sigma(np.dot(X_batch, w)) - y_true[start_index:end_index].values</span><br><span class="line">            sum_error += <span class="built_in">sum</span>(error)</span><br><span class="line">            w -= eta * np.dot(X_batch.T, error)</span><br><span class="line">            start_index += batch_size</span><br><span class="line">        rounds += <span class="number">1</span></span><br><span class="line">        end = time.time()</span><br><span class="line">        accuracy = accuracy_score(sigma(np.dot(X, w)), y_true)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch: &#x27;</span> + <span class="built_in">str</span>(rounds) + <span class="string">&#x27;  time: &#x27;</span>+ <span class="built_in">str</span>(<span class="built_in">round</span>(end - start, <span class="number">2</span>)) + <span class="string">&#x27;s&#x27;</span> + <span class="string">&#x27;   error: &#x27;</span> + <span class="built_in">str</span>(sum_error) + <span class="string">&#x27;  accuracy: &#x27;</span> + <span class="built_in">str</span>(accuracy))</span><br><span class="line">    <span class="keyword">return</span>(w)</span><br></pre></td></tr></table></figure>
<p>这里用的是minibatch_sgd，速度会快一点。但是不知道为什么我这边跑的每一个epoch的loss都会乱荡，有点尴尬，后面再仔细debug。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>台大李宏毅机器学习作业——CNN visualization</title>
    <url>/machine_learning_hw03/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>基于MXNet的入门级CNN visualization。嗯TF无脑黑，MXNet &amp; PyTorch一生推。</p>
<span id="more"></span>
<p>具体的可以去看这里的<a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/mxnet/mxnet-101-4.ipynb">ipython notebook</a>，可以直接跑的。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> gluon</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> image</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> init</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon.data <span class="keyword">import</span> vision</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon.model_zoo <span class="keyword">import</span> vision <span class="keyword">as</span> models</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> jupyterthemes <span class="keyword">import</span> jtplot</span><br><span class="line">jtplot.style(theme=<span class="string">&#x27;onedork&#x27;</span>, grid=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<p>修改模型下载源，对国内下载速度友好一点</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">os.environ[<span class="string">&#x27;MXNET_GLUON_REPO&#x27;</span>]=<span class="string">&#x27;https://apache-mxnet.s3.cn-north-1.amazonaws.com.cn/&#x27;</span></span><br></pre></td></tr></table></figure>
<p>加载三个预训练好的模型</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vgg19 = models.vgg19(pretrained=<span class="literal">True</span>)</span><br><span class="line">vgg16 = models.vgg16(pretrained=<span class="literal">True</span>)</span><br><span class="line">resnet152 = models.resnet152_v1(pretrained=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<p>这里读取一张妹子图片</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = nd.array(np.asarray(Image.<span class="built_in">open</span>(<span class="string">&#x27;000000.jpg&#x27;</span>)))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.imshow(np.asarray(Image.<span class="built_in">open</span>(<span class="string">&#x27;000000.jpg&#x27;</span>)))</span><br></pre></td></tr></table></figure>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_8_1.png></p>
<p>这里要注意，预训练好的模型输入的图片大小是<span class="math inline">\(224 \times 224\)</span>的，因此这里对图片重新进行缩放，另外因为MXNet的输入格式有需求，所以我们这里也做了reshape的动作。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data = mx.image.imresize(data, <span class="number">224</span>, <span class="number">224</span>)</span><br><span class="line">data = nd.transpose(data, (<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line"><span class="comment"># data = data.astype(np.float32)/127.5-1</span></span><br><span class="line">data = data.astype(np.float32)/<span class="number">255</span></span><br><span class="line">data = data.reshape((<span class="number">1</span>,)+data.shape)</span><br><span class="line"><span class="built_in">print</span>(data.shape)</span><br></pre></td></tr></table></figure>
<pre><code>(1, 3, 224, 224)</code></pre>
<p>接下来要画的是Saliency Map。可以参考这篇<a href="https://arxiv.org/pdf/1312.6034.pdf">论文</a>。实际上就是看哪个位置的梯度最大。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data.attach_grad()</span><br><span class="line"><span class="keyword">with</span> autograd.record():</span><br><span class="line">    out = vgg19(data)</span><br><span class="line">out.backward()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">plt.imshow((data[<span class="number">0</span>].asnumpy().transpose(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)*<span class="number">255</span>).astype(np.uint8))</span><br><span class="line"><span class="comment"># plt.imshow(((data[0].asnumpy().transpose(1, 2, 0)+1)*127.5).astype(np.uint8))</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">plt.imshow(np.<span class="built_in">abs</span>(data.grad.asnumpy()[<span class="number">0</span>]).<span class="built_in">max</span>(axis=<span class="number">0</span>), cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">plt.imshow(np.<span class="built_in">abs</span>(data.grad.asnumpy()[<span class="number">0</span>]).<span class="built_in">max</span>(axis=<span class="number">0</span>), cmap=plt.cm.jet)</span><br></pre></td></tr></table></figure>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_13_1.png></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data.attach_grad()</span><br><span class="line"><span class="keyword">with</span> autograd.record():</span><br><span class="line">    out = resnet152(data)</span><br><span class="line">out.backward()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">plt.imshow((data[<span class="number">0</span>].asnumpy().transpose(<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>)*<span class="number">255</span>).astype(np.uint8))</span><br><span class="line"><span class="comment"># plt.imshow(((data[0].asnumpy().transpose(1, 2, 0)+1)*127.5).astype(np.uint8))</span></span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">plt.imshow(np.<span class="built_in">abs</span>(data.grad.asnumpy()[<span class="number">0</span>]).<span class="built_in">max</span>(axis=<span class="number">0</span>), cmap=<span class="string">&#x27;gray&#x27;</span>)</span><br><span class="line">plt.subplot(<span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">plt.imshow(np.<span class="built_in">abs</span>(data.grad.asnumpy()[<span class="number">0</span>]).<span class="built_in">max</span>(axis=<span class="number">0</span>), cmap=plt.cm.jet)</span><br></pre></td></tr></table></figure>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_15_1.png></p>
<p>这里有个很有意思的现象，VGG-19偏向找人的头部区域，而ResNet则是找到了腿。另外可以多试验几张图，看看效果。一般试下来VGG偏向把轮廓弄出来，ResNet就会找到各种奇奇怪怪的地方去。但是ResNet效果很好，暂时不能理解为什么。</p>
<p>接下来我们把filter画出来，先看一下VGG的结构。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">print</span>(vgg19)</span><br></pre></td></tr></table></figure>
<pre><code>VGG(
  (features): HybridSequential(
    (0): Conv2D(3 -&gt; 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): Activation(relu)
    (2): Conv2D(64 -&gt; 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): Activation(relu)
    (4): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)
    (5): Conv2D(64 -&gt; 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): Activation(relu)
    (7): Conv2D(128 -&gt; 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): Activation(relu)
    (9): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)
    (10): Conv2D(128 -&gt; 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): Activation(relu)
    (12): Conv2D(256 -&gt; 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): Activation(relu)
    (14): Conv2D(256 -&gt; 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): Activation(relu)
    (16): Conv2D(256 -&gt; 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (17): Activation(relu)
    (18): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)
    (19): Conv2D(256 -&gt; 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): Activation(relu)
    (21): Conv2D(512 -&gt; 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): Activation(relu)
    (23): Conv2D(512 -&gt; 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (24): Activation(relu)
    (25): Conv2D(512 -&gt; 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (26): Activation(relu)
    (27): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)
    (28): Conv2D(512 -&gt; 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): Activation(relu)
    (30): Conv2D(512 -&gt; 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (31): Activation(relu)
    (32): Conv2D(512 -&gt; 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (33): Activation(relu)
    (34): Conv2D(512 -&gt; 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (35): Activation(relu)
    (36): MaxPool2D(size=(2, 2), stride=(2, 2), padding=(0, 0), ceil_mode=False)
    (37): Dense(25088 -&gt; 4096, Activation(relu))
    (38): Dropout(p = 0.5)
    (39): Dense(4096 -&gt; 4096, Activation(relu))
    (40): Dropout(p = 0.5)
  )
  (https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output): Dense(4096 -&gt; 1000, linear)
)</code></pre>
<p>把每个卷积层的权重打出来。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> vgg19.features:</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(i, nn.Conv2D):</span><br><span class="line">        j = i.weight.data()</span><br><span class="line">        <span class="built_in">print</span>(i.weight.data()[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p>将最后一层卷积层的第一个filter画出来，然而，完全看不出到底这个filter能起到什么效果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.imshow(np.<span class="built_in">abs</span>(j[<span class="number">0</span>][<span class="number">0</span>].asnumpy()))</span><br></pre></td></tr></table></figure>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_23_1.png></p>
<p>取第一层卷积层出来</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> vgg19.features:</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">isinstance</span>(i, nn.Conv2D):</span><br><span class="line">        j = i</span><br><span class="line">        <span class="built_in">print</span>(i.weight.data()[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<pre><code>[[[-0.05347426 -0.04925704 -0.06794177]
  [ 0.01531445  0.04506842  0.0021444 ]
  [ 0.03622622  0.01999945  0.01986402]]

 [[ 0.01701478  0.05540261 -0.0062293 ]
  [ 0.14164735  0.22705214  0.13758276]
  [ 0.12000094  0.2002953   0.09211431]]

 [[-0.04488515  0.01267995 -0.01449722]
  [ 0.05974238  0.13954678  0.05410246]
  [-0.00096141  0.058304   -0.02966315]]]
&lt;NDArray 3x3x3 @cpu(0)&gt;</code></pre>
<p>看一张图片进入第一个卷积层后会得到什么样的结果</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> num <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">64</span>):</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    ax.imshow(np.<span class="built_in">abs</span>(i(data)[<span class="number">0</span>].asnumpy())[num])</span><br></pre></td></tr></table></figure>
<pre><code>/home/samael/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py:528: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  max_open_warning, RuntimeWarning)</code></pre>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_1.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_2.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_3.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_4.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_5.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_6.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_7.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_8.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_9.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_10.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_11.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_12.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_13.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_14.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_15.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_16.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_17.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_18.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_19.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_20.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_21.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_22.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_23.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_24.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_25.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_26.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_27.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_28.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_29.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_30.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_31.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_32.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_33.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_34.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_35.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_36.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_37.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_38.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_39.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_40.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_41.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_42.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_43.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_44.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_45.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_46.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_47.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_48.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_49.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_50.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_51.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_52.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_53.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_54.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_55.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_56.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_57.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_58.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_59.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_60.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_61.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_62.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_63.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_28_64.png></p>
<p>然后，嗯，很神奇，第一个卷积的64个通道的效果都在上面。中间有一些看上去还有点像浮雕的效果。某一张嘴唇位置及其显眼。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sample = np.random.uniform(<span class="number">150</span>, <span class="number">180</span>, (<span class="number">224</span>, <span class="number">224</span>, <span class="number">3</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.imshow(sample)</span><br></pre></td></tr></table></figure>
<pre><code>&lt;matplotlib.image.AxesImage at 0x7f4ec0496e10&gt;</code></pre>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_31_1.png></p>
<p>这里生成一张充满噪声的点，再来看看每个filter在做什么。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">mean = [<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>]</span><br><span class="line">std = [<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>]</span><br><span class="line"><span class="keyword">for</span> channel <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    sample[:, :, channel] /= <span class="number">255</span></span><br><span class="line">    sample[:, :, channel] -= mean[channel]</span><br><span class="line">    sample[:, :, channel] /= std[channel]</span><br><span class="line"></span><br><span class="line">sample = sample.reshape((<span class="number">1</span>,)+sample.shape)</span><br><span class="line">sample = sample.transpose(<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">sample = nd.array(sample)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> num <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">64</span>):</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    ax.imshow(np.<span class="built_in">abs</span>(i(sample)[<span class="number">0</span>].asnumpy())[num])</span><br></pre></td></tr></table></figure>
<pre><code>/home/samael/anaconda3/lib/python3.6/site-packages/matplotlib/pyplot.py:528: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  max_open_warning, RuntimeWarning)</code></pre>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_1.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_2.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_3.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_4.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_5.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_6.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_7.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_8.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_9.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_10.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_11.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_12.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_13.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_14.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_15.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_16.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_17.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_18.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_19.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_20.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_21.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_22.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_23.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_24.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_25.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_26.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_27.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_28.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_29.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_30.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_31.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_32.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_33.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_34.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_35.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_36.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_37.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_38.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_39.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_40.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_41.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_42.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_43.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_44.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_45.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_46.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_47.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_48.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_49.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_50.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_51.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_52.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_53.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_54.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_55.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_56.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_57.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_58.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_59.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_60.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_61.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_62.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_63.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/output_34_64.png></p>
<p>入门级别的CNN visualization基本上就这些了。网上没找到MXNet做这个的教程，只能自己摸索了。还好gluon跟pytorch接口很像，可以照着MXNet的源码，再借鉴pytorch的教程慢慢摸索。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>台大李宏毅机器学习作业——反向传播</title>
    <url>/machine_learning_hw04/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>严格来说不是课程的第四次作业，自己实现了一下全连接反向传播。 <span id="more"></span></p>
<p>反向传播的原理在这一篇<a href="https://samaelchen.github.io/machine_learning_step6/">博客</a>里面其实就已经大概讲过了，如果我们用的是sigmoid function作为激活函数，我们其实可以将每一层的一个神经元看做是一次逻辑回归。这里也不做太多解释，直接上代码。原本是想用MXNet实现的，但是MXNet和PyTorch都有自动求导函数，这样直接调用不利于深刻理解反向传播的具体过程，因此这里用numpy自己实现了一下。</p>
<p>首先定义自己的数据 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataset = np.array([[<span class="number">2.7810836</span>, <span class="number">2.550537003</span>, <span class="number">0</span>],</span><br><span class="line">                    [<span class="number">1.465489372</span>, <span class="number">2.362125076</span>, <span class="number">0</span>],</span><br><span class="line">                    [<span class="number">3.396561688</span>, <span class="number">4.400293529</span>, <span class="number">0</span>],</span><br><span class="line">                    [<span class="number">1.38807019</span>, <span class="number">1.850220317</span>, <span class="number">0</span>],</span><br><span class="line">                    [<span class="number">3.06407232</span>, <span class="number">3.005305973</span>, <span class="number">0</span>],</span><br><span class="line">                    [<span class="number">7.627531214</span>, <span class="number">2.759262235</span>, <span class="number">1</span>],</span><br><span class="line">                    [<span class="number">5.332441248</span>, <span class="number">2.088626775</span>, <span class="number">1</span>],</span><br><span class="line">                    [<span class="number">6.922596716</span>, <span class="number">1.77106367</span>, <span class="number">1</span>],</span><br><span class="line">                    [<span class="number">8.675418651</span>, -<span class="number">0.242068655</span>, <span class="number">1</span>],</span><br><span class="line">                    [<span class="number">7.673756466</span>, <span class="number">3.508563011</span>, <span class="number">1</span>]])</span><br></pre></td></tr></table></figure></p>
<p>这个数据集有十个样本，前面两列是feature，最后一列是y。</p>
<p>首先我们将一些零零散散的函数定义掉，比如说激活函数以及激活函数的导数，还有metric。这里使用了最经典的sigmoid作为激活函数，如果要用ReLu或者其他的都可以自己实现。 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span>(<span class="params">weights, inputs</span>):</span></span><br><span class="line">    z = np.dot(inputs, weights)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.exp(-z))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid_derivative</span>(<span class="params">weights, inputs</span>):</span></span><br><span class="line">    z = sigmoid(weights, inputs)</span><br><span class="line">    <span class="keyword">return</span> z * (<span class="number">1</span> - z)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">accuracy</span>(<span class="params">y, y_hat</span>):</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(y)):</span><br><span class="line">        <span class="keyword">if</span> y[i] == y_hat[i]:</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> count / <span class="built_in">len</span>(y)</span><br></pre></td></tr></table></figure></p>
<p>对于一个神经网络而言，实际上每一个隐藏层都是一组weight，因此我们定义一个函数来初始化隐藏层： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize_layer</span>(<span class="params">num_features, num_hidden</span>):</span></span><br><span class="line">    weights = np.random.uniform(-<span class="number">0.1</span>, <span class="number">0.1</span>, num_features * num_hidden).reshape((num_features, num_hidden))</span><br><span class="line">    <span class="keyword">return</span> weights</span><br></pre></td></tr></table></figure></p>
<p>这个函数是按照指定的输入特征数量和指定的隐藏节点数量生成一个weight matrix。这里我没有加入bias，当然要加入也很简单。</p>
<p>然后我们知道，全连接，或者说神经网络其实都是两个步骤，第一步forward propagation，计算结果，第二部backward propagation将误差告诉给weight。所以我们先实现第一步的forward propagation。 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">forwark_propagate</span>(<span class="params">network, inputs</span>):</span></span><br><span class="line">    outputs = []</span><br><span class="line">    input_data = [inputs]</span><br><span class="line">    outputs_derivative = []</span><br><span class="line">    next_inputs = inputs</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(network)):</span><br><span class="line">        output = sigmoid(network[i], next_inputs)</span><br><span class="line">        output_d = sigmoid_derivative(network[i], next_inputs)</span><br><span class="line">        outputs.append(output)</span><br><span class="line">        input_data.append(output)</span><br><span class="line">        outputs_derivative.append(output_d)</span><br><span class="line">        next_inputs = output.copy()</span><br><span class="line">    <span class="keyword">return</span> outputs, outputs_derivative, input_data[:<span class="built_in">len</span>(network)]</span><br></pre></td></tr></table></figure> 这里需要解释一下，我实现的比较绕，前馈的过程其实很简单，就是一个样本放进去，各种叉乘weight就好了，但是我们其实backward propagation需要用到很多中间过程的数据，回顾一下下面的过程：</p>
<p><img data-src='https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml029.png'></p>
<p><img data-src='https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml030.png'></p>
<p>我们可以看到，如果我们要计算第<span class="math inline">\(i\)</span>层cost function对<span class="math inline">\(w\)</span>的导数，我们需要用到第<span class="math inline">\(i\)</span>层的输入以及激活函数的导数，第<span class="math inline">\(i+1\)</span>层的weight和cost function对下一层<span class="math inline">\(z\)</span>的导数。所以我将forward propagation过程中每一层的输入，每一层的输出，每一层activation derivative在输出的取值都保存下来。那实际上，上一层的输出就是下一层的输入。</p>
<p>然后我们实现一下反馈，也就是最核心的部分： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward_propagate</span>(<span class="params">network, outputs, outputs_derivative, inputs, y_true</span>):</span></span><br><span class="line">    gradients = [<span class="number">1</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(network))]</span><br><span class="line">    deltas = [<span class="number">1</span> <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(network))]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">reversed</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(network))):</span><br><span class="line">        <span class="keyword">if</span> i == <span class="built_in">len</span>(network) - <span class="number">1</span>:</span><br><span class="line">            delta = -(y_true - outputs[i].flatten())</span><br><span class="line">            delta = delta.reshape(outputs[i].shape)</span><br><span class="line">            deltas[i] = delta</span><br><span class="line">            gradient = np.dot(inputs[i].T, delta)</span><br><span class="line">            gradients[i] = gradient</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            delta = outputs_derivative[i] * np.dot(deltas[i + <span class="number">1</span>], network[i + <span class="number">1</span>].T)</span><br><span class="line">            deltas[i] = delta</span><br><span class="line">            gradient = np.dot(inputs[i].T, delta)</span><br><span class="line">            gradients[i] = gradient</span><br><span class="line">    <span class="keyword">return</span> gradients</span><br></pre></td></tr></table></figure></p>
<p>这里我写的也还是有点绕，因为最后一层输出层是比较特殊的层，我们如果将这里看做是一个逻辑回归，那么我们就可以用之前逻辑回归推的方法，直接算出这一层的权重，然后就会发现，原本后面那个<span class="math inline">\(\sigma&#39;(z)[w_3 \frac{\partial l}{\partial z&#39;} + w_4 \frac{\partial l}{\partial z&#39;&#39;}]\)</span>其实就是<span class="math inline">\(-(y_{true} - y_{predict})\)</span>，然后我们一样的，将每一层的导数存下来。这样我们就把核心的部分全部实现了。然后就是试一下能不能跑。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">network = []</span><br><span class="line"><span class="comment"># network.append(initialize_layer(2, 1))</span></span><br><span class="line">network.append(initialize_layer(<span class="number">2</span>, <span class="number">4</span>))</span><br><span class="line">network.append(initialize_layer(<span class="number">4</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    output, output_d, inputs = forwark_propagate(network, dataset[:, :<span class="number">2</span>])</span><br><span class="line">    gradients = backward_propagate(network, output, output_d, inputs, dataset[:, <span class="number">2</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(network)):</span><br><span class="line">        network[i] -= <span class="number">0.1</span> * gradients[i]</span><br><span class="line">    res = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> output[<span class="built_in">len</span>(network) - <span class="number">1</span>]:</span><br><span class="line">        <span class="keyword">if</span> i &lt; <span class="number">0.5</span>:</span><br><span class="line">            res.append(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            res.append(<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Epoch %s, Accu %.2f%%&#x27;</span> %(<span class="built_in">str</span>(epoch), accuracy(dataset[:, <span class="number">2</span>], res)*<span class="number">100</span>))</span><br></pre></td></tr></table></figure>
<p>结果如下</p>
<pre><code>Epoch 0, Accu 50.00%
Epoch 1, Accu 50.00%
Epoch 2, Accu 70.00%
Epoch 3, Accu 100.00%
Epoch 4, Accu 100.00%
Epoch 5, Accu 100.00%
Epoch 6, Accu 100.00%
Epoch 7, Accu 100.00%
Epoch 8, Accu 100.00%
Epoch 9, Accu 100.00%
Epoch 10, Accu 100.00%
Epoch 11, Accu 100.00%
Epoch 12, Accu 100.00%
Epoch 13, Accu 100.00%
Epoch 14, Accu 100.00%
Epoch 15, Accu 100.00%
Epoch 16, Accu 100.00%
Epoch 17, Accu 100.00%
Epoch 18, Accu 100.00%
Epoch 19, Accu 100.00%</code></pre>
<p>当然，如果只有一层的话就是最普通的逻辑回归，效果也差不多。另外这样一个tiny fc可以试试看learning rate对结果的影响，效果非常明显。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>台大李宏毅机器学习——回归</title>
    <url>/machine_learning_step1/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>机器学习就是寻找一个函数来解决某一类问题，回归就是预测一个实数的方法。</p>
<span id="more"></span>
<p>假设我们要预测神奇宝贝的CP值，我们用 <span class="math inline">\(x\)</span> 来表示一只神奇宝贝。用 <span class="math inline">\(x_{cp}\)</span> 表示CP值，用 <span class="math inline">\(x^i\)</span> 表示 <span class="math inline">\(i\)</span>th 神奇宝贝。</p>
<p>如果我们想解决这样一个问题，我们可以参照下面的步骤：</p>
<p>Step1. 假设影响神奇宝贝CP值的真实函数是 <span class="math inline">\(y = b + w \cdot x_{cp}\)</span>，而集合 <span class="math inline">\(\{f_1, f_2, \dots \}\)</span> 是我们的模型。理论上这个集合包含无数多个函数。这个函数假设 CP 值只受原来 CP 值的影响。一般而言，我们用 <span class="math inline">\(y = b + \sum w_i \cdot x_i\)</span>, <span class="math inline">\(x_i\)</span> 表示所有的特征。</p>
<p>Step2. 我们需要找一个评价标准来评价我们模型的好坏。在这个例子中，我们可以用 MSE(Mean Square error)。 <span class="math inline">\(MSE = \sum (y_i - \hat{y}_i)^2\)</span>。习惯上我用 <span class="math inline">\(y\)</span> 表示真实值，<span class="math inline">\(\hat{y}\)</span> 表示预测值。这一点跟视频中不太一样，不过只是个标记而已，并不影响理解。实际上有很多类似 MSE 的方程用来衡量预测值与实际值的差距，我们称之为 Loss function。那么第一步中的MSE可以写作 <span class="math display">\[L(f) = \sum (y^n - f(x^n))^2 = \sum (y^n - (w \cdot x^n + b))^2.\]</span></p>
<p>Step3. 现在我们只需要找到损失函数最小的一个假设 <span class="math inline">\(f\)</span>。这个例子中我们要做的就是 <span class="math display">\[\arg \min L(w, b) = \sum (y^n - (w \cdot x^n + b))^2.\]</span> 这个函数可以用最小二乘来直接解决，但是通常实践上我们用梯度下降来做。我们记损失函数为<span class="math inline">\(L(w, b)\)</span>，而且只有一个参数<span class="math inline">\(w\)</span>.</p>
<ul>
<li>首先，我们随机初始化一个 <span class="math inline">\(w^0\)</span></li>
<li>其次，计算 <span class="math inline">\(\frac{\partial{L}}{\partial{w}}|_{w=w^0}\)</span>.</li>
<li>第三，更新 <span class="math inline">\(w\)</span>。 <span class="math inline">\(w^1 = w^0 - \eta \frac{\partial{L}}{\partial{w}}|_{w=w^0}\)</span>, <span class="math inline">\(\eta\)</span> as learning rate.</li>
<li>重复上面三个步骤直到 <span class="math inline">\(\frac{\partial{L}}{\partial{w}}|_{w=w^n} = 0\)</span>.</li>
</ul>
<p>事实上梯度下降是一个典型的贪心算法，很容易停在saddle point或者局部最优解上。通常损失函数非常复杂的时候会这样（当然，理论上会这样，实际上会比这个更惨）。</p>
<p>如果现在有多个参数呢？我们可以用一个向量的方式来表示 <span class="math display">\[
\bigtriangledown L = \begin{bmatrix}
\frac{\partial{L}}{\partial{w}} \\
\frac{\partial{L}}{\partial{b}} \\
\end{bmatrix}_{\text{gradient}}.
\]</span></p>
<p>在这个例子当中，李老师得到了 <span class="math inline">\(\text{error}_{\text{training}} = 31.9\)</span> and <span class="math inline">\(\text{error}_{\text{test}} = 35\)</span>.</p>
<p>我们这里讨论了最简单的一个模型，如果现在提高模型复杂度会怎么样呢？如果我们假设 <span class="math inline">\(y = b + w_1 \cdot x + w_2 \cdot x^2\)</span>，我们就能得到 <span class="math inline">\(\text{error}_{\text{training}} = 15.4, \ \text{error}_{\text{test}} = 18.4\)</span>，看起来好了不少。如果进一步假设 <span class="math inline">\(y = b + w_1 \cdot x + w_2 \cdot x^2 + w_3 \cdot x^3\)</span>， 则 <span class="math inline">\(\text{error}_{\text{training}} = 15.3,\  \text{error}_{\text{test}} = 18.1\)</span>。</p>
<p>如果现在假设 <span class="math inline">\(x\)</span> 是四次方的一个方程，则 <span class="math inline">\(\text{error}_{\text{training}} = 14.9, \ \text{error}_{\text{test}} = 28.8\)</span>. 如果假设<span class="math inline">\(y = b + w_1 \cdot x + w_2 \cdot x^2 + w_3 \cdot x^3 + w_4 \cdot x^4 + w_5 \cdot x^5\)</span>，则<span class="math inline">\(\text{error}_{\text{training}} = 12.8，\ \text{error}_{\text{test}} = 232.1\)</span>.</p>
<p>当模型复杂度提高的时候，模型在训练集的错误会下降，测试集的也会下降，但是当模型复杂度到了一定程度，随着训练集错误下降，测试集错误上升，我们称之为 <strong>overfitting</strong>。</p>
<p>事实上，解决overfitting的方法可以是使用domain knowledge来设计一个更合理的方程，或者减少模型的复杂度。另一个操作性更强的方法就是增加数据和feature。</p>
<p>比如我们收集了四个神奇宝贝，增加了数据量，那么我们可以把模型设计为： <span class="math display">\[
\begin{align}
y &amp;= b_1 \cdot \delta(x_s=\text{Pidgey}) \\
&amp;+ w_1 \cdot \delta(x_s = \text{Pidgey})x_{cp} \\
&amp;+ b_2 \cdot \delta(x_s = \text{Weedle}) \\
&amp;+ w_2 \cdot \delta(x_s = \text{Weedle})x_{cp} \\
&amp;+ b_3 \cdot \delta(x_s = \text{Caterpie}) \\
&amp;+ w_3 \cdot \delta(x_s = \text{Caterpie})x_{cp} \\
&amp;+ b_4 \cdot \delta(x_s = \text{Eevee}) \\
&amp;+ w_4 \cdot \delta(x_s = \text{Eevee})x_{cp}
\end{align}
\]</span> 其中 <span class="math display">\[
\delta(x_s = Pidgey) = \begin{cases}
=1 \quad \text{if } x_s = \text{Pidgey} \\
=0 \quad \text{otherwise}
\end{cases}.
\]</span></p>
<p>这样我们得到的就是<span class="math inline">\(\text{error}_{\text{training}} = 3.8, \; \text{error}_{\text{test}} = 14.3\)</span>.</p>
<p>如果我们继续增加特征或者模型复杂度，有可能再次导致overfitting，所以这里我们对损失函数进行改造： <span class="math display">\[
L = \sum_n \Big(y^n - \big( b + \sum w_i x_i \big) \Big)^2 + \lambda \sum (w_i)^2.
\]</span> 我们增加了一个惩罚项。当模型过于复杂的时候，这个惩罚项会变得很大。因此，越小的系数效果越好。越小的系数意味着越光滑的模型。一般来说我们认为比较光滑的模型更有可能正确。我们将这个方法叫做regularization。这里我们可以看到，我们没有必要对<span class="math inline">\(b\)</span>做regularaizaion，因为这个参数仅仅影响模型上下移动，并不影响模型的复杂度。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>台大李宏毅机器学习——半监督学习的四种方法</title>
    <url>/machine_learning_step10/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>传统机器学习，或者说现在主流的机器学习都是有监督学习，也就是supervised learning。但是实际上，数据好拿，labeled data不好拿。这就是我们为什么要用semi-supervised learning。</p>
<span id="more"></span>
<p>semi-supervised learning并不一定总会work，这要看你的假设是否真的会成立。</p>
<h1 id="em算法">EM算法</h1>
<p>现在我们来看一下半监督模型如何使用在generative model。之前我们学的是supervied learning，都是在labeled data上做的假设，如下图：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml059.png></p>
<p>那现在我们如果有了其他的unlabeled data，在实践上，我们就不能够用原来labeled data的<span class="math inline">\(\mu, \Sigma\)</span>。也就是说，这些unlabeled data的分布会影响我们对均值和方差的假设，从而影响我们算posterior probability。</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml060.png></p>
<p>那么我在做semi-supervised generative model步骤如下：</p>
<ul>
<li><p>首先初始化参数：<span class="math inline">\(\theta = \{P(C_1), P(C_2), \mu^1, \mu^2, \Sigma \}\)</span></p></li>
<li><p>其次计算unlabeled data的后验概率：<span class="math inline">\(P_{\theta}(C_1 | x^u)\)</span></p></li>
<li><p>然后更新模型：<span class="math inline">\(P(C_1) = \frac{N_1 + \Sigma_{x^u}P(C_1|x^u)}{N}\)</span>，其中<span class="math inline">\(N\)</span>是所有的sample，<span class="math inline">\(N_1\)</span>是标记为<span class="math inline">\(C_1\)</span>的样本。那如果不考虑unlabeled data，原来<span class="math inline">\(C_1\)</span>的概率就是<span class="math inline">\(\frac{N_1}{N}\)</span>。现在我们因为考虑了unlabeled data，因此我们在计算的时候就需要加上所有unlabeled data属于<span class="math inline">\(C_1\)</span>的概率。而<span class="math inline">\(\mu_1 = \frac{1}{N} \sum_{x^r \in C_1} x^r + \frac{1}{\sum_{x^u} P(C_1|x^u)} \sum_{x^u}P(C_1|x^u)x^u\)</span>。然后重复更新后验概率，直到收敛。</p></li>
</ul>
<p>这两个步骤，第一个计算后延概率就是计算expectation，更新模型就是做maximization，也就是EM模型。但是这个模型会受到初始化参数的影响，这一点跟梯度下降很像。</p>
<p>那为什么这个模型要这样设计呢？</p>
<p>首先回过头来看极大似然法，在只有labeled data的情况下，likelihood可以写作<span class="math inline">\(log L(\theta) = \sum_{x^r} log P_{\theta}(x^r, y^r)\)</span>，其中<span class="math inline">\(P_{\theta}(x^r, y^r) = P_{\theta}(x^r|y^r)P(y^r)\)</span>。这个算法，在我们假设的概率分布已知的情况下，很容易就能算出来。</p>
<p>在考虑unlabeled data的情况下，我们的likelihood就可以写作是<span class="math inline">\(log L(\theta) = \sum_{x^r} log P_{\theta}(x^r, y^r) + \sum_{x^u} log P_{\theta}(x^u)\)</span>，其中<span class="math inline">\(P_{\theta}(x^u) = P_{\theta}(x^u|C_1)P(C_1) + P_{\theta}(x^u|C_2)P(C_2)\)</span>。因为未标注的数据我们不确定到底属于哪一种类别。那EM算法做的事情，就是让这个log likelihood最大。跟极大似然法不一样的地方就是，EM算法只能一次次迭代来逼近最优解。</p>
<h1 id="low-density-separation">Low density separation</h1>
<p>另外一个semi-supervised learning的方法是self-training。这种方法假设的就是这个世界上的数据只有一种标签，那self-training的方法就是先用labeled data训练数据，然后对unlabeled data进行分类。分类之后我们选择比较confidence的结果加入训练集继续训练模型。但是这里要注意，这个过程实际上不能在regression上起到效果。因为regression输出的就是个real number，而这个predict的值就是原来的函数计算的，所以再放进来也没法对模型起到调优的效果。</p>
<p>self-training有一个进阶版本，也就是entropy-based regularization。这个值用来计算我们预测结果的分布是否集中。也就是说，我们要计算<span class="math inline">\(\hat{y}^u\)</span>的entropy，也就是<span class="math inline">\(E(\hat{y}^u = \sum_{m=1}^M \hat{y}_m^u ln(\hat{y}_m^u))\)</span>。那我们希望这个值越小越好。越小，说明值越集中，越大说明越分散。所以我们的loss function就可以从原来的cross entropy改进为<span class="math inline">\(L = \sum_{x^r}C(y^r, \hat{y}^r) + \lambda \sum_{x^u} E(\hat{y}^u)\)</span>。这个结果就可以用梯度下降来做，另外因为后面加上了unlabeled data的这个尾巴，看上去就像是之前做regularization，所以这里就叫做了regularization。</p>
<p>那这两个方法都属于low density separation，也就是假设这个世界是非黑即白的。</p>
<h1 id="smoothness-assumption">Smoothness assumption</h1>
<p>第三种方法就是smoothness assumption。简单说就是如果有两笔数据<span class="math inline">\(x_1, x_2\)</span>，如果他们在high density region是类似的，那么他们的标签应该是一致的。如下图：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml061.png></p>
<p>图中的<span class="math inline">\(x^1\)</span>和<span class="math inline">\(x^2\)</span>更接近，而<span class="math inline">\(x^3\)</span>就更远。另一个直观一点的例子就是：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml062.png></p>
<p>在两个2之间，虽然在像素点上看起来不是很像，但是中间有很多过渡的数据，因此左边两个2是类似的。同时，我们没有2过渡到3的数据，因此右边的2和3很不像。</p>
<p>那实践上，要做到这一点的一种方法就是先做cluster，然后做label。但是实际上，这个做法真正实践起来是比较麻烦的。</p>
<p>另一种方法就是graph-based approach。也就是说两个数据点必须有通道可以到达这样才是距离近的，如果两个数据点之间没有连接，那就是距离远的。这样做一般需要设计好如何计算两个数据点之间的相似度<span class="math inline">\(s(x^i, x^j)\)</span>。然后我们可以用KNN或者e-neighborhood来添加edge。如下图：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml063.png></p>
<p>KNN是连接离自己最近的K个点，而e-neighborhood是去连接半径为e内的所有点。</p>
<p>除了添加edge之外，我们可以给edge加上weight，可以用similarity，也可以用RBF（径向基）<span class="math inline">\(s(x^i, x^j) = exp(- \gamma||x^i - x^j||^2)\)</span>。因为这里用了exponential，因此只有非常近的两点才会有高的similarity。</p>
<p>graph-based的方法的好处是可以将label传递出去，当然这样的做法就需要海量的数据，否则这种连接就可能断掉。如下图：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml064.png></p>
<p>那么如何从数学上来计算这件事情？</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml065.png></p>
<p>如上图，我们定义一个值叫smooth，smooth越小，就越好。这个公式也可以用矩阵的方式来表示： <span class="math display">\[
S = \frac{1}{2} \sum_{i,j} w_{i,j}(y^i - y^j)^2 = \boldsymbol{y}^{\top} \boldsymbol{Ly}
\]</span> 那这里的<span class="math inline">\(\boldsymbol{y}\)</span>是包含了labeled data和unlabeled data的一个向量，所以dimension是<span class="math inline">\(R+U\)</span>。那<span class="math inline">\(\boldsymbol{L}\)</span>相应的就是一个<span class="math inline">\((R+U) \times (R+U)\)</span>的一个矩阵。那<span class="math inline">\(\boldsymbol{L}\)</span>这个矩阵叫做<strong>graph laplacian matrix</strong>，定义为<span class="math inline">\(L = D - W\)</span>，其中<span class="math inline">\(D\)</span>是原来graph的邻接矩阵，而<span class="math inline">\(W\)</span>则是<span class="math inline">\(D\)</span>的degree组成的对角矩阵。如下图：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml070.png></p>
<p>那我们也就可以将这个smooth作为一个惩罚项加入到原来cross entropy这个loss function中，得到<span class="math inline">\(L = \sum_{x^r} C(y^r, \hat{y^r}) + \lambda S\)</span>。</p>
<p>上面是三种半监督学习的方法。第四种方法是寻找latent factor。这个在后面的无监督学习里讲。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>台大李宏毅机器学习——neighbor embedding &amp; auto-encoder</title>
    <url>/machine_learning_step12/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>无监督学习的neighbor embedding和deep auto-encoder部分。 <span id="more"></span></p>
<h1 id="neighbor-embedding">neighbor embedding</h1>
<p>近邻嵌入其实在做的事情就是manifold，白话一点说就是把高维的数据强行压缩到低维空间中。</p>
<h2 id="locally-linear-embedding">locally linear embedding</h2>
<p>这个模型一开始假设，每一个<span class="math inline">\(x_i\)</span>都可以是<span class="math inline">\(x_j\)</span>的linear combination。所以这样就有了一个假设，我们在原始的数据空间中，我们有这样需要优化的函数<span class="math inline">\(\sum_i||x_i - \sum_j w_{i,j} x_j||_2\)</span>。我们希望这个函数越小越好。那相应的，当我们将数据映射到一个新的空间中的时候，我们得到<span class="math inline">\(\{z_i\}\)</span>，我们同样希望，在这个空间中，我们能够保留原来数据空间中的特性，也就是原来接近的数据点在这个空间中还是接近的，而且两者之间的关系不变。所以相应的，在这个新的空间中，我们又有一个可以优化的函数<span class="math inline">\(\sum_i||z_i - \sum_j w_{i,j} z_j||_2\)</span>。</p>
<p>那么我们做的事情就是，用第一个优化的函数找<span class="math inline">\(w_{ij}\)</span>，然后固定住<span class="math inline">\(w_{ij}\)</span>，找一组<span class="math inline">\(\{ z_i \}\)</span>使得第二个优化函数最小。</p>
<p>那用LLE一般来说，邻居选太多或者选太少效果都不会太好。如下图：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml071.png></p>
<h2 id="laplacian-eigenmaps">laplacian eigenmaps</h2>
<p>拉普拉斯特征映射用的是graph-based的方法。那跟之前semi-supervised learning就用过这种方法。在半监督学习的时候，我们的loss function是被设计为交叉熵加上一个相似度。但是在有监督学习的时候，我们有labeled data，unsupervised learning是没有labeled data的，因此，我们的函数就设计为<span class="math inline">\(S = \frac{1}{2} \sum_{i,j} w_{ij}(z_i - z_j)^2\)</span>。</p>
<p>那这里有一个问题，如果不加限制的话，我们一开始就将原来所有的点都映射到新空间中的一个点上，那不就使得这个函数最小了吗？所以这里需要加上一定的限制。假设我们新的空间上有<span class="math inline">\(M\)</span>个维度，我们希望在新的空间上，<span class="math inline">\(\text{Span} \{z_1, z_2, \dots, z_N \} = R^M\)</span>。白话一点讲就是我们新空间上的数据摊开后，可以铺满新的空间。</p>
<h2 id="t-sne">t-SNE</h2>
<p>t-SNE全称是T-distributed Stochastic Neighbor Embedding。</p>
<p>那上面两种方法的缺点就是，这两种方法可以找到接近的点，也就是说他们可以保留原来相近点的信息，但是无法保持原来很远的两点的信息。比如这两个算法在mnist上面做降维的时候会得到每一个类别的图像都聚集在一起，但是每一个类别也都堆叠在一起。</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml072.png></p>
<p>那t-SNE比较强大的地方就是可以同时保留相近点的信息，也能保留两个远的点的信息，同时可以将这种gap放大。</p>
<p>那t-SNE的设计就是，在原来的空间上，我们定一个概率：<span class="math inline">\(P(x_j | x_i) = \frac{S(x_i, x_j)}{\sum_{k \ne l} S(x_k, x_l)}\)</span>。在新的空间上面我们也定义一个概率：<span class="math inline">\(Q(z_j | z_i) = \frac{S&#39;(z_i, z_j)}{\sum_{k \ne l} S&#39;(z_k, z_l)}\)</span>。这里我们可以发现，t-SNE一个很不一样的地方就是，两个空间上计算similarity的函数可以不一样。</p>
<p>然后我们要做的事情就是，我们尽可能让原始空间上的分布和新空间上的分布尽可能相似，那用KL divergence，也就是相对熵来计算（默默觉得信息论也得看起来的节奏）。 <span class="math display">\[
L = \sum_i KL(P(*|x_i) || Q(*|z_i)) = \sum_i \sum_j P(x_j | x_i) \log(\frac{P(x_j|x_i)}{Q(z_j|z_i)})
\]</span></p>
<p>那因为这边用的是probability，所以我们就可以更改我们的similarity function。那t-SNE选用的similarity函数分别是，在原空间上<span class="math inline">\(S(x_i, x_j) = \exp(-||x_i - x_j||_2)\)</span>，在新空间上<span class="math inline">\(S&#39;(z_i, z_j) = \frac{1}{1+||z_i - z_j||_2}\)</span>。那效果是这样的：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml073.png></p>
<p>也就是做到原来近的更近，原来远的更远。目前高位数据可视化最好的方案就是t-SNE。</p>
<h1 id="deep-auto-encoder">deep auto-encoder</h1>
<p>深度自编码模型其实在某种程度上而言，跟PCA非常相似。我们回顾一下PCA的过程，PCA本身就可以看做是一个加密解密的过程，如果用神经网络的方式来表现，那就是input layer encoding到一个hidden layer，我们叫做Bottleneck layer。因为在整个网络结构中就是最窄的位置，就像是瓶颈。然后从这个hidden layer还原。所以整个PCA的结构就可以用神经网络的结构表示为：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml074.png></p>
<p>那这样表示的PCA优化的目标就是之前讲到的用最小误差来做。那实际上如果看深度学习圣经的话，书里面用的就是这个方法来求的。</p>
<p>所以我们就可以从PCA很自然引导到deep auto-encoder。在结构上看，deep auto-encoder就是在bottleneck layer前面加好几个hidden layer。结构大致如下：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml075.png></p>
<p>那这里需要注意一点，那就是deep auto-encoder已经不需要RBM来做initialize了。另外这几层layer之间的weight也没必要保证是之前的transpose，因为这样的限制在早期是为了加速训练，减少需要学习的参数。</p>
<p>deep auto-encoder的效果相对会比PCA更好一点，因为PCA是linear的转换，而deep auto-encoder是非线性的变化，所以效果上会更好一点。</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml076.png></p>
<p>那deep auto-encoder在训练的时候要注意的事情就是，我们一定是要有bottleneck layer的，因为如果我们把每一个hidden layer都设计得比input layer大，那很有可能machine学到的就是直接复制一份input layer出来，这样input跟output的误差就最小了。</p>
<p>那所以如果我们不放bottleneck layer的话，就一定要在input中增加一些noise。</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml077.png></p>
<p>加noise这个事情，其实不管怎么训练deep auto-encoder都可以放，只是说如果没有降维的动作，那一般在实践上是一定要放的。放多少就看缘分吧。后面讲VAE会让模型自己学这个noise。</p>
<p>那模型的训练也就是反向传播，用梯度下降的方法来求。万能的梯度下降。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>台大李宏毅机器学习——词向量</title>
    <url>/machine_learning_step13/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Word embedding <span id="more"></span></p>
<h1 id="word-embedding">Word embedding</h1>
<p>Word embedding之前比较流行的叫法是word vector。那其实要理解word embedding之前我们需要回过头看一下到底之前是如何做这个操作的。</p>
<p>在很古老很古老的时候，如果我们要用向量表示一个单词，只能用一个one-hot的方法来表示，也就是一串很长很长的0-1向量。这个很长很长的向量长度跟单词的数量一样多。比如说，我们有10w个英文单词，那么这个向量就有10w维，然后给每个词在这个向量里面找个位置标记为1，其他位置标记为0，这样就得到了最原始的词向量。</p>
<p>但是这个向量不用想都知道，一个很突出的问题，太大了。另外有一个很大的问题就是这样的表示，没有办法表达出词语的含义。所以word embedding做的事情就是将这个很长很长的向量，压缩到低维。比如现在最常用的100-200维之间。</p>
<p>那word embedding实际上可以做到通过读海量的文档内容，然后理解单词的意思。比如 The cat sat on the pat和The dog sat on the pat这两句话，cat和dog是接近的。</p>
<p>那做到word embedding有两种做法。第一种是计算词语的共现次数，另一种是通过上下文的方法去做预测。</p>
<h2 id="词语共现">词语共现</h2>
<p>这种做法的代表是Glove Vector。这种方法是假设两个经常共同出现的词，他们的向量应该是类似的。</p>
<p>用其实就是<span class="math inline">\(V(w_i) \cdot V(w_j)\)</span>和他们共现的次数<span class="math inline">\(N_{ij}\)</span>相关。</p>
<h2 id="基于预测">基于预测</h2>
<p>基于预测的word embedding做法一般是按照输入的单词，预测输出的单词。就像下图表示的：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml078.png></p>
<p>这样的方法可以用来文本接龙，当然也可以用于语音辨识上面。</p>
<p>那实践上，我们做的事情就像下面：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml079.png></p>
<p>我们实际上得到的word vector就是中间绿色层的output。那理论上而言，我们在这一层得到的vector在某种程度上，可以体现一定程度的语义。因为同样类型的词，后面需要predict的词应该也是类似的。</p>
<p>那我们做predict-based approach的时候，我们实际使用的神经网络仅有一层，而且激活函数是linear的。那这么做的原因其实是因为作者实验发现，单层网络就可以做到很好的效果，同时，因为我们需要训练海量的数据，因此单层的网络速度上可以做到更快。</p>
<p>现在我们直接考虑这个模型，我们在训练模型的时候，其实会碰到一个很明显的问题，那就是我们做linear transform的时候，每一个input layer需要乘上一个非常大的weight matrix。所以实践上，我们会用共享参数的方法：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml080.png></p>
<p>那我们用这样的方法有什么好处呢？第一是我们不需要训练非常多很大的weight matrix，另外同一个word不会得到不一样的vector。训练模型的方法一般有两种，一种是cbow，一种是skip-gram。cbow就是用上下文猜中间的词，skip-gram是按照中间的词猜上下文。两个结构大概如下：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml081.jpg></p>
<p>不过坦白说，课程里面讲的比较少，也不是很深。网上流传最广的是有道团队写的一篇。不过渣渣表示，二十几页拆源码的看起来好累。实践上有很多trick的地方，比如说最后的loss function用的不是softmax，现在用的比较多的是nce。文章及代码可以参考项亮的<a href="https://zhuanlan.zhihu.com/p/21642643">专栏</a>或者<a href="http://www.jianshu.com/p/e439b43ea464">简书</a>。或者参考TensorFlow的一个实现<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="https://fangpin.github.io/2016/08/22/word2vec
">[1]</span></a></sup>，代码看起来相对简单一点点。TensorFlow这个实现可以比较清楚看懂训练数据是如何准备的，之前一直没搞懂的就是不知道训练数据是怎么准备的。不过这个博客里面说用的是cbow，我看了源码，感觉博主写的不是cbow，就是根据上一个词猜下一个词。</p>
<p>最好理解的就是训练数据在准备的时候需要准备两份，第一份是词表以及词频等数据，另一个就是每个sentence，这样才能找到每个词的上下文。然后根据不同的模型决定如何设计feature和target。</p>
<div id="footnotes">
<hr>
<div id="footnotelist">
<ol style="list-style: none; padding-left: 0; margin-left: 40px">
<li id="fn:1">
<span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">https://fangpin.github.io/2016/08/22/word2vec<a href="#fnref:1" rev="footnote"> ↩︎</a></span>
</li>
</ol>
</div>
</div>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>台大李宏毅机器学习-GAN简介</title>
    <url>/machine_learning_step14/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>生成模型，可以用来做炼金术师的禁忌之术，人体炼成。 <span id="more"></span></p>
<p>生成模型基本上有三大类做法，一种是基于序列的预测，比如可以用RNN。一种是基于deep auto-encoder。最后一种就是现在最火的GAN。</p>
<h1 id="基于序列预测">基于序列预测</h1>
<p>基于序列预测的方法听上去很像w2v，就是把每一个pixel看做是一个词，然后用训练的时候就像是训练w2v一样。所以整个训练的过程就是：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml082.png></p>
<p>那这种方法有一个很强大的应用就是做WaveNet。</p>
<h1 id="auto-encoder">auto-encoder</h1>
<p>前几节课有讲过一个auto-encoder，实际操作的时候就是在抽取pattern，然后再还原回原来的样子。所以原理上来说其实我们可以输入一个vector，然后通过decoder输出一个新的结果。</p>
<p>但是用auto-encoder有一个很大的问题，那就是很多时候，其实我们输入的vector不会得到很好的结果。因为我们放进去的vector是decoder没见过的code。这个就很好理解，我们可以想象一个加密解密过程。我们将原文加密成密文，接收方用密钥解密得到原文。那现在我们用了另一个加密方法加密得到的密文发过去，接收方就没法解密了。</p>
<p>基于上面的情况，我们就有了新的一个加强版auto-encoder，就是VAE。</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml083.png></p>
<p>那这里我们有两个minimize的方向，一个是跟auto-encoder一样的minimize reconstruction error，另一个就是上图右下角一个minimize的项目。简单来说VAE自动学习了两个noise，一个是<span class="math inline">\(\sigma\)</span>另一个是<span class="math inline">\(e\)</span>。<span class="math inline">\(\sigma\)</span>其实是机器自己学习出来的noise，那如果不做这样的限定的话，出于最小化reconstruction error的目的，我们就可以让<span class="math inline">\(\sigma\)</span>变得非常大，那这样的话noise就变得很小。noise越小，那么这个模型就回到了原来的auto-encoder的模型上去了。那加noise的目的直观来讲，可以认为是给了一些可能性让模型来生成没见过的密文。</p>
<h1 id="gan">GAN</h1>
<p>GAN本身是一个很有意思的设计。GAN设计了两个网络，一个网络是generate网络，用来生成模型，另外一个网络是discriminator网络，用来分辨两个图是模型生成的还是真实的。</p>
<p>那这样设计的目的其实也是很明显的，generate网络的目的是去尽可能逼近真实分布，就像下面的样子：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml084.png></p>
<p>那我们要去衡量两个分布是否相近是很难的，所以就设计了一个分类器来分辨。当分类器分不清到底是真实图片还是生成的图片的时候，那么我们就可以认为两个分布很接近了。</p>
<p>那整个的训练过程就像是一个进化的过程。一开始，第一代generate模型生成的结果会被第一代的discriminator模型区分，然后更新参数，第二代generate模型生成的结果被第二代discriminator模型区分。一直迭代到discriminator模型分不清为止。</p>
<p>GAN一开始是非常难训练的，因为GAN一开始设计的衡量方法是JS divergence。JS divergence在两个分布没有overlap的时候，计算出来的结果都是一样的，所以模型本身不知道自己是不是训练的越来越好。后来WGAN是一个新的解决方案，将JS divergence改为Wasserstein distance。Wasserstein distance是一个可以衡量两个分布是不是越来越接近的方法，因此用WGAN就可以让模型知道是不是训练效果越来越好。在NLP里面，因为语言不是连续分布，所以WGAN就可以用来生成语句，而原来的GAN就不可以。这两个的差别就像下图：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml085.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml086.png></p>
<p>那GAN有各种各样的变种，具体还是要多读论文。</p>
<p>这个都是一些简单介绍，具体的原理还是要读原来的paper，推公式。留个大坑，先把课程上完。括弧笑。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>台大李宏毅机器学习——无监督学习</title>
    <url>/machine_learning_step11/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>无监督学习——clustering &amp; PCA，主要介绍了PCA，顺带讲了一些clustering。 <span id="more"></span></p>
<p>无监督学习课里被分为clustering，dimension reduction，和generate。</p>
<p>clustering是关注在样本方向上的，一般在数据里面就是行方向上的。dimension reduction是对feature的降维，也就是在列方向上的收缩。</p>
<h1 id="clustering">clustering</h1>
<p>clustering其实是无监督算法的一个大类，比较常用的是算法有层次聚类和k-means。</p>
<p>k-means是一种非常简单的算法，就是一开始先随机选择一些k个初始点，然后计算每一个点离这k个点的距离，选择最近的一个点合并为一类。然后取这k个类的中心点作为新的中心点，再更新一次，直到中心点不再移动为止。</p>
<p>k-means算法是非常简单的算法，后面有了很多新的玩法，现在大部分框架最常用的其实是k-means++，是对初始点选择的一些优化的k-means算法。后面其他玩法还有加核函数，做半监督啥的。其实这样的一些做法都是让算法更稳健一点，但是其实k-means最麻烦的地方是对k的选择。</p>
<p>那除了k-means以外，还有一种算法就是层次聚类（HAC），层次聚类就是一开始就计算每个sample两两之间的距离，然后将最近的几个点合并起来，一直合并到只剩下一类。看上去就像是一棵树。那最后我们只要决定在一个什么样的阈值下，将数据分成几类。示意图如下：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml066.png></p>
<h1 id="pca">PCA</h1>
<p>降维的方法有很多种，最简单粗暴的方法就是drop feature。那实际上在回归中就有这样的算法来做这样的事情。比如forward，backward，还有stepwise。这些都是直接drop一些feature的方法。</p>
<p>另外一种常用的方法就是PCA。PCA实际上是对原来的数据进行一次线性转换，映射到新的坐标空间中。它能起到降维的效果是，我们只需要少量的坐标，就可以最大程度上还原数据。</p>
<p>PCA的数学解法有很多种角度，一种是最大方差法，一种是投影再恢复的最小误差法，还有一种是根据SVD求解。鉴于自己忘到很捉急的线代水平，这里就找一种我能理解的解法推算一下好了。（默默滚去补线代）</p>
<p>这里我们考虑最大方差法，需要使用拉格朗日乘子来求解（就是视频里面的Lagrange multiplier）。我们这里假设所有的向量默认都是列向量。下面开始证明：</p>
<p>假设我们有<span class="math inline">\(n\)</span>个维度<span class="math inline">\(\{x_1, x_2, \dots, x_n \}\)</span>，每个列向量有<span class="math inline">\(m\)</span>个元素。我们对这些样本做一个中心化的动作，就是每一行的元素减去各自行的均值，我们可以表示为<span class="math inline">\(\{x_1 - \bar{x_1}, x_2 - \bar{x_2}, \dots, x_n - \bar{x_n} \}\)</span>。做中心化的目的就是使得我变化后的矩阵每一行均值为0，这样算后面的协方差矩阵会更好算一点。</p>
<p>算协方差矩阵之前，我们可以看一下如何计算一维的方差。对于包含<span class="math inline">\(m\)</span>个元素的一维向量<span class="math inline">\(a\)</span>，我们可以算<span class="math inline">\(Var(a) = \frac{1}{m} \sum_i^m(a_i - \bar a)^2\)</span>，如果按照上面说的都减掉均值的话，我们就可以得到<span class="math inline">\(\bar a = 0\)</span>，因此我们可以让方差简化为<span class="math inline">\(Var(a&#39;) = \frac{1}{m} \sum_i^m (a&#39;)^2\)</span>，因为向量点内积可以表示为向量的转置乘向量，所以上式又可以改写为<span class="math inline">\(Var(a&#39;) = \frac{1}{m} a&#39;^{\top} a&#39;\)</span>。</p>
<p>那现在我们知道，多维数据我计算的是协方差，所以同样减掉均值以后，我们的协方差也可以化简到各个维度的内积。所以我们可以把协方差求出来<span class="math inline">\(Cov(a, b) = \frac{1}{m} \sum_i^m a_i b_i = \frac{1}{m} a \cdot b = \frac{1}{m} a^{\top} b\)</span>（嗯，其实最好每个向量都用<span class="math inline">\(\boldsymbol{a}\)</span>或者<span class="math inline">\(\vec{a}\)</span>这样的形式表示向量，不过写<span class="math inline">\(\LaTeX\)</span>真的挺烦的，所以就意会哈）。</p>
<p>现在再假设我们有一组基<span class="math inline">\(\{u_1, u_2, \dots u_m \}\)</span>可以完全对应原来的维度。那我们希望中心化以后的数据在<span class="math inline">\(u_1\)</span>方向上散的最开，也就是方差最大。因为我们的数据做了中心化处理，所以我们可以将这个事情用公式表示为<span class="math inline">\(\frac{1}{n} \sum_i^n(x_i \cdot u_1)^2\)</span>（嗯，这里我又偷懒了，假装我们现在的<span class="math inline">\(x_i\)</span>都是已经中心化以后的向量）。那这个公式就可以转化为<span class="math inline">\(\frac{1}{n} \sum_i^n (x_i^{\top} u_1)^{\top} (x_i^{\top} u_1)= \frac{1}{n} \sum_i^n u_1^{\top} x_i x_i^{\top} u_1\)</span>。我们用<span class="math inline">\(X = [x_1, x_2, \dots, x_n]\)</span>来表示所有的数据，那么因为上面<span class="math inline">\(u_1\)</span>跟<span class="math inline">\(i\)</span>无关，因此我们可以将<span class="math inline">\(u_1\)</span>提到求和符号外面，那我们的公式可以进一步化简为<span class="math inline">\(\frac{1}{n} u_1 (\sum_i^n x_i x_i^{\top}) u_1 = \frac{1}{n} u_1 XX^{\top} u_1\)</span>。</p>
<p>那么现在问题来了，算到这一步，如何计算能得到一个<span class="math inline">\(u_1\)</span>使得上式最大呢？我们对公式分解一下发现<span class="math inline">\(XX^{\top}\)</span>其实非常的眼熟，这不就是原来各个维度的协方差矩阵么，那我们用<span class="math inline">\(S\)</span>来表示，<span class="math inline">\(S\)</span>是一个实对称矩阵。那现在我们可以想一下手上有哪些条件，第一我们希望求的是<span class="math inline">\(\arg \max(u_1^{\top} S u_1)\)</span>，同时，因为<span class="math inline">\(u_1\)</span>是基向量，因此<span class="math inline">\(u_1^{\top} u_1 = 1\)</span>。那么我们可以建立拉格朗日函数： <span class="math display">\[
f(u_1) = u_1^{\top} S u_1 + \lambda(1 - u_1^{\top} u_1)
\]</span> 这样我们就可以对<span class="math inline">\(u_1\)</span>求导数： <span class="math display">\[
\frac{\partial f}{\partial u_1} = 2S u_1 - 2\lambda u_1
\]</span> 另上式等0，我们就可以得到<span class="math inline">\(S u_1 = \lambda u_1\)</span>。更熟悉的来了，这就是矩阵的特征根公式嘛，所以我们可以知道，我们在找的就是原来矩阵的协方差矩阵的特征根，所以很自然的，当我们选择特征值最大的那个特征根的时候，我们的目标值就最大。这样我们就求到了第一个最大的主成分。</p>
<p>那如果我们现在想要做的事情是做多维的PCA的话，那么我们得到第一个主成分，现在要求第二个主成分需要注意什么呢？首先我们希望的事情是：</p>
<ul>
<li><p>第二个主成分方向上的方差也是最大的</p></li>
<li><p>第二个主成分最好跟第一个主成分是垂直的，因为这样我们可以保证第二个主成分的信息和第一个主成分的信息之间没有重叠。换句话说，可以保证两个成分之间是完全无关的。</p></li>
</ul>
<p>那为了达到这样的目的，我们会发现，如果我们让两个成分相互垂直，那么我们其实很容易就会发现，其实这第二个成分也有可能是协方差矩阵的特征向量，而且就是次大的特征值对应的特征向量。当然这种想法是一种想当然的做法，最保险的方式还是用拉格朗日乘子去求解。那第二个主成分的拉格朗日方程就可以协作是<span class="math inline">\(f(u_2) = u_2^{\top} S u_2 + \alpha(1-u_2^{\top}u_2) + \beta(0-u_2^{\top}u_1)\)</span>，然后求偏导一步步得到结果。</p>
<p>那么PCA还剩下最后一个没有解决的问题就是实操的时候，我们要选择留下多少个component呢？这里一般用的方法就是去计算<span class="math inline">\(\frac{\lambda_j}{\sum_i^n \lambda_i}\)</span>，然后我们自己觉得大概累计百分比到什么程度我们可以接受，然后就可以保留多少component。</p>
<h1 id="矩阵分解">矩阵分解</h1>
<p>那用PCA我们说是在抽取某种pattern，或者说latent factor。但是实际上因为PCA的系数可以正可以负，所以PCA实际上并不能真正抽取到pattern。举个例子来说，如果我们对面部图像进行PCA分解，我们会得到的是如下的图：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml067.png></p>
<p>那我们会发现，其实我们没有找到所谓的component，我们反而得到的是各种各样的脸。那其实这是很容易理解的，因为<span class="math inline">\(x = a_1 w_1 + a_2 w_2 + \dots\)</span>，那每一个系数可以是正的，也可以是负的，所以我们得到的每一个principle component其实可以是各种复杂的元素相互组合起来的结果。比如一个component是有胡子，然后系数是负的，刚好就把胡子减掉，然后就还原回原来的脸了。</p>
<p>那么我们为了解决这个问题，真正做到抽component，我们可以强制要求，系数是正的，同时每一个分解的component的元素也都是正的。也就是做NMF，就是non-negative matrix factorization。那我们就可以得到下面的图：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml068.png></p>
<p>这样我们就会发现，比如有抽出眉毛，下巴之类的。</p>
<p>那matrix factorization有很多种抽法，最常见的就是SVD和NMF。那矩阵分解其实就是在抽取某种latent factor。</p>
<p>比如说下图：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml069.png></p>
<p>我们手上有一个矩阵，就是每个死宅买的手办个数。那我们就可以用这样的方法将这个大矩阵拆解为两个相乘的矩阵，而这两个矩阵的<span class="math inline">\(K\)</span>实际上就是latent factor。这里其实课件里有个错误，约等号右边的第一个矩阵应该是M行。</p>
<p>那么我们要怎么解这个公式呢？嗯，一般书上解SVD的方法我表示线代太差，先留着后面再来刚。所以换一条路，我们想要做的事情就让分解后乘回去的矩阵跟原来的矩阵几乎一样，所以我们其实就可以用<span class="math inline">\(L_2\)</span> norm来做梯度下降。也就是求<span class="math inline">\(\arg \min \sum_{(i, j)} (r^i \cdot r^j - a_{ij})^2\)</span>。那其实这样把每个元素都加起来，然后用梯度下降来求解就好了。所以看到这里，突然一想，上面PCA其实也可以用梯度下降来做吼。</p>
<p>那上面的做法是只考虑了内在的共同属性，其实有时候某些人就是喜欢买手办，或者说某些手办就是卖得好之类的。那其实这里面就是说可能存在bias。所以我们可以将bias引入，那上面的公式就变成是<span class="math inline">\(\arg \min \sum_{(i, j)} (r^i \cdot r^j + bias_i + bias_j - a_{ij})^2\)</span>。那其实这就是SVD++算法。既然可以加bias，那就意味着可以加regularization。</p>
<p>那么矩阵分解其实有很多很多的应用。在NLP里面，如果做无监督文档分类会用到一种非常简单的方法叫LSA(latent semantic analysis)，那其实就是前面的矩阵分解，只是换了个名字而已。另外现在很流行的topic model用的是LDA，全称是latent dirichlet allocation，其实也是LSA的一种变化。跟线性判别LDA（linear discriminant analysis）是两码事情。不过线性判别需要labeled data。</p>
<p>其他还有很多很多的降维方法，基本上都是各种线性变换。嗯，线代少不了。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>台大李宏毅机器学习-迁移学习</title>
    <url>/machine_learning_step15/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>迁移学习的一些介绍</p>
<span id="more"></span>
<p>迁移学习其实就是当我们要做的target数据量不够多的时候，我们就可以用其他相关的target来训练我们的机器做这个target。</p>
<p>迁移学习分类可以分为下面四种：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml095.png></p>
<p>那实际上，一般来说，迁移学习是用源数据和源目标来学习目标域的函数。这fine-tuning说起都不能算是transfer learning。不过不同分类影响不大。</p>
<p>fine-tuning其实是一种非常简单的做法，比如说我们现在要做一个分辨马和驴的分类器，同时我们手上有一个训练好的猫和狗的分类器，那么我们就可以用猫和狗的分类器学会的隐藏层来微调学习分辨驴和马。</p>
<p>那做这个叫conservation learning，实现上就是将训练好的source network里面的参数来做初始化，然后用target data训练整个网络。但是这里的要求是，新训练出来的network跟原来source的network不要差太多。不要差太多的意思就是parameter不要差太多，另外同样的input，output也不要差太多。某种程度上，旧的network是新的network的regularization。</p>
<p>那实践上就是做layer transfer。当target数据很少的时候，就将所有的参数都迁移过来，然后固定住一部分的参数不变，重新训练没有被固定住的隐藏层。如果数据量够大，也可以学所有的layer。</p>
<p>那不同的场景做layer transfer也不太一样。在语音识别上，一般是copy最后几层layer，因为最后输出的文字都是类似的。在图像上一般是copy前几层layer，因为卷积一开始就是去找某种隐藏的模式，不同的图片都可以找类似的模式。这一点可以参考一下之前的卷积网络，可以看到前面几层卷积就是找色块啊，形状什么的。</p>
<p>那实际上在图像上的训练效果：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml096.png></p>
<p>还有一种transfer是做multitask learning。那multitask learning在实践上，有可能是同样的数据做不同的task，也可能是不同的数据做不同的task。网络设计大概就是：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml097.png></p>
<p>在机器翻译上，如果用左边的那个网络架构，同时混杂各种语言，然后做多个task，实验证明比单个语料的模型效果要好。</p>
<p>还有一种做法是progressive neural network，这个看上去很像RNN。</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml098.png></p>
<p>这个网络就是将之前的task的output作为下一个task的input来一起训练，这样可以避免每个task训练的时候完全遗忘掉之前学习的结果。</p>
<p>上面的source data和target data都是labeled的数据。那如果现在target data是unlabeled的，有两种学习方法，一种是domain adversarial learning，domain adversarial在设计的时候有两个目标，一个目标是构造一个feature extractor使得一个domain classifier无法分辨这个数据是来自target还是source，这样就能把二者的feature做到接近。另外一个目标就是同事要保证抽取出来的feature可以正确分类source data。做这个限制的原因是，如果不做这个限制，那么feature extractor把所有的feature全抽成0，那么domain classifier就完全无法区分了。</p>
<p>所以结构上设计为：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml099.png></p>
<p>在训练的时候就是：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml100.png></p>
<p>另一种学习方法是zero-shot learning。跟domain adversarial learning的区别就是，zero-shot learning的task是不一样的，而domain adversarial learning的task是一致的，所以后者会希望将feature做到一起去。</p>
<p>那么zero-shot learning要如何实现呢？因为我们要做的目标是不一致的，那我们就需要让机器学习到最基本的单位。比如说我们有一个区分猫狗的模型，想做区分驴和马的模型，我们希望机器学到的是动物有没有尾巴，有没有毛茸茸的，有没有爪子这样。</p>
<p>但是这样做的前提是，我们要有一个这样的特征库，也就是说，我们要有一个数据库记录的是猫是不是有尾巴，是不是毛茸茸，是不是有爪子，并将这个作为训练目标。那如果我们连这个数据库都没有，怎么办呢？这里有一个非常trick的做法，我们可以用word2vec生成的向量来作为y训练。</p>
<p>那另外一个zero-shot learning的方法是把所有数据的feature都embedding到一个空间中：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml102.png></p>
<p>这里希望学到的就是图片直接抽取的特征<span class="math inline">\(f(*)\)</span>和通过attribute学习到的vector <span class="math inline">\(g(*)\)</span>越接近越好。同样的，如果没有attribute，我们就用word2vec来替代attribute。</p>
<p>那这里又有一个问题，如果直接计算两个vector的点内积这样训练的话，<span class="math inline">\(f(*)\)</span>和<span class="math inline">\(g(*)\)</span>都变成0就完全一样了。所以我们需要加一点限制：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml103.png></p>
<p>那我们这里加的限制是，同一个类别的两个vector越近越好，两个不同类别的vector越远越好。这样就可以保证所有的vector不会都堆到0这个地方，另外也不会导致几个类别离得太远。</p>
<p>那其实也可以用现成的模型，然后将新的数据扔进去做分类。这个方法叫• Convex Combination of Semantic Embedding。如下：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml104.png></p>
<p>整个做法就是这边把图片放进来，然后预测出来一半是狮子，一半是老虎，然后把老虎跟狮子的词向量做加权平均，找一个离加权平均值最近的向量就是最终的label。这个实验可以看这篇文章<a href="https://arxiv.org/pdf/1312.5650v3.pdf">https://arxiv.org/pdf/1312.5650v3.pdf</a>。</p>
<p>最后的一开始的那个表格内剩下的两个格子没有细讲，都是一笔带过，这里也就不多说啥了。感觉那两个才是真的transfer learning，毕竟还有戴文渊的文章。后面好好研读吧。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>台大李宏毅机器学习——RNN</title>
    <url>/machine_learning_step16/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>RNN是一种比较复杂的网络结构，每一个layer还会利用上一个layer的一些信息。</p>
<span id="more"></span>
<p>比如说，我们要做slot filling的task。我们有两个句子，一个是“arrive Taipei on November 2nd”，另一个是“leave Taipei on November 2nd”。我们可以发现在第一个句子中，Taipei是destination，而第二个句子中Taipei是departure。如果我们不去考虑Taipei前一个词的话，Taipei的vector只有一个，那么同样的vector进来吐出的predict就是一致的。所以我们在做的时候就需要把前一个的结果存起来，在下一个词进来的时候用了参考。</p>
<p>所以这样我们就在neuron中设计一个大脑来存储这个值。所以网络长这样：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml105.png></p>
<p>我们用这个网络来举例。假设我们每个weight都是1，每个activation都是linear的。那么我们现在有一个序列<span class="math inline">\((1, 1)，(1, 1)，(2, 2)\)</span>，那么一开始memory里面的值是<span class="math inline">\((a_1 = 0, a_2 = 0)\)</span>，现在将序列第一个值传入network，我们得到<span class="math inline">\((x_1 = 1, x_2 = 1)\)</span>，因为active function都是linear的，所以经过第一个hidden layer，我们输出的就是<span class="math inline">\(1 \times a_1 + 1 \times a_2 + 1 \times x_1 + 1 \times x_2\)</span>，两个节点一致。所以第一个hidden layer得到<span class="math inline">\((2, 2)\)</span>，同时我们将<span class="math inline">\((2， 2)\)</span>保存起来，更新一下得到<span class="math inline">\((a_1 = 2, a_2 = 2)\)</span>，output layer是<span class="math inline">\((4, 4)\)</span>，所以得到第一个<span class="math inline">\((y_1=4, y_2=4)\)</span>。第二个input <span class="math inline">\((1, 1)\)</span>，同样计算一下，hidden layer得到的是<span class="math inline">\((6, 6)\)</span>和<span class="math inline">\((a_1 = 6, a_2 = 6)\)</span>，output layer是<span class="math inline">\((12, 12)\)</span>。同理第三个input最后得到的output是<span class="math inline">\((32, 32)\)</span>，所最后得到的三个output序列是<span class="math inline">\((4, 4), (12, 12), (32, 32)\)</span>。</p>
<p>那么我们可以想一下，如果现在的序列顺序变化一下，结果是否会不一致？如果现在的序列是<span class="math inline">\((1, 1), (2, 2), (1, 1)\)</span>，我们得到的是<span class="math inline">\((4, 4), (16, 16), (36, 36)\)</span>。结果发生了变化。所以RNN对序列是敏感的，这样的特性就表示，在slot filling的task里面，我们前面的arrive和leave将会影响后面接着的Taipei的结果。</p>
<p>当然RNN也可以是深度的，设计上就是</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml106.png></p>
<p>这种深度的设计有两种方法</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml107.png></p>
<p>传说Jordan Network一般效果会比较好。另外RNN也可以双向训练：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml108.png></p>
<p>这里也需要注意一下，RNN不是训练好多个NN，而是一个NN用好多遍。所以可以看到RNN里面的这些network的参数都是一致的。</p>
<p>那这个是一个非常原始简单的RNN，每一个输入都会被memory记住。现在RNN的标准做法基本上已经是LSTM。LSTM是一个更加复杂的设计，最简单的设计，每一个neuron都有四个输入，而一般的NN只有一个输入。</p>
<p>LSTM的一个简单结构长这样：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml109.png></p>
<p>可以看到，一个output受到三个gate的影响，首先是input gate决定一个input是否可以进入memory cell，forget gate决定是否要忘记之前的memory，而output gate决定最后是否可以输出。这样一个非常复杂的neuron。</p>
<p>那么实作上这个neuron是如何工作的呢？假设我们现在有一个最简单的LSTM，每个gate的input都是一样的vector，那么我们这边在做的时候就是每一个input乘以每个gate的matrix，然后通过active function进行计算。这里做一个最简单的人肉LSTM。假设我们有一个序列是：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml110.png></p>
<p>我们希望，当<span class="math inline">\(x_2 = 1\)</span>的时候，我们将<span class="math inline">\(x_1\)</span>加入memory中，当<span class="math inline">\(x_2 = -1\)</span>的时候，memory重置为0，当<span class="math inline">\(x_3 = 1\)</span>的时候，我们输出结果。</p>
<p>那么我们再假设一个很简单的LSTM，长这样：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml111.png></p>
<p>这个cell的input activate function是linear的，memory cell的activate function也是linear的。</p>
<p>那么我们可以将上面的序列简化一下</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml112.png></p>
<p>现在，将第一个元素放进来，我们得到是3，input gate部分的结果是90，经过activate function得到的是1，所以允许通过进入memory cell。forget gate这里计算的结果是110，经过activate function是1，所以我们记住这个值（这里要注意，虽然这个gate叫forget gate，但是当取值是1的时候其实是记住，0的时候是遗忘）。然后到output gate这里，output gate计算是-10，activate function输出是0，所以我们不output结果。</p>
<p>输入下一个元素。直接输入计算是4，经过input gate，得到的是4。因为原来memory cell里面已经存了3，所以这一轮的计算是原来的memory加上新进入的4，得到7。然后output gate依然关闭，所以memory cell还是存7。</p>
<p>第三个元素类似的计算，发现input gate关闭，所以没法进入memory cell，因此memory cell没有更新。同时output gate关闭，没有输出。</p>
<p>第四个元素进入，input gate关闭，memory cell不更新，但是这时候output gate的activate function得到1，所以开放输出结果。因为之前memory cell里面存放的是7，所以输出7。但是要注意一点，虽然memory cell的值输出了，里面的值并没有被清空，仍然保留着，所以这个时候的memory cell还是7。</p>
<p>最后一个元素进入，input gate关闭，memory cell不更新，这时候，forget gate的activate function得到的是0，所以我们清空记忆，memory cell里面现在是0。output gate仍然关闭，所以没有output。</p>
<p>上面五个过程用图表示如下：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml113.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml114.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml115.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml116.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml117.png></p>
<p>那实作的时候，一个简化的LSTM是：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml118.png></p>
<p>如图中，我们输入一个原始的<span class="math inline">\(x^t\)</span>，会通过四个linear transform变成四个vector，然后每个vector输入到对应的gate。这里要注意的是，转换后的<span class="math inline">\(z\)</span>有多少个维度，那么我们就需要建立多少个LSTM的cell，同时，每次进入cell训练的只是<span class="math inline">\(z\)</span>的一个维度。</p>
<p>这是实作上的运算过程：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml119.png></p>
<p>这里的乘不是inner product，是elementwise product。</p>
<p>上面是最simple的LSTM，实际的LSTM如下图：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml120.png></p>
<p>嗯，天书。不过这是现在LSTM的标准做法。</p>
<p>RNN很难训练，因为有个问题就是可能梯度爆炸，有可能梯度消失。我们用一个最简单的模型来体验一下这个问题。假设我们现在的模型是非常simple的RNN，activate function是linear的，如下图：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml121.png></p>
<p>我们可以发现，如果我们有1000个cell，那么我们的weight从1 update到1.01的时候，到了第一千个输出就从原来的1变成了2w，梯度爆炸了。但是当我们weight从1 update到0.99，那么到了第一千个输出就变成了0。更极端一点，如果我们因为之前选择了一个很大的lr，那么我们一步就把weight调到了0.01，我们发现，第一千个输出还是0。所以一般的RNN难以训练是有两个问题的，一个是梯度爆炸，一个是梯度消失。</p>
<p>那么LSTM在实现的时候，因为有了forget gate的存在，只要forget gate长期保持开启，那么很久以前的数据会持续影响后面的数据，所以可以抹消掉梯度消失的问题。另外memory cell里面的值是加起来而不是simple RNN里面直接抹消的，所以这也是能解决梯度消失的问题。</p>
<p>LSTM有个简化加强版叫GRU这个在李老师另一门课有讲，先把坑留着。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>台大李宏毅机器学习——SVM</title>
    <url>/machine_learning_step18/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>机器学习的算法太多了，但是很多算法之间的差异实际上不大。SVM是曾经风靡一时，一度虐杀神经网络的算法，不过DL出现以后，没落了不少啊。真是三十年河东三十年河西。</p>
<span id="more"></span>
<p>这里简单学习一下SVM。SVM跟其他的机器学习算法的差别在于，第一，loss function不一样，第二，有一个kernel function。其中的kernel function是大杀器。</p>
<p>现在我们假想我们要学的是一个二分类的问题，我们回顾一下几种损失函数。最开始的时候，我们学的是平方误差，也就是MSE，在二分类问题上，我们想要达到的效果就是当<span class="math inline">\(y=1\)</span>的时候，<span class="math inline">\(f(x)\)</span>越接近1越好，同理，<span class="math inline">\(y=-1\)</span>的时候，<span class="math inline">\(f(x)\)</span>越接近-1越好。那么基于square error，我们可以得到： <span class="math display">\[
\begin{cases}
(f(x) - 1)^2 &amp; \mbox{if } y = 1 \\
(f(x) + 1)^2 &amp; \mbox{if } y = -1
\end{cases}
\]</span> 那其实上面两个式子可以统一成<span class="math inline">\((yf(x) - 1)^2\)</span>。</p>
<p>那逻辑回归的loss function是cross entropy，那实际上也可以写作是<span class="math inline">\(\ln(1+\exp(-yf(x)))\)</span>。之前的adaboost的loss function我们也可以用exponential loss来表示，也就是<span class="math inline">\(\exp(-yf(x))\)</span>，那么我们将这几种loss function跟最简单的<span class="math inline">\(\delta(f(x) \ne y)\)</span>对比一下：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml125.png></p>
<p>那我们回过头看一下，如果是用<span class="math inline">\(\delta\)</span>函数，那么我们根本就没有办法求梯度。而如果我们用square loss，我们可以发现，左右两边的loss都很大，但是实际上，当<span class="math inline">\(f(x)\)</span>跟<span class="math inline">\(y\)</span>的方向一致，两个乘积非常大，本质上loss应该趋近于0才对，所以square loss不是非常合适。那另外的logistics loss跟exponential loss，都可以做到越远越好。另外这里需要注意一下，为了让logistics loss能够成为<span class="math inline">\(\delta\)</span>函数的upper bound，我们会将原来的公式除以<span class="math inline">\(\ln 2\)</span>。</p>
<p>那么SVM呢？SVM用的是hinge loss，hinge loss表示为：<span class="math inline">\(\max(0, 1-yf(x))\)</span>。那hinge loss的一个好处就是，当<span class="math inline">\(f(x)\)</span>跟<span class="math inline">\(y\)</span>一致，就可以了，那么hinge loss的好处就是，hinge loss对异常值比较不敏感，差不多就好了，而cross entropy跟exponential loss都会拼命去拟合异常值。</p>
<p>那linear SVM实际上就是logistics regression把loss function换成hinge loss。所以假设也是： <span class="math display">\[
f(x) = \sum_i w_i x_i + b
\]</span> 而loss function就是： <span class="math display">\[
L(f) = \sum_n(l(f(x_n), y_n)) + \lambda \|w_i\|_2
\]</span> 那么，实际上，如果我们神经元用了一个linear SVM，一样也是可以做deep learning的。所以，嗯，别整天BB自己在做DL，好好学基础再BB。</p>
<p>那么SVM的loss function不是处处可微分的，有没有可能做梯度下降呢。实际上，DL中的ReLU函数都可以，所以SVM实际上也是可以做的。那求导的过程是这样的： <span class="math display">\[
\frac{\partial l(f(x_n), y_n)}{\partial wi} = \frac{\partial l(f(x_n), y_n)}{\partial f(x_n)} \frac{\partial f(x_n)}{\partial w_i}
\]</span> 那其实<span class="math inline">\(\frac{\partial f(x_n)}{\partial w_i}\)</span>就是<span class="math inline">\(x_n^i\)</span>。</p>
<p>那<span class="math inline">\(\frac{\partial l(f(x_n), y_n)}{\partial f(x_n)}\)</span>怎么计算呢，这是一个分段函数，所以分段求导： <span class="math display">\[
\frac{\partial \max(0, 1 - y_n f(x_n))}{\partial f(x_n)} =
\begin{cases}
-y_n &amp; \mbox{if } y_n f(x_n) &lt; 1 \\
0 &amp; \mbox{if } y_n f(x_n) \ge 1
\end{cases}
\]</span> 所以实际上linear SVM的梯度就是： <span class="math display">\[
\frac{\partial L(f)}{\partial w_i} = \sum_n -\delta(y_n f(x_n) &lt; 1) y_n x_n^i
\]</span></p>
<p>现在我们回过头看一下，因为用梯度下降，所以我们可以用一个linear combination来表示<span class="math inline">\(w\)</span>，也就是表示为：<span class="math inline">\(w^* = \sum_n \alpha_n^* x_n\)</span>。那实际上可以这样表示是因为： <span class="math display">\[
w_i^t = w_i^{t-1} - \eta \sum_n -\delta(y_n f(x_n) &lt; 1) y_n x_n^i
\]</span> 这个式子如果将<span class="math inline">\(w\)</span>串成一个vector来看，那么就是 <span class="math display">\[
w^t = w^{t-1} - \eta \sum_n -\delta(y_n f(x_n) &lt; 1) y_n x_n
\]</span> 那实际上这个迭代到最后，<span class="math inline">\(w\)</span>就是<span class="math inline">\(x\)</span>的一个linear combination。这样一来，我们就可以把<span class="math inline">\(w\)</span>直接表示为<span class="math inline">\(w = \boldsymbol{X \alpha}\)</span>。那我们因为用的是hinge loss，所以正确分类的sample就不会再提供梯度，所以<span class="math inline">\(\boldsymbol{\alpha}\)</span>是一个sparse的向量，这个向量我们就叫做support vector。这里要注意一点，如果没有做特殊说明，所有的vector这里都是表示列向量，所以这边的<span class="math inline">\(\boldsymbol{X}\)</span>的行表示feature，列表示sample。</p>
<p>现在因为我们的模型是linear的，所以<span class="math inline">\(f(x) = w^{\top} x = \boldsymbol{\alpha}^{\top} \boldsymbol{X}^{\top} x = \sum_n \alpha_n K(x_n, x)\)</span>，这样我们就把kernel function带出来了。</p>
<p>kernel function的好处就是，非常方便快速可以做到feature transform。比如说，我们的kernel用的是polynomial，那么我们就是将<span class="math inline">\(x\)</span>投影到<span class="math inline">\(\phi(x)\)</span>。比如说我们要做的一个polynomial是将<span class="math inline">\(\begin{bmatrix} x_1 \\ x_2 \end{bmatrix}\)</span>变成<span class="math inline">\(\begin{bmatrix} x_1^2 \\ \sqrt{2} x_1 x_2 \\ x_2^2 \end{bmatrix}\)</span>，那么不用kernel function的话，我们需要先做feature transform，把所有的feature变成这样，然后计算。现在因为有了kernel function，所以我们要做的事情就很简单，我们只要做<span class="math inline">\(\phi(x)\)</span>跟对应的<span class="math inline">\(\phi(z)\)</span>的inner product就可以了。也就是： <span class="math display">\[
\begin{align}
K(x, z) &amp;= \phi(x) \cdot \phi(z) = \begin{bmatrix} x_1^2 \\ \sqrt{2} x_1 x_2 \\ x_2^2 \end{bmatrix} \cdot \begin{bmatrix} z_1^2 \\ \sqrt{2} z_1 z_2 \\ z_2^2 \end{bmatrix} \\
&amp;= x_1^2 z_1^2 + 2 x_1 x_2 z_1 z_2 + x_2^2 z_2^2 \\
&amp;= (x_1 z_1 + x_2 z_2)^2 \\
&amp;= (x \cdot z)^2
\end{align}
\]</span> 所以有了这个kernel function，我们就可以很快速做到feature transform。那上面这个变化就是做了一个二阶的多项式变化。</p>
<p>那如果我们用的是radial的kernel，实际上我们就做到了无穷多阶的polynomial。怎么说呢？radial的kernel做的事情是<span class="math inline">\(K(x, z) = \exp(-\frac{1}{2} \|x - z\|_2)\)</span>，我们化简一下这个公式： <span class="math display">\[
\begin{align}
K(x, z) &amp;= \exp(-\frac{1}{2} \|x - z\|_2) \\
&amp;= \exp(-\frac{1}{2} \|x\|_2 -\frac{1}{2} \|z\|_2 + x \cdot z) \\
&amp;= \exp(-\frac{1}{2} \|x\|_2) \exp(-\frac{1}{2} \|z\|_2) \exp(x \cdot z) \\
&amp;= C_x C_z \exp(x \cdot z)
\end{align}
\]</span> 现在开始表演了，我们得到<span class="math inline">\(\exp(x \cdot z)\)</span>，根据泰勒展开，我们得到的是<span class="math inline">\(\sum_0^{\infty} \frac{(x \cdot z)^i}{i!}\)</span>，这就是一个无穷多维的多项式了，也就意味着，我们将原来的feature映射到了无穷多维的空间中去，而不需要提前做feature transform。不过这里的问题就是，因为维度太高了，一来运算慢，二来很可能过拟合。</p>
<p>最后介绍一个很常见的kernel，就是sigmoid kernel。sigmoid kernel的公式是<span class="math inline">\(K(x, z) = tanh(x \cdot z)\)</span>，那在实作的时候，其实我们的公式是这样的<span class="math inline">\(K(x_n, x)\)</span>，所以我们可以将这个过程看作是一个单层的神经网络，结构如下：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml126.png></p>
<p>也就是说将每个<span class="math inline">\(x\)</span>的各个维度的值当做为weight，所以有多少样本就有多少neuron。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>台大李宏毅机器学习——集成算法</title>
    <url>/machine_learning_step17/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>集成算法基本上现在已经成为机器学习的标准套路，单一模型的几乎已经销声匿迹。集成算法有两大分类，一类是bagging，一类是boosting。</p>
<span id="more"></span>
<h1 id="bagging">bagging</h1>
<p>bagging是一种比较简单的集成策略，做法就是原来有<span class="math inline">\(N\)</span>个sample，现在sample出<span class="math inline">\(N&#39;\)</span>个sample，重复这样的动作多次，就可以得到很多个模型，然后如果是regression就做average，如果是classification就做voting。所以这样的策略是非常非常简单的。那这么做的目的其实是为了降低复杂模型的variance。这个可以回过头看之前的内容。所以bagging并不会解决overfitting，也不会起到什么加强模型预测能力的效果。只能说，用bagging的方法，模型会比较平滑。</p>
<p>那什么模型非常复杂容易overfitting呢？其实决策树是最容易overfitting的算法，NN反而没有那么容易overfitting，只是说NN建的模型多，variance比较大一点而已。</p>
<p>这边有个<a href="http://speech.ee.ntu.edu.tw/~tlkagk/courses/MLDS_2015_2/theano/miku">实验数据</a>，这个数据是miku的一个黑白图，如果正确分类就可以画出miku。</p>
<p>如果我们用一个单一模型，我们得到的是：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml122.png></p>
<p>当树深度一点点增加到20层的时候，就可以完美画出miku。</p>
<p>那如果用bagging，也就是random forest的方法，我们得到的是：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml123.png></p>
<p>我们可以跟上面的结果做一个对比，可以发现，单一的树画出来的miku没有那么平滑。用random forest画出来的结果相对比较平滑。</p>
<h1 id="boosting">boosting</h1>
<p>boosting是比bagging要强大的一种策略，bagging只是平滑复杂函数，而boosting是将大量的弱分类器集成为一个强分类器。adaboost是boosting算法里面的典型。</p>
<p>adaboosting的策略是，首先建一个分类器<span class="math inline">\(f_1(x)\)</span>，然后根据<span class="math inline">\(f_1(x)\)</span>的分类结果，错误分类的样本权重变大，正确分类的样本权重调低，调整到正确率差不多50%，然后这样re-weighted的结果拿来训练第二个分类器<span class="math inline">\(f_2(x)\)</span>。这样一直重复多次，将这些弱分类器都combine起来就是最终的强分类器。这里要跟bagging对比一下，boosting是没有resample数据的，只是改变了weight。那么现在的大杀器xgboost和lightgbm其实都是站着前人的基础上，继承了boosting和bagging的特性，也可以在boosting的时候做bagging的事情。</p>
<p>下图是我们做boosting的一个示意：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml124.png></p>
<p>我们一开始所有样本的weight都是一致的，训练了一个分类器，错误率<span class="math inline">\(\varepsilon_1 = 0.25\)</span>，我们重新修改weight，把错误分类的weight改到<span class="math inline">\(\sqrt{3}\)</span>，正确分类的修改到<span class="math inline">\(\frac{1}{\sqrt{3}}\)</span>，我们就可以把错误率调到<span class="math inline">\(\varepsilon_1&#39; = 0.5\)</span>。然后我们用这个新的weight来训练第二个模型，如此循环往复。</p>
<p>数学上来看这件事情是这样的，原本我们的loss function是<span class="math inline">\(\frac{\delta(f_1(x) \ne \hat{y}^n)}{n}\)</span>，加权的loss function是<span class="math inline">\(\frac{\sum_n u_n^1 \delta(f_1(x_n) \ne y_n)}{\sum_n u_n}\)</span>。这里做了一个归一化，因为所有的weight加起来未必等于1。然后要做的事情就是调整<span class="math inline">\(u_i^1\)</span>使得<span class="math inline">\(u_i^2\)</span>能让<span class="math inline">\(f_1\)</span>失效。不断的迭代其实就是把<span class="math inline">\(u_i^{t-1}\)</span>调整到<span class="math inline">\(u_i^{t}\)</span>，使得上一个模型失效。那实际上就是当<span class="math inline">\(t\)</span>轮模型进行预测，分类正确，weight除以<span class="math inline">\(d^t\)</span>，分类错误weight乘以<span class="math inline">\(d^t\)</span>。</p>
<p>那么每一步的<span class="math inline">\(d\)</span>应该是多少呢？因为我们要保证这个数可以让函数的误分类率刚好被调整到0.5左右。我们可以推导一下，其实非常简单。首先科普一下<span class="math inline">\(\delta\)</span>函数，这里的<span class="math inline">\(\delta (f(x), y)\)</span>表示的是，当<span class="math inline">\(f(x) = y\)</span>时为0，不等为1。所以我们可以知道<span class="math inline">\(\varepsilon_1到\varepsilon_2\)</span>的过程中，分类正确的<span class="math inline">\(u_i\)</span>全变成<span class="math inline">\(u_i^1 / d^1\)</span>，错误的变成<span class="math inline">\(u_i^1 \times d^1\)</span>。所以原来的结果是： <span class="math display">\[
\varepsilon_1 = \frac{\sum_n u_n^1 \delta(f_1(x_n) \ne y_n)}{\sum_n u_n^1}
\]</span> 现在的结果是： <span class="math display">\[
\varepsilon_2 = \frac{\sum_n u_n^2 \delta(f_2(x_n) \ne y_n)}{\sum_n u_n^2}
\]</span> 那么其中的<span class="math inline">\(\sum_n u_n^2 = \sum_{f(x) = y} u_n^1 / d^1 + \sum_{f(x) \ne y} u_n^1 \times d^1\)</span>。而分子部分就等于<span class="math inline">\(\sum_{f(x) \ne y} u_n^1 \times d^1\)</span>。现在我们要<span class="math inline">\(\varepsilon_2 = 0.5\)</span>，可以知道，就是让<span class="math inline">\(\sum_{f(x) \ne y} u_n^1 \times d^1 = \sum_{f(x) = y} u_n^1 / d^1\)</span>。因为<span class="math inline">\(d^1\)</span>是常数，可以提取出来，然后<span class="math inline">\(\sum_{f(x) = y} u_n^1 = \sum_n u^1_n (1-\varepsilon_1)，\sum_{f(x) \ne y} u_n^1 = \sum_n u^1_n \varepsilon_1\)</span>。刚好<span class="math inline">\(\sum_n u^1_n\)</span>又是常数，再消掉，我们可以轻松得到<span class="math inline">\(d^1 = \sqrt{\frac{1-\varepsilon_1}{\varepsilon_1}}\)</span>。这里我们需要做乘法和除法，虽然对程序而言问题不大，但是公式上不是那么好看。我们可以将这个系数改成<span class="math inline">\(a^t = \ln(d^t)\)</span>这样一来，我们就可以把公式改成 <span class="math display">\[
u^{t+1}_n = u^t \times \exp(-a^t) \text{ if } f(x) = y \\
u^{t+1}_n = u^t \times \exp(a^t) \text{ if } f(x) \ne y
\]</span> 然后我们又发现，如果我们做二分类的问题，我们可以将<span class="math inline">\(y\)</span>的取值改为<span class="math inline">\(\pm 1\)</span>，这样一来，我们上面的公式就可以化简到一个非常舒服的样子： <span class="math display">\[
u^{t+1}_n = u^t \times \exp(- y f_t(x) a^t)
\]</span></p>
<p>那么adaboost基本上的工作原理就是这样。那么最后我们得到的分类函数就是之前所有弱分类器的集成版： <span class="math display">\[
H(x) = \text{sign}(\sum^T_t a^t f_t(x))
\]</span></p>
<p>现在的问题就是，adaboost为什么可以收敛呢？我们知道adaboost的error rate函数是 <span class="math display">\[
\frac{1}{N} \sum_n \delta(H(x_n) \ne y_n)
\]</span> 我们定义一个函数<span class="math inline">\(g(x) = \sum_{t=1}^T a^t f_t(x)\)</span>，那上面的式子实际上就是 <span class="math display">\[
\frac{1}{N} \sum_n \delta(y_n \times g(x_n) &lt; 0)
\]</span> 然后这里我们定一个exponential loss function，就是<span class="math inline">\(\exp(-y_n \times g(x_n))\)</span>。这里很直觉的，错误率函数是小于等于这个，所以我们可以得到： <span class="math display">\[
\frac{1}{N} \sum_n \delta(y_n \times g(x_n) &lt; 0) \le \frac{1}{N} \sum_n(\exp(-y_n \times g(x_n)))
\]</span> 实际上这个upper-bound是非常宽松的一个限制，只要让这个upper-bound收敛，那么我们的错误率就一定会收敛。</p>
<p>怎么做到呢？我们回过头看之前的数据，在更新<span class="math inline">\(u^t\)</span>的时候，我们用到了<span class="math inline">\(\exp(-y_n f_t(x_n))\)</span>，而<span class="math inline">\(g(x)\)</span>是<span class="math inline">\(f(x)\)</span>的最终加权平均的集成版，所以我们尝试将所有的<span class="math inline">\(u\)</span>加起来会怎么样？所有的<span class="math inline">\(u\)</span>加起来我们用<span class="math inline">\(Z\)</span>表示，因为<span class="math inline">\(u_1 = 1\)</span>，<span class="math inline">\(u_{t+1} = u_t \exp(-y f_t(x) a_t)\)</span>，这是一个等比数列，所以 <span class="math display">\[
u_{T+1} = \prod_{t=1}^T \exp(-y f_t(x) a_t)
\]</span> 所以 <span class="math display">\[
\begin{align}
Z_{T+1} &amp;= \sum_n \prod_{t=1}^T \exp(-y_n f_t(x_n) a_t) \\
&amp;= \sum_n \exp (-y_n \sum_{t=1}^T(f_t(x_n) a_t))
\end{align}
\]</span> 然后我们发现，尾巴部分的其实就是<span class="math inline">\(g(x)\)</span>。于是我们就把上面的upper-bound跟<span class="math inline">\(Z\)</span>统一了起来，得到： <span class="math display">\[
\frac{1}{N} \sum_n \delta(y_n \times g(x_n) &lt; 0) \le \frac{1}{N} \sum_n(\exp(-y_n \times g(x_n))) = \frac{1}{N} Z_{T+1}
\]</span> 然后要证明的就是<span class="math inline">\(Z_{T+1}\)</span>会越来越小。</p>
<p>因为 <span class="math display">\[
\begin{align}
Z_{t+1} &amp;= Z_{t} \varepsilon_t \exp(a_t) + Z_{t} (1 - \varepsilon_t) \exp(-a_t) \\ &amp;= Z_{t} \varepsilon_t \sqrt{\frac{1-\varepsilon_t}{\varepsilon_t}} + Z_{t} (1 - \varepsilon_t) \sqrt{\frac{\varepsilon_t}{1 - \varepsilon_t}} \\ &amp;= 2 \times Z_{t} \times \sqrt{\varepsilon_t(1-\varepsilon_t)}。
\end{align}
\]</span> 所以我们可以 得到<span class="math inline">\(Z_{T} = N \prod_{t=1}^T 2 \sqrt{\varepsilon_t(1-\varepsilon_t)}\)</span>。因为<span class="math inline">\(\varepsilon\)</span>只有刚好取到0.5的时候才会等于1，否则会一路收敛，越来越小。</p>
<p>然后我们可以看到gradient boosting这种方法。事实上，gradient boosting优化的方向不再是对样本，而是直接作用于function。如果我们现在接受一个function其实就是一个weight的vector，那么其实我们就是可以对function求偏导的。我们从梯度下降的角度来看这个问题，那么我们在做的事情就是 <span class="math display">\[
g_t(x) = g_{t-1}(x) - \eta \frac{\partial L}{\partial g(x)} |_{g(x) = g_{t-1}(x)}
\]</span> 但是换个角度，从boosting的角度来看，我们其实boosting的过程是每一次找一个<span class="math inline">\(f_t(x)\)</span>和<span class="math inline">\(a_t\)</span>，使得最终的模型<span class="math inline">\(g_t(x)\)</span>更好。这个过程就是： <span class="math display">\[
g_t(x) = g_{t-1}(x) + a_t f_t(x)
\]</span> 考虑到跟上梯度的过程，我们可以知道，其实我们希望梯度的方向跟我们boosting优化的方向最好能够是一样的。如果这里我们的loss function选择的是exponential loss，那么loss function就是<span class="math inline">\(\sum_n \exp(-y_n g(x_n))\)</span>， 梯度就是<span class="math inline">\(\sum_n \exp(-y_n g(x_n))(-y_n)\)</span>，刚好跟梯度前面的负号抵消掉。在这种情况下，如果要让二者的方向一样，我们可以用这样的公式来表示： <span class="math display">\[
\sum_n \exp(-y_n g_{t-1}(x_n)) y_n f_t(x)
\]</span> 当这个公式越大，表示二者的方向越一致。在adaboost中，<span class="math inline">\(\sum_n \exp(-y_n g_{t-1}(x_n))\)</span>这个刚好就是我们在<span class="math inline">\(t\)</span>轮得到的样本权重。</p>
<p>回到损失函数这里，我们的损失函数是： <span class="math display">\[
\begin{align}
L(g) &amp;= \sum_n \exp(-y_n g_t(x_n)) \\
&amp;= \sum_n \exp(-y_n (g_{t-1}(x_n) + a_t f_t(x_n))) \\
&amp;= \sum_n \exp(-y_n g_{t-1}(x_n)) \exp(-y_n a_t f_t(x_n)) \\
&amp;= \sum_{f_t(x) \ne y} \exp(-y_n g_{t-1}(x_n)) \exp(a_t) + \sum_{f_t(x) = y} \exp(-y_n g_{t-1}(x_n)) \exp(-a_t)
\end{align}
\]</span> 我们希望得到的是<span class="math inline">\(\frac{\partial L}{\partial a_t} = 0\)</span>，因为 <span class="math display">\[
\begin{align}
L &amp;= \sum_n \exp(-y_n g_t(x_n)) \\
&amp;= Z_{t+1} \\
&amp;= Z_t \varepsilon_t \exp(a_t) + Z_t (1-\varepsilon_t) \exp(-a_t)
\end{align}
\]</span> 前面的系数<span class="math inline">\(Z_t\)</span>跟<span class="math inline">\(a_t\)</span>没关系直接消掉，然后求导数我们得到的就是： <span class="math display">\[
\frac{\partial L}{\partial a_t} = \varepsilon_t \exp(a_t) - (1-\varepsilon_t) \exp(-a_t) = 0
\]</span> 这样我们就可以求出来<span class="math inline">\(a_t = \ln \sqrt{\frac{1-\varepsilon_t}{\varepsilon_t}}\)</span>刚好就是adaboost。</p>
<p>实际上gradient boosting是可以改变loss function的，adaboost就是一个特殊的gradient boosting。台大另一个老师，林轩田的课程里面是有更general的介绍。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>台大李宏毅机器学习——bias&amp;variance</title>
    <url>/machine_learning_step2/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>现在我们来看看误差从何而来</p>
<span id="more"></span>
<p>模型误差来源于 <strong>bias</strong> and <strong>variance</strong>.</p>
<p>假设有一组数据 <span class="math inline">\(x\)</span>, 均值是 <span class="math inline">\(\mu\)</span>，方差是 <span class="math inline">\(\sigma^2\)</span>。</p>
<p>我们现在估计均值。抽样 N 个样本 <span class="math inline">\(\{x^1, x^2, \dots , x^N \}\)</span>， 那么我们的均值就是<span class="math inline">\(m = \frac{1}{N} \sum_n x^n \ne \mu\)</span>。也就是说，一般而言，我们抽样得到的均值并不会严格等于数学期望。但是 <span class="math inline">\(E(m) = E\Big(\frac{1}{N} \sum_n x^n \Big) = \frac{1}{N} \sum_n E(x^n) = \mu\)</span> 且 <span class="math inline">\(Var(m) = \frac{\sigma^2}{N}\)</span>。这说明，当我们抽样的数据越多，我们的样本均值越接近总体的期望，bias越小。</p>
<p>现在估计方差。同样抽 N 个样本， <span class="math inline">\(\{x^1, x^2, \dots , x^N \}\)</span>。样本方差<span class="math inline">\(s^2 = \frac{1}{N} \sum_n(x^n - m)^2\)</span>。这是一个有偏估计，样本方差的期望是<span class="math inline">\(E(s^2) = \frac{N-1}{N} \sigma^2\)</span>，同样当抽样的数据越多，越接近总体的方差，bias越小。</p>
<p>因此，模型的bias就是估计值的中心点到实际值中心点的距离，也就是<span class="math inline">\(E(\hat{y})\)</span>到<span class="math inline">\(E(y)\)</span>的距离，而模型的方差就是各个估计点<span class="math inline">\(\hat{y}\)</span>到其中心点<span class="math inline">\(E(\hat{y})\)</span>的距离。</p>
<p>那我们只有一组数据，为什么会有方差呢？实际上当我们抽样不同的数据，就会得到不同的模型，这样我们的估计值就会不同。所以模型存在方差（这已经是集成算法方面的东西了，bagging、boosting这一类的方法）。</p>
<p>现在假设我们抽样 5000 次，模型的复杂度和错误率会是什么样的关系呢？</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml001.png></p>
<p>如上图，当模型复杂度较低的时候，模型的variance很小，但是bias很大。当模型复杂度很高的时候，模型的variance很大，但是bias很小。</p>
<p>那么我们要如何解决这个问题？</p>
<p>如果我们的模型有很大的bias，那么我们应该增加模型的复杂度，或者增加模型的feature。</p>
<p>如果我们模型的variance很大，那么应该增加样本数量，或者做regularization。如下图，我们抽了100个样本，那么variance比10个样本的小很多。</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml002.png></p>
<p>而不增加样本，直接做regularization如下：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml003.png></p>
<p>但是我们需要注意到一点，regularization越大，bias会越大，因为模型复杂度降低了。这是一个trade-off。所以我们需要将数据分为 training 和 testing。一个比较有用的方法是 cross validation。它的原理如下：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml004.png></p>
<p>建模的禅道就是，别太在意training的效果，有时候validation和testing的效果反而不错。另外训练模型要注意的一点是，不要针对testing set做调优，否则会过拟合。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>台大李宏毅机器学习——梯度下降</title>
    <url>/machine_learning_step3/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>关于梯度下降的理论基础</p>
<span id="more"></span>
<p>梯度下降是要解决这样的一个凸优化问题： <span class="math display">\[
\theta^* = \arg \max_{\theta} L(\theta), L \text{ is the object function, }\theta \text{ is parameters}.
\]</span> 用向量 <span class="math inline">\(\bigtriangledown L = \begin{bmatrix} \frac{\partial L}{\partial \theta_1} \\ \frac{\partial L}{\partial \theta_2} \\ \vdots \\ \frac{\partial L}{\partial \theta_n} \end{bmatrix}\)</span> 表示梯度。</p>
<p>为了求解这个目标函数，我们需要按照下面方法更新<span class="math inline">\(\theta\)</span>: <span class="math inline">\(\boldsymbol{\theta}^1 = \boldsymbol{\theta}^0 - \eta \begin{bmatrix} \frac{\partial L}{\partial \theta_1} \\ \frac{\partial L}{\partial \theta_2} \\ \vdots \\ \frac{\partial L}{\partial \theta_n} \end{bmatrix}.\)</span></p>
<p>这里我们发现有个<span class="math inline">\(\eta\)</span>参数。这个参数我们称为 Learning Rate。Learning Rate 本身是机器学习中需要调节的一个重要参数。如果我们将梯度下降当做是爬山，那么Learning Rate我们可以认为是每一次爬山迈的步子大小，而梯度是我们面向的方向。所以很多时候，我们会将learning rate叫做步长。</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml005.png></p>
<p>上图显示了，如果我们有个很大的learning rate，那么我们可能会一步跨到更高的地方，或者来回震荡，无法迭代到最优解，相反，很小的 learning rate 可以让你到达最优解，但是将消耗大量的时间。</p>
<p>那么是否有办法来自适应调节 learning rate？</p>
<p>比如，一开始我们通常离最优解很远，那么可以让 learning rate 大一点，随着迭代次数增加，可以减少 learning rate。因此我们可以这样设计 <span class="math inline">\(\eta^t = \eta^0 / \sqrt{t+1}\)</span>。</p>
<p>事实上有很多种自适应的方法，例如Adagrad，Adam等。李老师介绍了Adagrad。定义为每个 parameter 都去除之前所有的微分的 root mean square。</p>
<p>比如我们有一个参数 <span class="math inline">\(w\)</span>, 原来的梯度下降是这样定义的： <span class="math display">\[
w^{t+1} = w^t - \eta^t g^t,
\]</span> 而adagrad是： <span class="math display">\[
w^{t+1} = w^t - \frac{\eta^t}{\sigma^n} g^t,
\]</span> 那 <span class="math inline">\(\sigma^t\)</span> 就是 <span class="math inline">\(w\)</span> 在前 <span class="math inline">\(t\)</span> 步微分的 root mean square。</p>
<p>所以 adagrad 这样更新 <span class="math inline">\(w\)</span>：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml006.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml007.png></p>
<p>这里要注意一点，adagrad中，梯度算出来越大，说明在这个参数方向上离最优解越远。直观上步长应该越大越快收敛，但是分母部分，梯度越大，步长越小。对于单个参数而言，这没有问题，但是如果现在考虑多个参数，那么梯度越大，不一定离最优解越远。如下图：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml008.png></p>
<p>我们看到，绿色的曲线，我们离最优解其实是比较近的，但是梯度很大，这时候如果我们只用一阶导数的话，要走好几步才能走到底。这就跟我们之前想的不太一致了，我们希望步长是慢慢减小的，但是我们希望的是步长在该大的地方大，改小的地方小。所以只考虑一阶段远远不够。</p>
<p>因此最好的 step 应该是还要考虑二次微分，最好的步长是一阶微分的绝对值除以二阶微分，这样才能在不同参数之间比较。</p>
<p>回过头来，adagrad 里面的分母起到的作用就是来替代二阶微分的作用。为什么这样的设计能够work？</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml009.png></p>
<p>如图所示，我们画出一阶微分的绝对值，只要我们 sample 足够多的数据，我们就能得到，由于二阶微分较小，所以左边的函数一阶微分的 root mean square 就比较小。</p>
<p>那么有没有提高梯度下降效率的方法？一般梯度下降需要遍历整个数据集，而 SGD（Stochastic Gradient Descent）每一次更新仅仅考虑一个 sample 的 Loss。</p>
<p>从图上看，两者的差别就在于 SGD 更散乱，但更快收敛。</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml010.png></p>
<p>实践上，为了让模型收敛不是太散乱，同时兼顾效率，会考虑使用 mini-batch SGD。也就是说，一次性用一小批数据来更新梯度。</p>
<p>另外一个可以加速收敛的方法是做 feature scaling。考虑下图的情况</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml011.png></p>
<p>在做 scaling 前，左边的 <span class="math inline">\(x_2\)</span> 导致 <span class="math inline">\(w_2\)</span> 的变动对 loss function 的影响非常大，所以左图最后收敛方向都会沿着 <span class="math inline">\(w_1\)</span> 缓慢下降。因此不用 adagrad 的话，左图的收敛非常慢。而右图因为接近正圆形，沿着任意方向都能快速接近最优解。</p>
<p>那么 scaling 方法有哪些？常见的比如 z-score，0-1标准化等。这个也叫作normalization。</p>
<hr />
<p>这里开始讨论梯度下降的数学原理。</p>
<p>梯度下降可以成立是基于泰勒展开，Taylor series。那么函数可以写成： <span class="math display">\[
\begin{align}
h(x) &amp;= \sum_{k=0}^{\infty} \frac{h^{(k)}(x_0)}{k!}(x-x_0)^k \\
&amp;= h(x_0) + h&#39;(x_0)(x-x_0)+\frac{h&#39;&#39;(x_0)}{2!}(x-x_0)^2+\dots
\end{align}
\]</span> 当 <span class="math inline">\(x\)</span> 无限趋近于 <span class="math inline">\(x_0\)</span> 时候，我们取一阶泰勒展开就可以近似估计原函数。相应的，如果原函数是个多元函数，那么相应的将导数改成偏导数就可以。</p>
<p>那么，基于泰勒公式，我们的 Loss function 就可以用泰勒展开为一阶多项式。</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml012.png></p>
<p>那么在图上一个红色的圆圈内，如何让 Loss 最小？我们可以发现，Loss 只跟两个向量有关，一个是<span class="math inline">\((u, v)\)</span>，另一个是<span class="math inline">\((\theta_1 - a, \theta_2 - b)\)</span>。我们用<span class="math inline">\((\Delta \theta_1\)</span>, <span class="math inline">\(\Delta \theta_2)\)</span> 表示第二个向量。</p>
<p>因此我们可以知道，要让Loss最小，就是上述两个向量的内积最小，也就是找<span class="math inline">\((u, v)\)</span>反方向上长度最长的向量： <span class="math display">\[
\begin{bmatrix}
\Delta \theta_1 \\
\Delta \theta_2
\end{bmatrix} = -\eta \begin{bmatrix}
u \\
v
\end{bmatrix}
\]</span> 也就是 <span class="math display">\[
\begin{bmatrix}
\theta_1 \\
\theta_2
\end{bmatrix} = \begin{bmatrix}
a \\
b
\end{bmatrix} - \eta \begin{bmatrix}
u \\
v
\end{bmatrix}
\]</span></p>
<p>优化方法有很多，之前博客里面写的遗传算法，退火算法，蚁群算法等都是，但是 mini-batch SGD 是目前最常用的方法，因为效率极高。但是这不意味着 SGD 就是最好的方法，因为这是一个很典型的贪心算法，大部分情况下只能接近局部最优。 如下图：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml013.png></p>
<p>通常我们发现梯度下降很慢的时候，就会停止迭代了，但实际上，可能我们离局部最优都很远，更不用说全局最优。深度学习常见的一个坑。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>台大李宏毅机器学习——分类算法</title>
    <url>/machine_learning_step4/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>分类算法的一些提纲挈领的概念。 <span id="more"></span></p>
<p>分类算法就是将数据喂给模型，模型输出这一笔数据属于哪一类。而之前的回归算法是喂一部分数据后吐出一串连续值。</p>
<p>不同于回归，分类模型的 Loss function 可以记为 <span class="math display">\[
L(f) = \sum_n \delta(f(x^n) \ne \hat{y}^n)。
\]</span></p>
<p>这一个 Loss function 不可微分，不能使用梯度下降的方法解。不用梯度下降的解法可以有感知机或者 SVM 等。</p>
<p>这里介绍另一种方法，也就是基于条件概率的方法。给定类1和类2的概率，以及类1下<span class="math inline">\(x\)</span>的条件概率，类2下<span class="math inline">\(x\)</span>的条件概率，然后我们就可以计算，给定<span class="math inline">\(x\)</span>的情况下，属于类1和类2的条件概率是多少。概率大的那个就是<span class="math inline">\(x\)</span>最有可能属于的类：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml014.png></p>
<p>上述方法有一个问题，如果数据不出现在 training data 里，怎么计算在某个类别下，这个 sample 的概率？比如下图中，海龟是 training data 外的数据，那么取到海龟的概率是否记为0？</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml015.png></p>
<p>那其实我们可以假设数据是符合一定分布的，然后去计算在该分布下，出现这个样本的概率。比如现在假设数据符合高维正态分布，这里可以使用极大似然法，也就是找到一组参数<span class="math inline">\(\mu\)</span>和<span class="math inline">\(\Sigma\)</span>使得每一个样本的概率乘积最大。</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml016.png></p>
<p>那么要让上面的 Likelihood 最大，就是要找到 <span class="math inline">\(\mu^*, \Sigma^*\)</span>得到<span class="math inline">\(\arg \max_{\mu,\Sigma} L(\mu, \Sigma)\)</span>。由于正态分布的概率密度函数在均值处取得最大值，因此相应的均值、标准差可以很直观计算出来。</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml017.png></p>
<p>不过实操上，用上图的计算方法，我们并不会得到一个很好的结果。因为我们没有考虑两个 class 的协方差。所以我们将原来的公式改进为：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml018.png></p>
<p>现在的公式就考虑了两个类的协方差，相对来说会比单考虑一个类别的方差效果好。</p>
<p>当然，理论上，这边可以用任意概率分布函数，可以根据实际的数据情况来决定，不过一般来说用正态分布比较多，毕竟好算。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>台大李宏毅机器学习——逻辑回归</title>
    <url>/machine_learning_step5/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>分类算法常年扛把子，逻辑回归 <span id="more"></span></p>
<p>逻辑回归是按照线性的方程进行分类的算法。最基本的逻辑回归是针对二分类的。二分类的数据我们记取值范围为<span class="math inline">\([0, 1]\)</span>，由于回归方程不能直接对分类数据进行计算，因此我们引入<span class="math inline">\(\sigma\)</span>函数。 <span class="math display">\[
\sigma(z) = \frac{1}{1+\exp(-z)}.
\]</span> <span class="math inline">\(\sigma\)</span>函数的作用就是将二分类的值平滑成一条曲线。 <img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml019.png></p>
<p>在开始逻辑回归之前，先回顾一下上一篇博客的内容。上一篇大致介绍了贝叶斯方法。贝叶斯方法是按照 posterior probability 来进行分类的。</p>
<p>posterior probability 在二分类时候表示为： <span class="math display">\[
\begin{align}
\text{P}(C_1|x) &amp;= \frac{\text{P}(x|C_1) \text{P}(C_1)}{\text{P}(x|C_1) \text{P}(C_1) + \text{P}(x|C_2) \text{P}(C_2)} \\
&amp;= \frac{1}{1+\frac{\text{P}(x|C_2) \text{P}(C_2)}{\text{P}(x|C_1) \text{P}(C_1)}} \\
&amp;= \frac{1}{1+\exp(-z)} \\
&amp;= \sigma(z)
\end{align}
\]</span></p>
<p>我们很神奇地发现，其实上下都除以分子以后，就变成了sigmoid函数的样子。</p>
<p>因此，<span class="math inline">\(z = -\ln(\frac{\text{P}(x|C_2) \text{P}(C_2)}{\text{P}(x|C_1) \text{P}(C_1)}) = \ln(\frac{\text{P}(x|C_1) \text{P}(C_1)}{\text{P}(x|C_2) \text{P}(C_2)})\)</span>。之前假设数据分布是符合正态分布的，因此<span class="math inline">\(\text{P}(x|C_i) \text{P}(C_i)\)</span>符合正态分布<span class="math inline">\(\frac{1}{(2\pi)^{D/2}} \frac{1}{|\Sigma|^{1/2}} \exp(-\frac{1}{2}(x-\mu)^{\top} \Sigma^{-1} (x-\mu))\)</span>。 因为<span class="math inline">\(P(C_i) = \frac{N_i}{\sum N_n}\)</span>，因此<span class="math inline">\(z = \ln(\frac{\text{P}(C_1)}{\text{P}(C_2)}) + \ln(\frac{\frac{1}{(2\pi)^{D/2}} \frac{1}{|\Sigma_1|^{1/2}} \exp(-\frac{1}{2}(x-\mu_1)^{\top} \Sigma_1^{-1} (x-\mu_1))}{\frac{1}{(2\pi)^{D/2}} \frac{1}{|\Sigma_2|^{1/2}} \exp(-\frac{1}{2}(x-\mu_2)^{\top} \Sigma_2^{-1} (x-\mu_2))})\)</span>。加号左边就是一个常数，很好计算，先不去管，化简一下右边的部分。 <span class="math display">\[
\ln(\frac{\frac{1}{(2\pi)^{D/2}} \frac{1}{|\Sigma_1|^{1/2}} \exp(-\frac{1}{2}(x-\mu_1)^{\top} \Sigma_1^{-1} (x-\mu_1))}{\frac{1}{(2\pi)^{D/2}} \frac{1}{|\Sigma_2|^{1/2}} \exp(-\frac{1}{2}(x-\mu_2)^{\top} \Sigma_2^{-1} (x-\mu_2))}) = \ln(\frac{\frac{1}{|\Sigma_1|^{1/2}}}{\frac{1}{|\Sigma_2|^{1/2}}}) + \ln(\frac{\exp(-\frac{1}{2}(x-\mu_1)^{\top} \Sigma_1^{-1} (x-\mu_1))}{\exp(-\frac{1}{2}(x-\mu_2)^{\top} \Sigma_2^{-1} (x-\mu_2))})
\]</span> 因为我们在上一节课中假设了两个变量的方差相等，因此上面式子的左边部分又可以消掉，只剩下右边部分。 将右边部分展开化简： <span class="math display">\[
\begin{align}
\ln(\frac{\exp(-\frac{1}{2}(x-\mu_1)^{\top} \Sigma_1^{-1} (x-\mu_1))}{\exp(-\frac{1}{2}(x-\mu_2)^{\top} \Sigma_2^{-1} (x-\mu_2))}) &amp;= \ln(\exp(-\frac{1}{2}(x-\mu_1)^{\top} \Sigma_1^{-1} (x-\mu_1))) - \ln(\exp(-\frac{1}{2}(x-\mu_2)^{\top} \Sigma_2^{-1} (x-\mu_2))) \\
&amp;=-\frac{1}{2}\Big[(x-\mu_1)^{\top}\Sigma^{-1}_1(x-\mu_1) - (x-\mu_2)^{\top}\Sigma^{-1}_2(x-\mu_2) \Big] \\
&amp;= -\frac{1}{2}\Big[x^{\top} \Sigma^{-1}_1 x - x^{\top} \Sigma^{-1}_1 \mu_1 - \mu_1^{\top} \Sigma_1^{-1} x + \mu_1^{\top} \Sigma_1^{-1} \mu_1 - (x^{\top} \Sigma^{-1}_2 x - x^{\top} \Sigma^{-1}_2 \mu_2 - \mu_2^{\top} \Sigma_2^{-1} x + \mu_2^{\top} \Sigma_2^{-1} \mu_2) \Big] \\
&amp;= -\frac{1}{2} \Big[ -2(x^{\top}\Sigma^{-1}\mu_1 - x^{\top}\Sigma^{-1}\mu_2) + \mu_1^{\top} \Sigma^{-1} \mu_1 - \mu_2^{\top} \Sigma^{-1} \mu_2 \Big]
\end{align}
\]</span> 将这个结果代回原来的式子当中，我们可以得到，其实这也是一个线性模型。 <span class="math display">\[
z = (\mu_1 - \mu_2)^{\top} \Sigma^{-1} x - \frac{1}{2}(\mu_1)^{\top}(\Sigma)^{-1} \mu_1 + \frac{1}{2}(\mu_2)^{\top}(\Sigma)^{-1} \mu_2 + \ln(\frac{N_1}{N_2})
\]</span> 所以，<span class="math inline">\(\boldsymbol{w}^{\top} = (\mu_1 - \mu_2)^{\top} \Sigma^{-1}，b = \frac{1}{2}(\mu_1)^{\top}(\Sigma)^{-1} \mu_1 + \frac{1}{2}(\mu_2)^{\top}(\Sigma)^{-1} \mu_2 + \ln(\frac{N_1}{N_2})\)</span>。</p>
<p>这个式子看起来很复杂，但是其实化简之后就是之前的线性模型。那么上一节课中，我们用的方法是 generate probability 的方法，也就是我们根据数据的情况，假设数据符合某种分布，比较常用的是正态分布，根据数据的均值和协方差矩阵计算当我们 sample 到一个 point 的时候，那么它属于某个 <span class="math inline">\(\text{class}_i\)</span> 的概率是多少。</p>
<p>这种方法也叫作线性判别，也就是 LDA ，需要跟 NLP 中的 LDA 区别。如果我们假设每个变量之间都是完全独立的，那么这个模型就变成了朴素贝叶斯模型。想想就觉得好神奇诶 :-)。</p>
<p>现在问题来了，如果是一个线性模型，那么我们能不能用梯度下降的方法一次性把参数学出来，而不是去计算好几个均值和方差？</p>
<p>首先我们先按照最早的线性模型的方法，构造我们的 function set。逻辑回归的目的是为了计算一个 sample 属于某个类别的概率有多少，因此，我们构建的函数可以是： <span class="math display">\[
f_{w, b}(x) = P_{w, b}(C_1|x) = \sigma(z)
\]</span> 其中<span class="math inline">\(z = w \cdot x + b = \sum w_i x_i + b\)</span>，这样就可以包括所有可能的<span class="math inline">\(w，b\)</span>。</p>
<p>同回归模型相比，因为逻辑回归这里加入了 sigmoid 函数，因此逻辑回归的取值范围只有<span class="math inline">\((0, 1)\)</span>，而线性方程因为没有做任何限制，因此取值是<span class="math inline">\((-\infty, \infty)\)</span>。</p>
<p>现在我们有了模型，那么如何衡量模型的好坏呢？参考原来的极大似然法，我们可以得到： <span class="math display">\[
L(w, b) = f_{w, b}(x^1)f_{w, b}(x^2)(1-f_{w, b}(x^3)) \dots
\]</span> 我们的目的是让这个概率最大，也就是 <span class="math display">\[
w^*, b^* = \arg \max_{w,b}(L(w, b)) = \arg \min_{w, b}-\ln(L(w, b))
\]</span> 我们假设 class 1 是 1， class 2 是 0。那么我们的目标函数就能写作： <span class="math display">\[
\sum_n-\Big[y^n \ln f_{w, b}(x^n) + (1-y^n) \ln (1-f_{w, b}(x^n)) \Big]
\]</span> 这个其实就是信息论里的交叉熵。交叉熵定义为：<span class="math inline">\(H(p, q) = - \sum_x p(x) \ln(q(x))\)</span>。因此，在这里，我们的 loss function 就可以定义为 <span class="math inline">\(L(f) = \sum_n C(f(x^n), y^n)\)</span>。根据交叉熵的意义，当<span class="math inline">\(f(x^n)\)</span>与真实概率越接近，交叉熵越小。</p>
<p>得到这个损失函数，我们就可以用梯度下降的方法来求解。我们想让损失函数最小，可以对其求偏导。实际上就是对 <span class="math inline">\(\ln f_{w,b}(x^n)\)</span> 和 <span class="math inline">\(\ln (1-f_{w,b}(x^n))\)</span> 求偏导。</p>
<p>分别计算一下，第一个式子： <span class="math display">\[
\begin{align}
\frac{\partial{\ln(f(x^n))}}{\partial{w_i}} &amp;= \frac{\partial{\ln \sigma(z)}}{\partial{z}} \frac{\partial{z}}{\partial{w_i}} \\
&amp;= \frac{\partial{\sigma(z)}}{\sigma(z)} \frac{\partial{\sigma(z)}}{\partial{z}} \frac{\partial{z}}{\partial{w_i}} \\
&amp;= \frac{1}{\sigma} \sigma(1-\sigma) x^n \\
&amp;= (1-\sigma)x^n
\end{align}
\]</span> 第二个式子： <span class="math display">\[
\begin{align}
\frac{\partial \ln(1-f_{w,b}(x))}{\partial w_i} &amp;= -\frac{1}{1-\sigma} \sigma(1-\sigma) x \\
&amp;= -\sigma x
\end{align}
\]</span> 代回原来的公式中，我们就能得到，原来函数对 <span class="math inline">\(w_i\)</span> 的偏导数为： <span class="math display">\[
\sum_n -\Big[y^n(1-\sigma)x^n_i - (1-y^n)\sigma x_i^n \Big] = \sum_n -\Big[y^n - y^n \sigma - \sigma + y^n \sigma \Big] x_i^n = \sum_n -(y^n - \sigma)x^n_i
\]</span></p>
<p>那么我们发现一个很有意思的事情，那就是，这个梯度下降的方程，和我们最早的 linear regression 的梯度下降是一模一样的。</p>
<p>这里就有一个问题，为什么同样是线性模型，这里不可以使用回归模型中的 MSE 作为 loss function？</p>
<p>我们这里强行使用 MSE 试验一下。</p>
<p>如果今天的 loss function 是 MSE，那么，我们的偏导数就是： <span class="math display">\[
\begin{align}
\frac{\partial \frac{1}{2}(f_{w, b}(x) - y)^2 }{\partial w_i} &amp;= (f_{w, b}(x) - y) \frac{\partial f_{w, b}(x)}{\partial z} \frac{\partial z}{\partial w_i} \\
&amp;= (f_{w, b}(x) - y) f_{w, b}(x) (1 - f_{w, b}(x)) x_i
\end{align}
\]</span> 假如，我们的<span class="math inline">\(y=1\)</span>，如果<span class="math inline">\(f_{w, b}(x) = 1\)</span>，那么我们的偏导数趋近于0，非常好。但是如果现在我们的<span class="math inline">\(f_{w, b}(x) = 0\)</span>，我们会发现，其实我们的损失函数依然等于0。 同样的，如果<span class="math inline">\(y=0\)</span>也会得到这样的结果。</p>
<p>下图就是这个结果的原因：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml020.png></p>
<p>一般情况下，逻辑回归这类的模型称为 Discriminative ，而上一节里的LDA，或者贝叶斯方法被称为 Generative。</p>
<p>一般情况下，Discriminative model 会比 Generative model 要好。但是因为 Generative model 带入了一定的分布的假设，因此只需要少量的数据就可以训练，同时对噪音比较鲁棒。另外，先验分布和类别依赖可以从不同的数据来源进行估计。</p>
<p>对于多分类问题，与二分类类似，我们对每一个类别计算<span class="math inline">\(\sigma\)</span>函数，计算<span class="math inline">\(y_i = \frac{e^{z_i}}{\sum e^(z_j)}\)</span>，这个就叫做 Softmax。那么<span class="math inline">\(y_i\)</span>就可以看成是属于 class i 的概率。然后依然用 cross entropy 作为 loss function，就能得到我们想要的结果。</p>
<p>那么事实上逻辑回归有很明显的缺点，那就是逻辑回归无法解决 XOR 问题。如何解决 XOR 问题呢？最简单的方法是做坐标映射，将原线性不可分的坐标映射到线性可分的空间中。但是事实上，这种坐标映射是非常 tricky 的，一般有 domain knowledge 会有很大的帮助，但是如果什么都不会怎么办呢？ 我们就可以使用两个逻辑回归来将原来的坐标进行转换。示例图如下：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml021.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml022.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml023.png></p>
<p>我们构造这样的一个模型，先用两个逻辑回归进行 feature transform，再用一个逻辑回归进行分类。</p>
<p>这样的设计就是传说中的多层感知机（MLP），也就是传统的神经网络，我们将这中间的每一个模型叫做神经元，每一个平行的神经元之间就叫做层。多放几层就变成了现在最火的深度学习，再加宽一点，也就是多放一些神经元，就能硬刚各种模型了。好神奇诶。</p>
<p>那么一个逻辑回归可以梯度下降，这里有三个，怎么算呢？现在的框架下，这三个模型是可以同时学习参数的。下一节的内容就是关于深度学习的。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>台大李宏毅机器学习——深度学习调参</title>
    <url>/machine_learning_step7/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>深度学习调参技巧入门 <span id="more"></span></p>
<p>深度学习有很多框架，个人最喜欢的是dmlc的MXNet还有PyTorch。keras是另外一个非常友好的框架，后台可以调用tensorflow，但是tensorflow本身不是一个非常友好的框架，所以有兴趣的可以自己看看，上手很快。</p>
<p>这里大概介绍深度学习炼丹的一些入门技巧。</p>
<p>之前提到，深度学习也是机器学习的一个特例，因此深度学习的过程也是设计模型，计算模型的好坏，选择一个最好的模型。</p>
<p>最基础的一个方法跟一般机器学习一样，先看在training上的效果，如果够好，再看在testing上的效果。但是这里有个不同，一般的机器学习基本上都可以在training上得到百分百正确的结果，例如决策树。但是深度学习并不一定能够在training上得到百分百正确。因此在训练深度学习的时候，不要一步到位只看testing的效果。</p>
<p>这里有入门的五个小技巧：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml036.png></p>
<p>那么首先我们想一下，是否深度网络越深效果越好呢？答案是不一定的。举例而言，用全连接层和sigmoid函数训练mnist的时候，当层数加到很大的时候，训练效果可能就很不好了。</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml031.png></p>
<p>一般而言，如果我们的neuron用的是sigmoid函数，那么我们会发生这样的现象。这个现象的原因就是梯度消失 Vanishing Gradient。</p>
<p>所以，按照之前的反向传播更新参数的方法，靠近output layer的地方参数已经更新完毕，但是靠近input layer的地方还没train。用图形表示如下：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml032.png></p>
<p>那sigmoid函数为什么会发生这样的事情呢？我们不用严格去计算<span class="math inline">\(\frac{\partial l}{\partial w}\)</span>，按照导数的定义，我们可以知道，<span class="math inline">\(\frac{\partial l}{\partial w} = \frac{\Delta l}{\Delta w}\)</span>。所以我们把这种思想代入sigmoid作为激活函数的神经网络当中，我们可以发现当第一层的<span class="math inline">\(w_1\)</span>发生很大的变化，那么<span class="math inline">\(z_1 = w_1 \cdot x + b_1\)</span>发生很大的变化，但是经过sigmoid函数后，这个变化被缩小了，因此<span class="math inline">\(\sigma(z_1)\)</span>是小于<span class="math inline">\(z_1\)</span>的，而随着层数的增加，这样的影响就会不断加强。这就导致了在input layer地方的梯度会变得很小。也就是梯度消失的问题。</p>
<p>那么理论上而言，用dynamic的learning rate也是可以解决这样的问题的，但是直接将sigmoid函数替换掉来得更干脆一点。</p>
<p>现在有一个很常用的激活函数是ReLu（Recitified Linear Unit）。ReLu有很多好处，一个是求导更快；一个是Hinton提出无数个sigmoid叠加可以得到ReLu；当然，最重要的是ReLu可以解决梯度消失的问题。ReLu的函数可以表示如下： <span class="math display">\[
a =
\begin{cases}
0, &amp;\mbox{if }z&lt;0 \\
z, &amp;\mbox{if }z&gt;0
\end{cases}
\]</span> 这样的一个激活函数不是一个连续可导的函数，那么梯度下降是依赖于导数的，能不能求解呢？</p>
<p>事实上，因为ReLu是分段可导的，而且在实际模型中，经过ReLu计算的神经元，每一层会有有一半的神经元是0，因此，如果现在一个神经网络的激活函数用的是ReLU，那么我们可以将整个神经网络表示为：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml033.png></p>
<p>那我们可以将那些<span class="math inline">\(0\)</span>的neuron直接不要掉，得到一个更瘦的网络。那因为现在的网络变成了线性的神经元，因此每次传递的梯度没有经过缩放，因此不会有梯度消失的问题。</p>
<p>ReLu有各种各样的变型，一种是Leaky ReLu： <span class="math display">\[
a =
\begin{cases}
0.01z, &amp;\mbox{if }z&lt;0 \\
z, &amp;\mbox{if }z&gt;0
\end{cases}
\]</span> 还有一个种是parametric ReLu： <span class="math display">\[
a =
\begin{cases}
\alpha z, &amp;\mbox{if }z&lt;0 \\
z, &amp;\mbox{if }z&gt;0
\end{cases}
\]</span></p>
<p>那事实上我们可以让网络自己决定每个neuron要用什么样的激活函数。这样的结构是Goodfellow提出的maxout network。maxout network跟之前的网络不一样的地方是，原先下图中我们得到的<span class="math inline">\(5\)</span>，<span class="math inline">\(7\)</span>这些数字都是要经过activation function变成其他值的。但是在maxout network里面，我们会把这些值变成一个group，然后去group里面的比较大的一个值作为output。这个事情，其实跟CNN里面的max pooling一样。</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml034.png></p>
<p>这个网络中，哪几个神经元要结合，每个group中放多少个element需要事先设定好。由此我们可以发现，ReLu其实是maxout的一种特殊情况，而ReLu的其他变种也是，都是maxout的真子集。那事实上maxout并不能学习非线性的activation function，它只能学习分段线性的激活函数。</p>
<p>那我们如果element放的越多，maxout network学到的激活函数分段就越多。从某种程度上而言，maxout network比较强的地方就是，不同的样本喂进去以后，通过更新<span class="math inline">\(w\)</span>，可能得到ReLu，也可能得到Leaky ReLu这样的activation function，也可能是其他的activation function。也就是说maxout network是一个会学习的网络。</p>
<p>现在的问题是，这样一个分段的函数是否开进行梯度下降呢？实践上而言，这是可行的。因为在maxout network中，每一次传递的都是最大的那个值，那其余的神经元不对loss做贡献，因此每一次传递的都是一个linear的结果，那梯度下降是可以对linear的函数求解的。这里不用担心有一些<span class="math inline">\(w\)</span>不会被训练到，因为不同的batch喂进去的时候，不同的<span class="math inline">\(w\)</span>会被影响到。示意图如下：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml035.png></p>
<p>在这个网络中，<span class="math inline">\(z_2^1\)</span>没被训练到，但是如果用一笔新的数据，就有可能训练到这个值。</p>
<p>另外如果不调整激活函数，我们就可以使用adagrad这样的方法。现在回顾一下adagrad，adagrad更新参数的方法是： <span class="math display">\[
w^{t+1} = w^t - \frac{\eta}{\sqrt{\sum g_i^2}} g^t
\]</span></p>
<p>Hinton 提出了一种新的更新方法，RMSProp，更新步骤如下： <span class="math display">\[
w^{t+1} = w^t - \frac{\eta}{\sigma^t} g^t, \sigma^t = \sqrt{\alpha (\sigma^{t-1})^2 + (1-\alpha) (g^t)^2}
\]</span> 这是对Adagrad的一种变形。</p>
<p>此外，我们也可以用物理的方法来考虑一下梯度下降。在物理世界中，一个小球从山丘向下滚动的时候，会因为惯性的关系继续滚动下去，越过saddle point，甚至可能越过一些小坑和小坡。因此我们也可以在梯度下降中加入一个类似的概念。示意如下：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml037.png></p>
<p>一般而言，这样的方法能躲过saddle point，但是不一定能够躲过local minimum。现在如果我们考虑了monentum，那么我们的迭代方式就是： <span class="math display">\[
w^{t+1} = w^t + v^{t+1} \\
v^{t+1} = \lambda v^t - \eta g^{t+1}
\]</span> 我们可以看到，实际上用momentum的更新方法，我们实际上是考虑了之前每一次的移动。现在流行的Adam这个方法，实际上结合了RMSProp和momentum两种。</p>
<p>那上面两种方法都是针对training进行优化的。现在看另外三种对testing进行优化的方法。</p>
<p>第一种是early stopping。 Early stopping就是当模型在validation set上没有提升的情况下，就提前停止模型训练。当然，前提是模型在training上面可以正常收敛。</p>
<p>第二种是regularization。Regularization有两种，一种是L1 regularization，另一种是L2 regularization。</p>
<p>L2 regularization表示如下： <span class="math display">\[
L&#39;(\theta) = L(\theta) + \lambda \frac{1}{2}||\theta||_2, \quad ||\theta||_2 = (w_1)^2 + (w_2)^2 + \dots + (w_n)^2
\]</span> 那么我们更新参数的方法就是： <span class="math display">\[
w^{t+1} = w^t - \eta \frac{\partial L&#39;}{\partial w} = w^t - \eta (\frac{\partial L}{\partial w} + \lambda w^t) = (1-\eta \lambda)w^t - \eta \frac{\partial L}{\partial w}
\]</span> 由于后面还跟了一个当前的梯度，因此不用担心这样的更新方法会导致所有的参数迭代到0。</p>
<p>那L1 regularization表示如下： <span class="math display">\[
L&#39;(\theta) = L(\theta) + \lambda ||\theta||_1, \quad ||\theta||_1 = |w_1| + |w_2| + \dots + |w_n|
\]</span> 所以我们更新参数的方法就是： <span class="math display">\[
w^{t+1} = w^{t} - \eta \frac{\partial L&#39;}{\partial w} = w^t - \eta(\frac{\partial L}{\partial w} + \lambda \text{sign}(w^t))
\]</span> 所以用L1，我们的参数每一次都会向0移动一个<span class="math inline">\(\lambda \eta\)</span>。</p>
<p>实际上因为deep learning在初始化参数的时候，都会选择接近0的位置开始，因此实际上regularization在深度学习当中的作用可能还不如early stopping来得有用。</p>
<p>最后一种就是dropout。dropout就是随机丢掉一部分的neuron，每一次mini-batch进入网络，都要重新dropout一些网络，也就是每一个batch的网络实际上是不一样的。从某种程度而言，这也是一种集成算法。</p>
<p>这里需要注意的是，我们在training的时候需要进行dropout，但是在testing的时候是不进行dropout的。那这个时候，我们在training学到的<span class="math inline">\(w\)</span>，在testing上就要将<span class="math inline">\(w\)</span>乘以<span class="math inline">\(1-p\)</span>，其中<span class="math inline">\(p\)</span>是dropout的概率。</p>
<p>那为什么要做这样的动作呢？之前我们说过，dropout是也是一种ensemble，ensemble方法如下：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml039.png></p>
<p>那dropout实际上是ensemble的一个变种。因为每一个neuron都有可能被drop，假设有m个neuron，那么我们理论上有可能得到<span class="math inline">\(2^m\)</span>种网络，而这些网络之间有些参数是共用的。</p>
<p>那如果我们用ensemble的方法，那么我们就是将所有网络的结果进行平均。但是dropout是直接将所有的参数乘以<span class="math inline">\(1-p\)</span>，然后直接预测。而神奇的地方就在于，这样的方法得到结果跟ensemble的结果是接近的。</p>
<p>举例而言，如下图：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml040.png></p>
<p>我们假设右上角就是我们用dropout训练好的模型，那么这个模型所有可能出现的network是左边的四种。假设每个neuron被dropout的概率一样，都是0.5，那么这四种结构出现的概率就是一样的，因此这四个结构的average就是右下角的结果，刚好就是training的weight乘以<span class="math inline">\(1-p\)</span>。</p>
<p>那实际上，这个事情理论上只有在activation function是linear的时候才能work，nonlinear的模型实际上是不work的。但是神奇的就是，在真实使用的时候，nonlinear的模型，一样也可以使用。不过一般来说，如果activation function是linear的时候，dropout的效果会比较好。</p>
<p>以上就是深度学习入门级的调参技巧。还是散沙的一句话，深度学习已经变成实验科学了，多动手是王道。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>台大李宏毅机器学习——CNN</title>
    <url>/machine_learning_step8/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>卷积网络入门 <span id="more"></span></p>
<p>卷积就是现在活跃在CV领域的大杀器，简写做CNN，实际上是一个fully connected network的变种。</p>
<p>CNN有三个特性：</p>
<ul>
<li><p>可以找到图像中特定的pattern，而且这个pattern是比原来的图像要小的。</p></li>
<li><p>同样的pattern可以出现在图片中不同的地方，都会被CNN探测到。</p></li>
<li><p>可以做subsampling而不会过分影响图像的质量</p></li>
</ul>
<p>如图，CNN的做法是这样的：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml042.png></p>
<p>假设我们有一个image如左图，有一个filter如右上角，这个filter是可以被学习出来的。不过filter的大小是需要被人工设定好的，例如这里设定的是<span class="math inline">\(3 \times 3\)</span>。那CNN的工作原理就是，将这个filter放到image上面，然后等大小的一个matrix跟filter做inner product。所以红色框出来的部分跟filter的内积就是3。</p>
<p>然后我们会移动filter来遍历整个image，这个移动的步伐我们叫做stride，stride的大小表示filter每一次移动的像素数量。例如这里我们stride设为1，那么我们下一步得到的就是下图：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml043.png></p>
<p>这样一直扫描，直到一排结束，然后向下移动stride的长度再向右继续扫描。</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml044.png></p>
<p>最后我们就能得到这样一个结果：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml045.png></p>
<p>这个filter因为对角线都是1，因此我们这个filter要寻找的就是对角线上的pattern。那么这个image中有两个对角线是1的pattern，一个在左上角，一个在左下角。这就满足了CNN的两个特性。一个是找到一个小于image的pattern，另一个是image中不同位置的pattern可以用一个filter找出来。</p>
<p>我们可以将这个结果看作是经过过滤器处理的新图片，多个filter过滤之后的图片就是feature map。</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml046.png></p>
<p>那这个是黑白单通道的图片，如果现在用的是彩色图片，实际上就是拆开为Red、Green、Blue三通道。整个做法如下图：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml047.png></p>
<p>下面一个动图比较形象：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml041.gif></p>
<p>那为什么我们认为卷积是一个特殊的fully connected network呢？</p>
<p>我们现在想象将一个image摊平，从一个matrix摊平成一个vector。我们上面的<span class="math inline">\(6 \times 6\)</span>的矩阵就可以摊平为一个长度为36的vector。这样我们每个filter的作用就可以用下图来表示：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml048.png></p>
<p>这样的设计有很多好处，首先，我们需要训练的参数远远小于全连接层，如果原来是全连接，我们需要训练36个参数，而现在我们只需要9个。其次，同一个filter移动的时候，参数是一样的，如下图：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml049.png></p>
<p>这样的好处同样是减少了需要训练的参数。如果是全连接层，现在接了2个neuron，我们需要训练的参数是72个，而在卷积里面，我们还是只需要训练9个。如果后面filter连接的越多，减少训练参数的效果越明显。</p>
<p>那卷积提取完特征之后，我们会过一个subsampling的过程，也就是pooling的过程。这个过程当中当前有两种常用的方法，一种是average pooling，一种是max pooling。</p>
<p>max pooling的过程就是下图：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml050.png></p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml051.png></p>
<p>这里我们需要设定的是每一次pooling的窗口有多大，也就是在多大的一个区域内做subsampling，上图设定的大小是<span class="math inline">\(2 \times 2\)</span>。当然，这里可以用average，也可以用max，也可以两个都用。但是实践上，max的效果好像比average要好一点。那事实上，现在的框架中，pooling层也有stride。</p>
<p>根据要做的事情不一样，不管是filter还是pooling，不一定非要将窗口设定为长宽相等的，也可以是不同的，比如filter也可以设计为<span class="math inline">\(2 \times 1\)</span>的矩阵这样。另外卷积的过程也可以反复进行好几次。比如VGG-16就有16层卷积。</p>
<p>另外需要提一点的就是，用现在的方法做卷积，原始图片的边边角角会丢失掉，如果filter设计的越大，丢失的就越多。那么为了避免丢失，有一种做法是filter设计的小一点，另一种就是把图片的边边角角拼一段像素上去。拼像素的方法在这里就叫做padding，有几种常见的方法，一种是填0，另一种是将边边角角的像素直接复制一个填进去。那padding要拼多少像素可以根据filter大小来定。filter越大，需要拼的就越多。padding是不是一定比不做效果好，这个视情况而定，多炼丹才知道。</p>
<p>那做完卷积后，我们需要做一个flatten的步骤，也就是将矩阵摊平成向量。如下图：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml052.png width=50%></p>
<p>这样的结果最后就可以过fully connected network做分类或者回归了。当然，现在深度学习做分类的还是比较多的。</p>
<p>我们现在可以看一看CNN到底在做什么，下图是AlexNet第一个layer的参数：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml053.png></p>
<p>这个图我们可以看到，上面的filter主要是看是否有一些pattern存在，而下面则是侦测颜色。事实上有各种各样的可视化方法，比如我们也可以把每个layer的output拿出来做图。</p>
<p>还有一种方法来看CNN是如何work的。例如我们想要分析更高层次layer，我们可以定义一个指标，degree of the activation of the k-th filter： <span class="math display">\[
a^k = \sum_i^m\sum_j^n a_{ij}^k
\]</span> 我们希望当input一个image（用<span class="math inline">\(x\)</span>表示）的时候，可以让<span class="math inline">\(a^k\)</span>最大。也就是<span class="math inline">\(x^* = \arg \max_x a^k\)</span>。举例来说，如果做了一个识别猫狗的分类器，从仿生的角度而言，我们可以认为一张猫的照片，可以让一个识别猫的特征的神经元兴奋，而识别狗的神经元静默。</p>
<p>那这个公式就可以求导用梯度下降来做这个事情。也就是计算<span class="math inline">\(\frac{a^k}{x_{ij}}\)</span>。如下图就是这样的效果：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml054.png></p>
<p>但是在CNN中，如果输出最后一层output的image，实际上我是得不到我们想象中的图片。比如说如果我们做mnist的分类，最后一层输出的图片并不会是数字，很可能得到的就是一片上世纪老电视的雪花屏。所以这样的结果是很容易被欺骗的，也就有了对抗样本这样的存在。另外就是网上有不少介绍的，将同一张图片变换一些像素，就会被识别为别的，比如把熊猫识别成汽车之类的。</p>
<p>那如果我们做一些regularization，比如<span class="math inline">\(x^* = \arg \max_x (a^k + \sum|x_{ij}|)\)</span>，这样最后一个layer的输出相对会规则一点。</p>
<p>当然我们还可以用另一个方式来看CNN到底是不是work的，比如我们取image中某个pixel，<span class="math inline">\(x_{ij}\)</span>，计算<span class="math inline">\(|\frac{\partial y_k}{\partial x_{ij}}|\)</span>，如果这个值很大，那么这个pixel是很重要的。将每个pixel算出来，我们就能知道模型到底聚焦在图片的哪个部分。如下图：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml055.png></p>
<p>那另外一种简单一点的方法是用一个纯色的框挡掉一些image，这样就能判断模型是不是无法正确识别，这样也能达到类似的效果。比如识别狗的模型，就挡掉狗，不挡掉狗分别试一下模型是不是可以正确分类。</p>
<p>CNN入门大概就这些，应用场景非常的多，反正还是那句话，多炼丹，多开脑洞。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>台大李宏毅机器学习——深度学习的必要性</title>
    <url>/machine_learning_step9/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>现在我们来考虑一下，模型是不是越深越好。 <span id="more"></span></p>
<p>如果我们要比较深度网络或者浅层网络，做公平比较的时候，深度网络和浅层网络直接需要设定的参数量是一致的才行。也就是说，深度网络可以瘦一点，而浅层网络就要胖一点。下图是一个比较：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml056.png></p>
<p>我们可以看到实际上，当参数一样的时候，层数越深，效果越好。那为什么会发生这样的情况？</p>
<p>从某种程度上而言，深度学习的结构是很像集成算法的。我们可以将前面的每一层都看成是多个弱分类器，最后根据多个弱分类器得到一个最终结果。</p>
<p>那事实上，这样的结构在一定程度上可以做到一定的举一反三，因此从某种程度而言，实际上正是因为数据量不够，所以我们才需要机器学习。如果现在真的有真正的big data，那根本不需要机器来做举一反三的事情。所以把大数据等同于AI，或者所谓大数据+深度学习=AI的，根本就是呵呵哒。</p>
<p>举个例子来说的话，深度学习做的事情有点像是剪窗花，通过多次对折，只要剪几次就能得到一个漂亮的窗花。下面是一个类似的例子：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml057.png></p>
<p>那我们可以看到，当数据量足够多的时候，单层神经网络也能有很好的结果，但是当数量只有2w笔的时候，三层网络崩坏的比较有规律。</p>
<p>那现在回过头想一下，我们说单层神经网络，只要neuron足够多，就可以拟合任意的连续函数。那么为什么我们要使用深层网络呢？其实用深层网络可以更高效实现这样的事情，因此我们偏向使用深度学习。另外很复杂的模型可以做到下面的例子：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml058.png></p>
<p>那么，既然理论上单层就能做到的事情，是不是实践上完全不可能做到多层的效果呢？也不是，微软的Rich提到，如果单层网络不去直接学习真实的label，而是去学习三层网络的output，实际上也能达到三层网络的效果。其实就是让单层网络去学习多层网络的特征。</p>
<p>嗯，今天被教育了，多动手，少逼逼，不要浪。进步从复现论文效果开始。</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>Char-RNN生成古诗</title>
    <url>/pytorch-char-rnn/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>尝试用char-RNN生成古诗，本来是想要尝试用来生成广告文案的，测试一波生成古诗的效果。嘛，虽然我对业务兴趣不大，不过这个模型居然把我硬盘跑挂了，也是醉。</p>
<span id="more"></span>
<p>其实Char-RNN来生成文本的逻辑非常简单，就是一个字一个字放进去，让RNN开始学，按照前面的字预测下面的字。所以就要想办法把文本揉成我们需要的格式。</p>
<p>比如说，我们现在有一句诗“床前明月光，疑是地上霜”。那么我们的输入就是“床前明月光”，那么我们的预测就是“前明月光，”，其实就是错位一位。</p>
<p>然后我们要考虑的是如何批量的把数据喂进去，这里参考了<a href="http://zh.gluon.ai/chapter_recurrent-neural-networks/lang-model-dataset.html">gluon的教程</a>上面的一个操作，因为诗歌是有上下文联系的，如果我们用随机选取的话，很可能就会丢掉很多有用的信息，所以我们还要想办法将诗歌的这种连续性保留下来。</p>
<p>mxnet教程的方法是先将所有的文本串成一行。所有的换行符替换为空格，所以空格在这里起到了分段的作用，空格也就有了意义。然后我们因为我们要批量训练，所以先按照我们每批打算训练多少行文本，将这一个超长的文本截断成这样，然后按照我们一次想看多少个字的窗口扫描过去。代码实现上如下： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_consecutive</span>(<span class="params">corpus_indices, batch_size, num_steps</span>):</span></span><br><span class="line">    corpus_indices = torch.tensor(corpus_indices)</span><br><span class="line">    data_len = <span class="built_in">len</span>(corpus_indices)</span><br><span class="line">    batch_len = data_len // batch_size</span><br><span class="line">    indices = corpus_indices[<span class="number">0</span>: batch_size*batch_len].reshape((</span><br><span class="line">        batch_size, batch_len))</span><br><span class="line">    epoch_size = (batch_len - <span class="number">1</span>) // num_steps</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch_size):</span><br><span class="line">        i = i * num_steps</span><br><span class="line">        X = indices[:, i: i + num_steps]</span><br><span class="line">        Y = indices[:, i + <span class="number">1</span>: i + num_steps + <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">yield</span> X, Y</span><br></pre></td></tr></table></figure></p>
<p>这样有一个好处就是可以保持诗句的连续性，效果上大概是：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 所有诗句拼成一行</span></span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>， <span class="number">10</span>， <span class="number">11</span>， <span class="number">12</span>]</span><br><span class="line"><span class="comment"># batch_size = 2, num_steps = 3</span></span><br><span class="line"><span class="comment"># batch 1</span></span><br><span class="line">[[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]</span><br><span class="line"><span class="comment"># batch 2</span></span><br><span class="line">[[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]]</span><br></pre></td></tr></table></figure>
<p>这样一来，一句诗[1, 2, 3, 4, 5, 6]就能在不同batch里面保持连贯性了。</p>
<p>然后就是很简单设计网络：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">lyricNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_dim, embed_dim, num_layers, weight,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_labels, bidirectional, dropout=<span class="number">0.5</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(lyricNet, self).__init__(**kwargs)</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.num_labels = num_labels</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        <span class="keyword">if</span> num_layers &lt;= <span class="number">1</span>:</span><br><span class="line">            self.dropout = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.dropout = dropout</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(weight)</span><br><span class="line">        self.embedding.weight.requires_grad = <span class="literal">False</span></span><br><span class="line"><span class="comment">#         self.embedding = nn.Embedding(num_labels, self.embed_dim)</span></span><br><span class="line">        self.rnn = nn.GRU(input_size=self.embed_dim, hidden_size=self.hidden_dim,</span><br><span class="line">                          num_layers=self.num_layers, bidirectional=self.bidirectional,</span><br><span class="line">                          dropout=self.dropout)</span><br><span class="line">        <span class="keyword">if</span> self.bidirectional:</span><br><span class="line">            self.decoder = nn.Linear(hidden_dim * <span class="number">2</span>, self.num_labels)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.decoder = nn.Linear(hidden_dim, self.num_labels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs, hidden=<span class="literal">None</span></span>):</span></span><br><span class="line">        embeddings = self.embedding(inputs)</span><br><span class="line">        states, hidden = self.rnn(embeddings.permute([<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]), hidden)</span><br><span class="line">        outputs = self.decoder(states.reshape((-<span class="number">1</span>, states.shape[-<span class="number">1</span>])))</span><br><span class="line">        <span class="keyword">return</span>(outputs, hidden)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span>(<span class="params">self, num_layers, batch_size, hidden_dim, **kwargs</span>):</span></span><br><span class="line">        hidden = torch.zeros(num_layers, batch_size, hidden_dim)</span><br><span class="line">        <span class="keyword">return</span> hidden</span><br></pre></td></tr></table></figure>
<p>这里我用的是很简单的one-hot做词向量，当然数据量大一点可以考虑pretrained的字向量。不过直观感受上用白话文训练的字向量应该效果不会太好吧。</p>
<p>接着就可以开始训练了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epoch):</span><br><span class="line">    start = time.time()</span><br><span class="line">    num, total_loss = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    data = data_iter_consecutive(corpus_indice, batch_size, <span class="number">35</span>)</span><br><span class="line">    hidden = model.init_hidden(num_layers, batch_size, hidden_dim)</span><br><span class="line">    <span class="keyword">for</span> X, Y <span class="keyword">in</span> data:</span><br><span class="line">        num += <span class="number">1</span></span><br><span class="line">        hidden.detach_()</span><br><span class="line">        <span class="keyword">if</span> use_gpu:</span><br><span class="line">            X = X.to(device)</span><br><span class="line">            Y = Y.to(device)</span><br><span class="line">            hidden = hidden.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output, hidden = model(X, hidden)</span><br><span class="line">        l = loss_function(output, Y.t().reshape((-<span class="number">1</span>,)))</span><br><span class="line">        l.backward()</span><br><span class="line">        norm = nn.utils.clip_grad_norm_(model.parameters(), <span class="number">1e-2</span>)</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += l.item()</span><br><span class="line">    end = time.time()</span><br><span class="line">    s = end - since</span><br><span class="line">    h = math.floor(s / <span class="number">3600</span>)</span><br><span class="line">    m = s - h * <span class="number">3600</span></span><br><span class="line">    m = math.floor(m / <span class="number">60</span>)</span><br><span class="line">    s -= m * <span class="number">60</span></span><br><span class="line">    <span class="keyword">if</span> (epoch % <span class="number">10</span> == <span class="number">0</span>) <span class="keyword">or</span> (epoch == (num_epoch - <span class="number">1</span>)):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch %d/%d, loss %.4f, norm %.4f, time %.3fs, since %dh %dm %ds&#x27;</span></span><br><span class="line">              %(epoch+<span class="number">1</span>, num_epoch, total_loss / num, norm, end-start, h, m, s))</span><br></pre></td></tr></table></figure>
<p>这里的训练过程需要注意两个点，一个是hidden的initial，因为我们想要保持句子的连续性，所以我们hidden的initial只要每个epoch的第一次initial一下就可以了，后面训练的过程中需要从计算图中拿掉。另外就是因为有梯度爆炸的问题，所以我们需要对梯度进行修剪。</p>
<p>最后一个是我自己最容易犯错的地方，死活记不住的就是RNN的输入输出每个dimension都代表了什么含义。原始的RNN接受的输入是(seq_len, batch_size, embedding_dimension)，输出的是(seq_len, batch_size, num_direction * hidden_dim)。所以我们习惯的batch在先的数据需要在这里做一个permute，将batch和seq做一下调换。然后就是我们做分类的时候，直接flatten成为一个长向量的时候，其实已经变成了[seq_len, seq_len, ...]这样的样子。简单理解就是本来我们都是横着看诗歌的，现在模型的输出是竖着输出的。所以我们后面算loss的时候，y也需要做一个转置再flatten。</p>
<p>具体的可以看我的这个<a href="&#39;https://github.com/SamaelChen/hexo-practice-code/blob/master/pytorch/text%20generater/generate%20poem.ipynb&#39;">notebook</a>。</p>
<p>接下来可能想试一下的是如果不用这种方法的话，是不是可以用padding的方法把句子长度统一再训练。</p>
<p>另外强势推荐<a href="https://github.com/chinese-poetry/chinese-poetry">最全中华古诗词数据库</a>。数据非常非常全了。</p>
<p>后面如果要做到很好的效果可以做的方向一个是做韵脚的信息，还有就是平仄的信息也带进去。</p>
<p>anyway，想了一下，这样训练完的hidden是不是就包含了一个作者的文风信息？！</p>
]]></content>
      <categories>
        <category>PyTorch</category>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>台大李宏毅机器学习——深度学习入门</title>
    <url>/machine_learning_step6/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>深度学习入门介绍，主要是MLP <span id="more"></span></p>
<p>深度学习本身不是很复杂的模型，只是机器学习的一个子分支，就是神经网络的加深加宽版本。当然，这么说也不完全对，不过深度学习的基础还是神经网络。</p>
<p>最早的时候深度学习就是 MLP 或者就叫神经网络，但是因为种种历史原因，早年间这种方法死活干不过 SVM，所以只能改名重出江湖。这其中各种恩怨情仇很八卦。</p>
<p>当然，后来 MLP 改名深度学习之后就重出江湖了，在 ImageNet 这个大赛上一举夺冠之后就一发不可收拾了。</p>
<p>深度学习其实就是机器学习的一个子分支，因此深度学习也满足机器学习的一般流程：提出假设模型，计算模型好坏，选择表现最好的模型。对于深度学习而言，模型就是神经网络。</p>
<p>一般最基础的深度学习模型就是全连接，也就是 Fully Connect Feedforward Network，如下图。</p>
<p><img data-src='https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml024.png'></p>
<p>这个神经网络的基本工作流程就是输入一个向量，乘以不同的系数，再将结果用 sigmoid 函数输出到下一层作为输入。这里每一个神经元都是一个函数，我们叫做激活函数。现在的神经网络设计中，sigmoid 函数已经很少用了，比较常用的是 ReLu 或者 tanh。</p>
<p>一个常见的神经网络可以分为输入层（Input Layer），隐藏层（Hidden Layer）以及输出层（Output Layer），基本结构如下：</p>
<p><img data-src='https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml025.png'></p>
<p>所以深度学习这边的模型假设就是设计神经网络结构，包括网络层数，每层的神经元数量等等。</p>
<p>其他的部分，设计 Loss function 和其他机器学习方法并没有什么本质区别，多分类一样可以用交叉熵，回归一样可以用 MSE。求解方法也是用梯度下降。</p>
<p>从理论上而言，只要有一个隐藏层就可以拟合任何连续函数。那么为什么我们要设计 Deep Learning 而不是 Fat Learning 呢？事实上，虽然理论上多层神经网络的表达能力和单层神经网络一样，但是在实操中，单层神经网络效果不如多层神经网络。</p>
<p>在之前的课程中，梯度下降需要求导的函数是比较简单的，现在多层神经网络要如何求导呢？神经网络本质上就是一个向量乘以多个矩阵，再经过激活函数进行变换。因此这里需要使用到链式求导：</p>
<ol type="1">
<li><p>如果<span class="math inline">\(y=g(x), z=h(y)\)</span>，则 <span class="math inline">\(\frac{d z}{d x} = \frac{d z}{d y} \frac{d y}{d x}\)</span></p></li>
<li><p>如果<span class="math inline">\(x=g(s), y=h(s), z=k(x, y)\)</span>，则<span class="math inline">\(\frac{d z}{d s} = \frac{\partial z}{\partial x} \frac{d x}{d s} + \frac{\partial z}{\partial y} \frac{d y}{d s}\)</span></p></li>
</ol>
<p>这样我们就可以想办法对神经网络进行求导。神经网络求系数的方法叫做 Backpropagation。假设我们这里要对多分类问题进行求导，我们整个模型的 loss function 定义为<span class="math inline">\(L(\theta) = \sum_{n=1}^N l_n(\theta)\)</span>，每一个 <span class="math inline">\(l\)</span> 是每个小分类的交叉熵。因此我们要求的就是<span class="math inline">\(\frac{\partial L(\theta)}{\partial w} = \sum_{n=1}^N \frac{\partial l_n(\theta)}{\partial w}\)</span>。</p>
<p>我们拿一个 neuron 来分析：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml026.png></p>
<p>那基于链式求导，我们可以知道，我们要求的是<span class="math inline">\(\frac{\partial l_n(\theta)}{\partial w}\)</span>，那么可以拆解为<span class="math inline">\(\frac{\partial z}{\partial w} \frac{\partial l_n(\theta)}{\partial z}\)</span>。因此Backpropagation 分为两个步骤，一个是 Forward（<span class="math inline">\(\frac{\partial z}{\partial w}\)</span>），一个是 Backward（<span class="math inline">\(\frac{\partial l_n(\theta)}{\partial w}\)</span>）。</p>
<p>首先是 Forward 部分，我们要计算<span class="math inline">\(\frac{\partial z}{\partial w}\)</span>，这个其实很好算，就是这一层的 input。</p>
<p>最后是 Backward 部分。我们假设 <span class="math inline">\(a = \sigma(z)\)</span>：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml027.png></p>
<p>那么<span class="math inline">\(\frac{\partial l}{\partial z} = \frac{\partial a}{\partial z} \frac{\partial l}{\partial a}\)</span>，而因为<span class="math inline">\(a\)</span>会影响后面的参数，因此 <span class="math inline">\(\frac{\partial l}{\partial a} = \frac{\partial z&#39;}{\partial a} \frac{\partial l}{\partial z&#39;} + \frac{\partial z&#39;&#39;}{\partial a} \frac{\partial l}{\partial z&#39;&#39;}\)</span>。那事实上，<span class="math inline">\(\frac{\partial z&#39;}{\partial a}\)</span>就是<span class="math inline">\(w_3\)</span>，其他相应可以算出来。如下图：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml028.png></p>
<p>我们现在假设<span class="math inline">\(\frac{\partial l}{\partial z&#39;}\)</span>和<span class="math inline">\(\frac{\partial l}{\partial z&#39;&#39;}\)</span>已知，那么<span class="math inline">\(\frac{\partial l}{\partial z} = \sigma&#39;(z)\Big[w_3 \frac{\partial l}{\partial z&#39;} + w_4 \frac{\partial l}{\partial z&#39;&#39;} \Big]\)</span>，这里用<span class="math inline">\(\sigma&#39;(z)\)</span>表示激活函数的导数。因此我们也可以将这个方程看成另一个类似神经网络的结构：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml029.png></p>
<p>如果这就是最后的output layer，那么<span class="math inline">\(\frac{\partial l}{\partial z&#39;} = \frac{\partial y_1}{\partial z&#39;} \frac{\partial l}{\partial y_1}\)</span>，其他同理可求。</p>
<p>如果这不是output layer，就一直算，一直算到最后的output layer。这样的方法计算量似乎非常大，那么为了提高运算效率，我们可以从output向前计算。所以整个的解法大概如下：</p>
<p><img data-src=https://raw.githubusercontent.com/SamaelChen/samaelchen.github.io/hexo/images/blog/ml030.png></p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
  <entry>
    <title>Progressive Growing of GANs</title>
    <url>/pytorch-pggan/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>虽然我没看完李嘉图，但是也没闲着呀，我还是在写pggan的呀。代号屁股gan计划。我是不会承认我想拿pggan去生成大长腿的。</p>
<span id="more"></span>
<p>这个GAN是NVIDIA在17年发表的<a href="https://arxiv.org/pdf/1710.10196.pdf">论文</a>，文章写的比较糙。一开始官方放出了Theano的版本，后来更新了基于TensorFlow的版本。都不是我喜欢的框架。然后就看到北大的一位很快做了一个PyTorch的版本。不过写的太复杂了，后面找到的其他版本基本上也写得跟官方的差不多复杂得一塌糊涂。最后找到一个我能看懂，并且很直观的实现方案：<a href="https://github.com/rosinality/progressive-gan-pytorch">https://github.com/rosinality/progressive-gan-pytorch</a>。然后我就在这个基础上进行修改，做成我比较舒服的脚本。</p>
<p>接下来把几个核心部分做个笔记。</p>
<h1 id="两个trick">两个trick</h1>
<h2 id="equalized-learning-rate">Equalized learning rate</h2>
<p>作者这里用了第一个trick，就是让每个weight的更新速度是一样的。用的公式是<span class="math inline">\(\hat{w_i} = w_i/c\)</span>。其中<span class="math inline">\(w_i\)</span>就是权重，而<span class="math inline">\(c\)</span>是每一层用何恺明标准化的一个常数。代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EqualLR</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, name</span>):</span></span><br><span class="line">        self.name = name</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_weight</span>(<span class="params">self, module</span>):</span></span><br><span class="line">        weight = <span class="built_in">getattr</span>(module, self.name + <span class="string">&#x27;_orig&#x27;</span>)</span><br><span class="line">        fan_in = weight.data.size(<span class="number">1</span>) * weight.data[<span class="number">0</span>][<span class="number">0</span>].numel()</span><br><span class="line">        <span class="keyword">return</span> weight * np.sqrt(<span class="number">2</span> / fan_in)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">apply</span>(<span class="params">module, name</span>):</span></span><br><span class="line">        fn = EqualLR(name)</span><br><span class="line">        weight = <span class="built_in">getattr</span>(module, name)</span><br><span class="line">        <span class="keyword">del</span> module._parameters[name]</span><br><span class="line">        module.register_parameter(name + <span class="string">&#x27;_orig&#x27;</span>, nn.Parameter(weight.data))</span><br><span class="line">        module.register_forward_pre_hook(fn)</span><br><span class="line">        <span class="keyword">return</span> fn</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__call__</span>(<span class="params">self, module, <span class="built_in">input</span></span>):</span></span><br><span class="line">        weight = self.compute_weight(module)</span><br><span class="line">        <span class="built_in">setattr</span>(module, self.name, weight)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">equal_lr</span>(<span class="params">module, name=<span class="string">&#x27;weight&#x27;</span></span>):</span></span><br><span class="line">    EqualLR.apply(module, name)</span><br><span class="line">    <span class="keyword">return</span> module</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EqualConv2d</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, *args, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        conv = nn.Conv2d(*args, **kwargs)</span><br><span class="line">        conv.weight.data.normal_()</span><br><span class="line">        conv.bias.data.zero_()</span><br><span class="line">        self.conv = equal_lr(conv)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">        <span class="keyword">return</span> self.conv(<span class="built_in">input</span>)</span><br></pre></td></tr></table></figure>
<p>这个很明显是原来作者写的啦，我大蟒蛇还没这么工程化的水平。</p>
<h2 id="pixelwise-normalization">Pixelwise normalization</h2>
<p>这个是在生成器中进行normalization。公式也很简单，就是<span class="math inline">\(b_{x,y} = a_{x,y} / \sqrt{\frac{1}{N} \sum_{j=0}{N-1}(a_{x,y}^j)^2 + \epsilon}\)</span>。其中<span class="math inline">\(\epsilon\)</span>是一个常数<span class="math inline">\(10^{-8}\)</span>，<span class="math inline">\(N\)</span>是有多少feature map，<span class="math inline">\(a_{x,y}和b_{x,y}\)</span>是原始feature vector和normalize后的feature vector。代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PixelNorm</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">input</span> / torch.sqrt(torch.mean(<span class="built_in">input</span> ** <span class="number">2</span>, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>) + <span class="number">1e-8</span>)</span><br></pre></td></tr></table></figure>
<p>这是两个文章重点提出来的trick。其他其实还有很多trick，不过是偏向设计网络结构的。</p>
<p>#PG-GAN主体</p>
<p>接下来就是最核心的部分，生成器和分类器。生成器和分类器的学习方法就是一步步放大图像的尺寸，从<span class="math inline">\(4\times 4\)</span>最后放大到<span class="math inline">\(1024 \times 1024\)</span>。生成器和分类器也是放大一次增加一个block。而这个block的设计也是参考了resnet，因为突然放大会导致模型不稳定，用这种方法可以平滑过渡。</p>
<p>然后就是PG-GAN和dcgan不一样的地方，dcgan放大的方式是用conv_transpose而PG-GAN用的是上采样的方法。<a href="https://distill.pub/2016/deconv-checkerboard/">Deconvolution and Checkerboard Artifacts</a>这篇文章讲了为什么用上采样更好，不过我没来得及细看。</p>
<p>所以我们先定义好一个block：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConvBlock</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, in_channel, out_channel, kernel1, pad1, kernel2, pad2, pixel_norm=<span class="literal">True</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.kernel1 = kernel1</span><br><span class="line">        self.kernel2 = kernel2</span><br><span class="line">        self.stride1 = <span class="number">1</span></span><br><span class="line">        self.stride2 = <span class="number">1</span></span><br><span class="line">        self.pad1 = pad1</span><br><span class="line">        self.pad2 = pad2</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> pixel_norm:</span><br><span class="line">            self.conv = nn.Sequential(EqualConv2d(in_channel, out_channel, self.kernel1, self.stride1, self.pad1),</span><br><span class="line">                                      PixelNorm(),</span><br><span class="line">                                      nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">                                      EqualConv2d(out_channel, out_channel, self.kernel2, self.stride2, self.pad2),</span><br><span class="line">                                      PixelNorm(),</span><br><span class="line">                                      nn.LeakyReLU(<span class="number">0.2</span>))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.conv = nn.Sequential(EqualConv2d(in_channel, out_channel, self.kernel1, self.stride1, self.pad1),</span><br><span class="line">                                      nn.LeakyReLU(<span class="number">0.2</span>),</span><br><span class="line">                                      EqualConv2d(out_channel, out_channel, self.kernel2, self.stride2, self.pad2),</span><br><span class="line">                                      nn.LeakyReLU(<span class="number">0.2</span>))</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span></span><br><span class="line">        out = self.conv(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<h2 id="generator">generator</h2>
<p>直接上代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Generator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, code_dim=<span class="number">512</span></span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.code_norm = PixelNorm()</span><br><span class="line">        self.progression = nn.ModuleList([ConvBlock(<span class="number">512</span>, <span class="number">512</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">1</span>),</span><br><span class="line">                                          ConvBlock(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>),</span><br><span class="line">                                          ConvBlock(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>),</span><br><span class="line">                                          ConvBlock(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>),</span><br><span class="line">                                          ConvBlock(<span class="number">512</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>),</span><br><span class="line">                                          ConvBlock(<span class="number">256</span>, <span class="number">128</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>)])</span><br><span class="line">        self.to_rgb = nn.ModuleList([nn.Conv2d(<span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>),</span><br><span class="line">                                     nn.Conv2d(<span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>),</span><br><span class="line">                                     nn.Conv2d(<span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>),</span><br><span class="line">                                     nn.Conv2d(<span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>),</span><br><span class="line">                                     nn.Conv2d(<span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>),</span><br><span class="line">                                     nn.Conv2d(<span class="number">128</span>, <span class="number">3</span>, <span class="number">1</span>),])</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>, expand=<span class="number">0</span>, alpha=-<span class="number">1</span></span>):</span></span><br><span class="line">        out = self.code_norm(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">for</span> i, (conv, to_rgb) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(self.progression, self.to_rgb)):</span><br><span class="line">            <span class="keyword">if</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> expand &gt; <span class="number">0</span>:</span><br><span class="line">                upsample = F.interpolate(out, scale_factor=<span class="number">2</span>)</span><br><span class="line">                out = conv(upsample)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                out = conv(out)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i == expand:</span><br><span class="line">                out = to_rgb(out)</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> <span class="number">0</span> &lt;= alpha &lt; <span class="number">1</span>:</span><br><span class="line">                    skip_rgb = self.to_rgb[i - <span class="number">1</span>](upsample)</span><br><span class="line">                    out = (<span class="number">1</span> - alpha) * skip_rgb + alpha * out</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>这个generator只定义到了<span class="math inline">\(128\times 128\)</span>这个分辨率的，要是想要增大分辨率可以参考文章最后的附录table 2的数据自己一个个加上去就好了，discriminator一样的操作就行。然后就是代码里面的这个skip_rgb，这个操作就是上面讲的平滑操作。</p>
<h2 id="discriminator">discriminator</h2>
<p>跟generator差不多。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Distriminator</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.progression = nn.ModuleList([ConvBlock(<span class="number">128</span>, <span class="number">256</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, pixel_norm=<span class="literal">False</span>),</span><br><span class="line">                                          ConvBlock(<span class="number">256</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, pixel_norm=<span class="literal">False</span>),</span><br><span class="line">                                          ConvBlock(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, pixel_norm=<span class="literal">False</span>),</span><br><span class="line">                                          ConvBlock(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, pixel_norm=<span class="literal">False</span>),</span><br><span class="line">                                          ConvBlock(<span class="number">512</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, pixel_norm=<span class="literal">False</span>),</span><br><span class="line">                                          ConvBlock(<span class="number">513</span>, <span class="number">512</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">0</span>, pixel_norm=<span class="literal">False</span>),])</span><br><span class="line">        self.from_rgb = nn.ModuleList([nn.Conv2d(<span class="number">3</span>, <span class="number">128</span>, <span class="number">1</span>),</span><br><span class="line">                                       nn.Conv2d(<span class="number">3</span>, <span class="number">256</span>, <span class="number">1</span>),</span><br><span class="line">                                       nn.Conv2d(<span class="number">3</span>, <span class="number">512</span>, <span class="number">1</span>),</span><br><span class="line">                                       nn.Conv2d(<span class="number">3</span>, <span class="number">512</span>, <span class="number">1</span>),</span><br><span class="line">                                       nn.Conv2d(<span class="number">3</span>, <span class="number">512</span>, <span class="number">1</span>),</span><br><span class="line">                                       nn.Conv2d(<span class="number">3</span>, <span class="number">512</span>, <span class="number">1</span>),])</span><br><span class="line">        self.n_layer = <span class="built_in">len</span>(self.progression)</span><br><span class="line">        self.linear = nn.Linear(<span class="number">512</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>, expand=<span class="number">0</span>, alpha=-<span class="number">1</span></span>):</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(expand, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">            index = self.n_layer - i - <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> i == expand:</span><br><span class="line">                out = self.from_rgb[index](<span class="built_in">input</span>)</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">                mean_std = <span class="built_in">input</span>.std(<span class="number">0</span>).mean()</span><br><span class="line">                mean_std = mean_std.expand(<span class="built_in">input</span>.size(<span class="number">0</span>), <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">                out = torch.cat([out, mean_std], <span class="number">1</span>)</span><br><span class="line">            out = self.progression[index](out)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> i &gt; <span class="number">0</span>:</span><br><span class="line">                out = F.avg_pool2d(out, <span class="number">2</span>)</span><br><span class="line">                <span class="keyword">if</span> i == expand <span class="keyword">and</span> <span class="number">0</span> &lt;= alpha &lt; <span class="number">1</span>:</span><br><span class="line">                    skip_rgb = F.avg_pool2d(<span class="built_in">input</span>, <span class="number">2</span>)</span><br><span class="line">                    skip_rgb = self.from_rgb[index + <span class="number">1</span>](skip_rgb)</span><br><span class="line">                    out = (<span class="number">1</span> - alpha) * skip_rgb + alpha * out</span><br><span class="line"></span><br><span class="line">        out = out.squeeze(<span class="number">2</span>).squeeze(<span class="number">2</span>)</span><br><span class="line">        out = self.linear(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p>然后mean_std这个地方就是文章里面的另一个trick，叫minibatch stddev，主要是用来增加差异性的，文章的第三部分。</p>
<p>最后只要按照wgan的方法训练就好了。不过还要注意一点的就是，wgan是discriminator训练5次，训练一次generator，而pggan是训一次discriminator，一次generator这样交替来。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">experiment_path = <span class="string">&#x27;checkpoint/pggan&#x27;</span></span><br><span class="line">img_list = []</span><br><span class="line">G_losses = []</span><br><span class="line">D_losses = []</span><br><span class="line">D_losses_tmp = []</span><br><span class="line">Grad_penalty = []</span><br><span class="line">i = <span class="number">0</span></span><br><span class="line">iters = <span class="number">0</span></span><br><span class="line">total_iters = <span class="number">0</span></span><br><span class="line">expand = <span class="number">0</span></span><br><span class="line">n_critic = <span class="number">1</span></span><br><span class="line">step = <span class="number">0</span></span><br><span class="line">alpha = <span class="number">0</span></span><br><span class="line">CLAMP = <span class="number">0.01</span></span><br><span class="line">one = torch.FloatTensor([<span class="number">1</span>]).cuda()</span><br><span class="line">mone = one * -<span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Training start!&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    <span class="keyword">if</span> epoch != <span class="number">0</span> <span class="keyword">and</span> epoch % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">        alpha = <span class="number">0</span></span><br><span class="line">        iters = <span class="number">0</span></span><br><span class="line">        expand += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> expand &gt;= <span class="number">3</span>:</span><br><span class="line">            batch_size = <span class="number">16</span></span><br><span class="line">        <span class="keyword">if</span> expand &gt; <span class="number">5</span>:</span><br><span class="line">            alpha = <span class="number">1</span></span><br><span class="line">            expand = <span class="number">5</span></span><br><span class="line">        dataset = modify_data(dataroot, image_size * <span class="number">2</span> ** expand)</span><br><span class="line">        dataloader = udata.DataLoader(dataset, batch_size=batch_size, shuffle=<span class="literal">True</span>, num_workers=workers)</span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">        real_cpu = data[<span class="number">0</span>].to(device)</span><br><span class="line">        b_size = real_cpu.size(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> step &lt; n_critic:</span><br><span class="line">            netD.zero_grad()</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> netD.parameters():</span><br><span class="line">                p.requires_grad = <span class="literal">True</span></span><br><span class="line"><span class="comment">#                 p.data.clamp_(-CLAMP, CLAMP)</span></span><br><span class="line">            output = netD(real_cpu, expand, alpha).view(-<span class="number">1</span>)</span><br><span class="line">            errD_real = (output.mean() - <span class="number">0.001</span> * (output ** <span class="number">2</span>).mean()).view(<span class="number">1</span>)</span><br><span class="line">            errD_real.backward(mone)</span><br><span class="line">            noise = torch.randn(b_size, nz, <span class="number">1</span>, <span class="number">1</span>, device=device)</span><br><span class="line">            fake = netG(noise, expand, alpha)</span><br><span class="line">            output = netD(fake.detach(), expand, alpha).view(-<span class="number">1</span>)</span><br><span class="line">            errD_fake = output.mean().view(<span class="number">1</span>)</span><br><span class="line">            errD_fake.backward(one)</span><br><span class="line">            eps = torch.rand(b_size, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, device=device)</span><br><span class="line">            x_hat = eps * real_cpu.data + (<span class="number">1</span> - eps) * fake.data</span><br><span class="line">            x_hat.requires_grad = <span class="literal">True</span></span><br><span class="line">            hat_predict = netD(x_hat, expand, alpha)</span><br><span class="line">            grad_x_hat = autograd.grad(outputs=hat_predict.<span class="built_in">sum</span>(), inputs=x_hat, create_graph=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line">            grad_penalty = ((grad_x_hat.view(grad_x_hat.size(<span class="number">0</span>), -<span class="number">1</span>).norm(<span class="number">2</span>, dim=<span class="number">1</span>) - <span class="number">1</span>) ** <span class="number">2</span>).mean()</span><br><span class="line">            grad_penalty = <span class="number">10</span> * grad_penalty</span><br><span class="line">            grad_penalty.backward()</span><br><span class="line">            errD = errD_real - errD_fake</span><br><span class="line">            d_optimizer.step()</span><br><span class="line">            D_losses_tmp.append(errD.item())</span><br><span class="line">            step += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> p <span class="keyword">in</span> netD.parameters():</span><br><span class="line">                p.requires_grad = <span class="literal">False</span></span><br><span class="line">            netG.zero_grad()</span><br><span class="line">            noise = torch.randn(b_size, nz, <span class="number">1</span>, <span class="number">1</span>, device=device)</span><br><span class="line">            fake = netG(noise, expand, alpha)</span><br><span class="line">            output = netD(fake, expand, alpha).view(-<span class="number">1</span>)</span><br><span class="line">            errG = -output.mean().view(<span class="number">1</span>)</span><br><span class="line">            errG.backward()</span><br><span class="line">            g_optimizer.step()</span><br><span class="line">            D_losses.append(np.mean(D_losses_tmp))</span><br><span class="line">            G_losses.append(errG.item())</span><br><span class="line">            D_losses_tmp = []</span><br><span class="line">            step = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> (total_iters+<span class="number">1</span>) % <span class="number">200</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[%d/%d][%d/%d](%d)\tLoss_D: %.4f\tLoss_G: %.4f\tD(x): %.4f\tD(G(z)): %.4f\tGrad: %.4f&#x27;</span></span><br><span class="line">                  % (epoch+<span class="number">1</span>, num_epochs, i+<span class="number">1</span>, <span class="built_in">len</span>(dataloader), total_iters + <span class="number">1</span>,</span><br><span class="line">                     errD.item(), errG.item(), errD_real.data.mean(), errD_fake.data.mean(), grad_penalty.data))</span><br><span class="line">        <span class="comment"># Check how the generator is doing by saving G&#x27;s output on fixed_noise</span></span><br><span class="line">        <span class="keyword">if</span> (total_iters % <span class="number">5000</span> == <span class="number">0</span>) <span class="keyword">or</span> ((epoch == num_epochs-<span class="number">1</span>) <span class="keyword">and</span> (i == <span class="built_in">len</span>(dataloader)-<span class="number">1</span>)):</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                fake = netG(fixed_noise, expand, alpha).detach().cpu()</span><br><span class="line">            img = vutils.make_grid(fake, padding=<span class="number">2</span>, normalize=<span class="literal">True</span>)</span><br><span class="line">            vutils.save_image(img, <span class="string">&#x27;checkpoint/pggan/fake_image/fake_iter_&#123;0&#125;.jpg&#x27;</span>.<span class="built_in">format</span>(total_iters))</span><br><span class="line">            img_list.append(img)</span><br><span class="line"></span><br><span class="line">        iters += <span class="number">1</span></span><br><span class="line">        total_iters += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> (epoch + <span class="number">1</span>) % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">            torch.save(netG.state_dict(), <span class="string">&#x27;&#123;0&#125;/netG_epoch_&#123;1&#125;.pth&#x27;</span>.<span class="built_in">format</span>(experiment_path, epoch+<span class="number">1</span>))</span><br><span class="line">            torch.save(netD.state_dict(), <span class="string">&#x27;&#123;0&#125;/netD_epoch_&#123;1&#125;.pth&#x27;</span>.<span class="built_in">format</span>(experiment_path, epoch+<span class="number">1</span>))</span><br></pre></td></tr></table></figure>
<p>然后这里面最要注意的是，wgan-gp里面用到了一个很重要的方法，就是gradient penalty，也就是训练里面的这一部分：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">eps = torch.rand(b_size, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, device=device)</span><br><span class="line">x_hat = eps * real_cpu.data + (<span class="number">1</span> - eps) * fake.data</span><br><span class="line">x_hat.requires_grad = <span class="literal">True</span></span><br><span class="line">hat_predict = netD(x_hat, expand, alpha)</span><br><span class="line">grad_x_hat = autograd.grad(outputs=hat_predict.<span class="built_in">sum</span>(), inputs=x_hat, create_graph=<span class="literal">True</span>)[<span class="number">0</span>]</span><br><span class="line">grad_penalty = ((grad_x_hat.view(grad_x_hat.size(<span class="number">0</span>), -<span class="number">1</span>).norm(<span class="number">2</span>, dim=<span class="number">1</span>) - <span class="number">1</span>) ** <span class="number">2</span>).mean()</span><br><span class="line">grad_penalty = <span class="number">10</span> * grad_penalty</span><br><span class="line">grad_penalty.backward()</span><br></pre></td></tr></table></figure>
<p>别的也就没什么了，坐等结果就好了。具体在我的<a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/pytorch/pggan/pggan-101.ipynb">notebook</a>里面。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>李宏毅深度学习作业——language model</title>
    <url>/pytorch_cloze/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>之前用LSTM做过情感分析，李宏毅老师17年的课程第一个大作业是做一个完形填空的language model，试着做了一个简单的demo。 <span id="more"></span></p>
<p>做完型填空其实很直观，就是跟CBOW很像，我们按照上下文猜被挖掉的那个词是什么。</p>
<p>这次用的还是之前训词向量的语料库，因为那个都是小说原文，所以我们要把数据揉成我们想要的形式，也就是context包含上下文，中间空掉的词是我们的target。</p>
<p>然后因为要训练LSTM，所以我们会再做一个padding的工作，最后看起来大概会是这样的： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tensor([[[ <span class="number">9405</span>,  <span class="number">1236</span>,  <span class="number">6282</span>,   <span class="number">371</span>,  <span class="number">1968</span>,     <span class="number">0</span>,     <span class="number">0</span>,     <span class="number">0</span>,     <span class="number">0</span>,     <span class="number">0</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">6085</span>, <span class="number">10586</span>,   <span class="number">900</span>,  <span class="number">7561</span>,     <span class="number">0</span>,     <span class="number">0</span>,     <span class="number">0</span>,     <span class="number">0</span>,     <span class="number">0</span>,     <span class="number">0</span>]]])</span><br></pre></td></tr></table></figure></p>
<p>形式上是<span class="math inline">\(2 \times \text{batch_size} \times \text{seq_len}\)</span>。</p>
<p>网络的设置非常简单，前半部分过一个LSTM，后半部分过一个LSTM，然后将这两个网络的output拼到一起最后过一个fc。</p>
<p>这里因为有可能完形填空的时候空的是第一个词或者是最后一个词，所以我们会在句子开头和结尾加上<bos>和<eos>的标志。</p>
<p>一个示例可以看这个<a href="‘https://github.com/SamaelChen/hexo-practice-code/blob/master/pytorch/langage%20model/LSTM-Full-text-Copy1.ipynb’">notebook</a>。</p>
<p>这个notebook的脚本没啥通用性，一个是其实没有解决unknown的词的问题，另外是没有解决训练效率的问题。PyTorch没有nce_loss或者是negative sampling这样的loss function，所以后面用softmax做cross entropy的时候复杂度是O(vocab_size)。之前写的negative sampling是针对word2vec写的，所以没什么通用性，看了其他人写的通用性的nce或者negative sampling，总感觉哪里怪怪的。后面还是要考虑自己实现一个。有点烦(╯﹏╰)。</p>
]]></content>
      <categories>
        <category>PyTorch</category>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>PyTorch实现LSTM情感分析</title>
    <url>/pytorch_lstm_sentiment/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>2018.08.16更新一个textCNN。</p>
<p>尝试使用LSTM做情感分析，这个gluon有非常详细的例子，可以直接参考gluon的<a href="http://zh.gluon.ai/chapter_natural-language-processing/sentiment-analysis.html">官方教程</a>。这里尝试使用PyTorch复现一个。数据用的是IMDB的数据<a href="http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz">http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz</a></p>
<span id="more"></span>
<p>首先我们导入相关的package：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.utils.data.dataloader <span class="keyword">as</span> dataloader</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> torch.autograd <span class="keyword">as</span> autograd</span><br><span class="line"><span class="keyword">import</span> torchtext.vocab <span class="keyword">as</span> torchvocab</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">import</span> gensim</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> snowballstemmer</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> chain</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br></pre></td></tr></table></figure>
<p>然后我们定义读数的函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readIMDB</span>(<span class="params">path, seg=<span class="string">&#x27;train&#x27;</span></span>):</span></span><br><span class="line">    pos_or_neg = [<span class="string">&#x27;pos&#x27;</span>, <span class="string">&#x27;neg&#x27;</span>]</span><br><span class="line">    data = []</span><br><span class="line">    <span class="keyword">for</span> label <span class="keyword">in</span> pos_or_neg:</span><br><span class="line">        files = os.listdir(os.path.join(path, seg, label))</span><br><span class="line">        <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(path, seg, label, file), <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf8&#x27;</span>) <span class="keyword">as</span> rf:</span><br><span class="line">                review = rf.read().replace(<span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">                <span class="keyword">if</span> label == <span class="string">&#x27;pos&#x27;</span>:</span><br><span class="line">                    data.append([review, <span class="number">1</span>])</span><br><span class="line">                <span class="keyword">elif</span> label == <span class="string">&#x27;neg&#x27;</span>:</span><br><span class="line">                    data.append([review, <span class="number">0</span>])</span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line">train_data = readIMDB(<span class="string">&#x27;aclImdb&#x27;</span>)</span><br><span class="line">test_data = readIMDB(<span class="string">&#x27;aclImdb&#x27;</span>, <span class="string">&#x27;test&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>接着是分词，这里只做非常简单的分词，也就是按照空格分词。当然按照一些传统的清洗方式效果会更好。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tokenizer</span>(<span class="params">text</span>):</span></span><br><span class="line">    <span class="keyword">return</span> [tok.lower() <span class="keyword">for</span> tok <span class="keyword">in</span> text.split(<span class="string">&#x27; &#x27;</span>)]</span><br><span class="line"></span><br><span class="line">train_tokenized = []</span><br><span class="line">test_tokenized = []</span><br><span class="line"><span class="keyword">for</span> review, score <span class="keyword">in</span> train_data:</span><br><span class="line">    train_tokenized.append(tokenizer(review))</span><br><span class="line"><span class="keyword">for</span> review, score <span class="keyword">in</span> test_data:</span><br><span class="line">    test_tokenized.append(tokenizer(review))</span><br><span class="line"></span><br><span class="line">vocab = <span class="built_in">set</span>(chain(*train_tokenized))</span><br><span class="line">vocab_size = <span class="built_in">len</span>(vocab)</span><br></pre></td></tr></table></figure>
<p>因为这个数据集非常小，所以如果我们用这个数据集做word embedding有可能过拟合，而且模型没有通用性，所以我们传入一个已经学好的word embedding。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">wvmodelwvmodel = gensim.models.KeyedVectors.load_word2vec_format(<span class="string">&#x27;test_word.txt&#x27;</span>,</span><br><span class="line">                                                          binary=<span class="literal">False</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p>这里的“test_word.txt”是我将glove的词向量转换后的结果，当时测试gensim的这个功能瞎起的名字，用的是glove的6B，100维的预训练数据。</p>
<p>然后一样要定义一个word to index的词典：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word_to_idxword_to  = &#123;word: i+<span class="number">1</span> <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)&#125;</span><br><span class="line">word_to_idx[<span class="string">&#x27;&lt;unk&gt;&#x27;</span>] = <span class="number">0</span></span><br><span class="line">idx_to_word = &#123;i+<span class="number">1</span>: word <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)&#125;</span><br><span class="line">idx_to_word[<span class="number">0</span>] = <span class="string">&#x27;&lt;unk&gt;&#x27;</span></span><br></pre></td></tr></table></figure>
<p>定义的目的是为了将预训练的weight跟我们的词库拼上。另外我们定义了一个unknown的词，也就是说没有出现在训练集里的词，我们都叫做unknown，词向量就定义为0。</p>
<p>然后就是编码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode_samples</span>(<span class="params">tokenized_samples, vocab</span>):</span></span><br><span class="line">    features = []</span><br><span class="line">    <span class="keyword">for</span> sample <span class="keyword">in</span> tokenized_samples:</span><br><span class="line">        feature = []</span><br><span class="line">        <span class="keyword">for</span> token <span class="keyword">in</span> sample:</span><br><span class="line">            <span class="keyword">if</span> token <span class="keyword">in</span> word_to_idx:</span><br><span class="line">                feature.append(word_to_idx[token])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                feature.append(<span class="number">0</span>)</span><br><span class="line">        features.append(feature)</span><br><span class="line">    <span class="keyword">return</span> features</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pad_samples</span>(<span class="params">features, maxlen=<span class="number">500</span>, PAD=<span class="number">0</span></span>):</span></span><br><span class="line">    padded_features = []</span><br><span class="line">    <span class="keyword">for</span> feature <span class="keyword">in</span> features:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(feature) &gt;= maxlen:</span><br><span class="line">            padded_feature = feature[:maxlen]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            padded_feature = feature</span><br><span class="line">            <span class="keyword">while</span>(<span class="built_in">len</span>(padded_feature) &lt; maxlen):</span><br><span class="line">                padded_feature.append(PAD)</span><br><span class="line">        padded_features.append(padded_feature)</span><br><span class="line">    <span class="keyword">return</span> padded_features</span><br></pre></td></tr></table></figure>
<p>我们这里为了解决评论长度不一致的问题，将所有的评论都取500个词，超过的就取前500个，不足的补0。</p>
<p>整理一下训练数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_features = torch.tensor(pad_samples(encode_samples(train_tokenized, vocab)))</span><br><span class="line">train_labels = torch.tensor([score <span class="keyword">for</span> _, score <span class="keyword">in</span> train_data])</span><br><span class="line">test_features = torch.tensor(pad_samples(encode_samples(test_tokenized, vocab)))</span><br><span class="line">test_labels = torch.tensor([score <span class="keyword">for</span> _, score <span class="keyword">in</span> test_data])</span><br></pre></td></tr></table></figure>
<p>然后就是定义网络：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SentimentNet</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embed_size, num_hiddens, num_layers,</span></span></span><br><span class="line"><span class="params"><span class="function">                 bidirectional, weight, labels, use_gpu, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(SentimentNet, self).__init__(**kwargs)</span><br><span class="line">        self.num_hiddens = num_hiddens</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.use_gpu = use_gpu</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(weight)</span><br><span class="line">        self.embedding.weight.requires_grad = <span class="literal">False</span></span><br><span class="line">        self.encoder = nn.LSTM(input_size=embed_size, hidden_size=self.num_hiddens,</span><br><span class="line">                               num_layers=num_layers, bidirectional=self.bidirectional,</span><br><span class="line">                               dropout=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> self.bidirectional:</span><br><span class="line">            self.decoder = nn.Linear(num_hiddens * <span class="number">4</span>, labels)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.decoder = nn.Linear(num_hiddens * <span class="number">2</span>, labels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        embeddings = self.embedding(inputs)</span><br><span class="line">        states, hidden = self.encoder(embeddings.permute([<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]))</span><br><span class="line">        encoding = torch.cat([states[<span class="number">0</span>], states[-<span class="number">1</span>]], dim=<span class="number">1</span>)</span><br><span class="line">        outputs = self.decoder(encoding)</span><br><span class="line">        <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure>
<p>那这里需要注意几个点，第一，LSTM可以不initialize hidden，如果不initialize的话，那么PyTorch会默认初始为0。</p>
<p>另外就是LSTM这里传进去的数据格式是[seq_len, batch_size, embedded_size]。而我们传进去的数据是[batch_size, seq_len]的样子，那经过embedding之后的结果是[batch_size, seq_len, embedded_size]。所以我们这里要将第二个维度和第一个维度做个调换。而LSTM这边output的dimension和inputs是一致的，如果这里我们不做维度的调换，可以将LSTM的batch_first参数设置为True。然后我们要拿到每个batch的初始状态和最后状态还是一样要去做一个第一第二维度的调换。这里非常的绕，我在这里卡了好久(=<span class="citation" data-cites="__">@__</span>@=)</p>
<p>第三就是我这里用了最初始的状态和最后的状态拼起来作为分类的输入。</p>
<p>另外有一点吐槽的就是，MXNet的dense层比较强大啊，不用定义输入的维度，只要定义输出的维度就可以了，操作比较骚啊。</p>
<p>然后我们把weight导进来：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">weight = torch.zeros(vocab_size+<span class="number">1</span>, embed_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(wvmodel.index2word)):</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        index = word_to_idx[wvmodel.index2word[i]]</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    weight[index, :] = torch.from_numpy(wvmodel.get_vector(</span><br><span class="line">        idx_to_word[word_to_idx[wvmodel.index2word[i]]]))</span><br></pre></td></tr></table></figure>
<p>这里我们将不在glove里面的词全部填为0，后面想了一下，其实也可以试试这些全部随机试试。</p>
<p>接着定义参数就可以训练了。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">embed_size = <span class="number">100</span></span><br><span class="line">num_hiddens = <span class="number">100</span></span><br><span class="line">num_layers = <span class="number">2</span></span><br><span class="line">bidirectional = <span class="literal">True</span></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">labels = <span class="number">2</span></span><br><span class="line">lr = <span class="number">0.8</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">use_gpu = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">net = SentimentNet(vocab_size=(vocab_size+<span class="number">1</span>), embed_size=embed_size,</span><br><span class="line">                   num_hiddens=num_hiddens, num_layers=num_layers,</span><br><span class="line">                   bidirectional=bidirectional, weight=weight,</span><br><span class="line">                   labels=labels, use_gpu=use_gpu)</span><br><span class="line">net.to(device)</span><br><span class="line">loss_function = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=lr)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_set = torch.utils.data.TensorDataset(train_features, train_labels)</span><br><span class="line">test_set = torch.utils.data.TensorDataset(test_features, test_labels)</span><br><span class="line"></span><br><span class="line">train_iter = torch.utils.data.DataLoader(train_set, batch_size=batch_size,</span><br><span class="line">                                         shuffle=<span class="literal">True</span>)</span><br><span class="line">test_iter = torch.utils.data.DataLoader(test_set, batch_size=batch_size,</span><br><span class="line">                                        shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<p>这个位置需要注意的是，我们在train加了一个shuffle，如果不加shuffle的话，模型会学到奇奇怪怪的地方去。</p>
<p>最后训练一下就好了</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epochs):</span><br><span class="line">    start = time.time()</span><br><span class="line">    train_loss, test_losses = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    train_acc, test_acc = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    n, m = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> feature, label <span class="keyword">in</span> train_iter:</span><br><span class="line">        n += <span class="number">1</span></span><br><span class="line">        net.zero_grad()</span><br><span class="line">        feature = Variable(feature.cuda())</span><br><span class="line">        label = Variable(label.cuda())</span><br><span class="line">        score = net(feature)</span><br><span class="line">        loss = loss_function(score, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        train_acc += accuracy_score(torch.argmax(score.cpu().data,</span><br><span class="line">                                                 dim=<span class="number">1</span>), label.cpu())</span><br><span class="line">        train_loss += loss</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> test_feature, test_label <span class="keyword">in</span> test_iter:</span><br><span class="line">            m += <span class="number">1</span></span><br><span class="line">            test_feature = test_feature.cuda()</span><br><span class="line">            test_label = test_label.cuda()</span><br><span class="line">            test_score = net(test_feature)</span><br><span class="line">            test_loss = loss_function(test_score, test_label)</span><br><span class="line">            test_acc += accuracy_score(torch.argmax(test_score.cpu().data,</span><br><span class="line">                                                    dim=<span class="number">1</span>), test_label.cpu())</span><br><span class="line">            test_losses += test_loss</span><br><span class="line">    end = time.time()</span><br><span class="line">    runtime = end - start</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch: %d, train loss: %.4f, train acc: %.2f, test loss: %.4f, test acc: %.2f, time: %.2f&#x27;</span> %</span><br><span class="line">          (epoch, train_loss.data / n, train_acc / n, test_losses.data / m, test_acc / m, runtime))</span><br></pre></td></tr></table></figure>
<p>也可以直接看我的<a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/pytorch/langage%20model/lstm-sentiment.ipynb">notebook</a></p>
<p>后面试试textCNN，感觉也挺骚气的。</p>
<hr />
<p>2018.08.16 更新一个textCNN的玩法。</p>
<p>CNN太熟了，很容易搞，其实只要把网络改一下，其他的动都不用动：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">textCNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embed_size, seq_len, labels, weight, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(textCNN, self).__init__(**kwargs)</span><br><span class="line">        self.labels = labels</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(weight)</span><br><span class="line">        self.embedding.weight.requires_grad = <span class="literal">False</span></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, (<span class="number">3</span>, embed_size))</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, (<span class="number">4</span>, embed_size))</span><br><span class="line">        self.conv3 = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, (<span class="number">5</span>, embed_size))</span><br><span class="line">        self.pool1 = nn.MaxPool2d((seq_len - <span class="number">3</span> + <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.pool2 = nn.MaxPool2d((seq_len - <span class="number">4</span> + <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.pool3 = nn.MaxPool2d((seq_len - <span class="number">5</span> + <span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        self.linear = nn.Linear(<span class="number">3</span>, labels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        inputs = self.embedding(inputs).view(inputs.shape[<span class="number">0</span>], <span class="number">1</span>, inputs.shape[<span class="number">1</span>], -<span class="number">1</span>)</span><br><span class="line">        x1 = F.relu(self.conv1(inputs))</span><br><span class="line">        x2 = F.relu(self.conv2(inputs))</span><br><span class="line">        x3 = F.relu(self.conv3(inputs))</span><br><span class="line"></span><br><span class="line">        x1 = self.pool1(x1)</span><br><span class="line">        x2 = self.pool2(x2)</span><br><span class="line">        x3 = self.pool3(x3)</span><br><span class="line"></span><br><span class="line">        x = torch.cat((x1, x2, x3), -<span class="number">1</span>)</span><br><span class="line">        x = x.view(inputs.shape[<span class="number">0</span>], <span class="number">1</span>, -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        x = self.linear(x)</span><br><span class="line">        x = x.view(-<span class="number">1</span>, self.labels)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span>(x)</span><br></pre></td></tr></table></figure>
<p>这里的网络设计很简单，就是用三个filter去扫一遍文章，filter的尺寸其实就是我们一次看多少个词。这样扫完以后是三个向量，然后pooling一下得到三个实数。把这三个实数拼成一个向量，然后用fc分类一下就结束了。</p>
<p>然后初始化网络：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">net = textCNN(vocab_size=(vocab_size+<span class="number">1</span>), embed_size=embed_size,</span><br><span class="line">              seq_len=<span class="number">500</span>, labels=labels, weight=weight)</span><br></pre></td></tr></table></figure>
<p>其他的都没改，就可以直接跑了。速度上CNN比LSTM的参数少，速度快很多，不过只跑几轮的话效果差一点。</p>
]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
  </entry>
  <entry>
    <title>NER</title>
    <url>/ner/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>暌违一年的更新， 最近用到NER相关的算法，简单记录一下，主要是HMM和CRF。感觉概率图比较牛逼。</p>
<span id="more"></span>
<h1 id="ner发展">NER发展</h1>
<p>NER是NLP里面一个非常基础的任务，从NLP的处理流程上看，NER可以看做是词法分析中未登录词的一种。同时NER也是关系抽取、事件抽取、知识图谱、机器翻译、问答系统等诸多NLP任务的基础。一般而言NER主要是识别人们、地名、组织机构等，常见的NER算法大赛就是这样。实际上任何我们想要的特殊文本片段都可以认为是实体。比如价格、产品型号等。</p>
<p>NER的发展基本上是四个阶段，最初是基于规则和字典的方法，依赖专家构建复杂的词库，通过分词器和正则表达式等方式抽取。第二阶段就是以HMM和CRF为代表的机器学习时代，第三阶段是CNN+CRF或者RNN+CRF的方式，第四阶段也就是现在基本上是半监督或者Attention等深度学习方法。</p>
<h1 id="马尔科夫链">马尔科夫链</h1>
<p>一般而言，我们假设<span class="math inline">\(X\)</span>是一个随机数据集合<span class="math inline">\(\{X_1, X_2, \ldots, X_t\}\)</span>，这些值源自状态集合<span class="math inline">\(S=\{s_1, \ldots, s_N\}\)</span>。一个马尔科夫链满足下面两个条件： <span class="math display">\[
\begin{matrix}
P(X_{t+1} = s_k|X_1,\ldots,X_t) = P(X_{t+1} = s_k|X_t) &amp; \text{Limited horizon} \\
P(X_2=s_k|X_1=s_j) = P(X_{t+1} = s_k|X_t = s_j), \forall t,k,j &amp; \text{Time invariant}
\end{matrix}
\]</span> 一个马尔科夫链会有一个转移矩阵来表示从每一个状态转移到下一个状态的概率，同时有一个初始概率来表示第一个时刻每个状态的概率。假设我们有两个状态0和1，有一个转移矩阵： <span class="math display">\[
\begin{array}
{|c|c|c|} \hline \ &amp; 0 &amp; 1 \\
\hline
0 &amp; 0.3 &amp; 0.7 \\
\hline
1 &amp; 0.6 &amp; 0.4 \\
\hline
\end{array}
\]</span> 初始概率<span class="math inline">\(P(S = 0)=0.2, P(S=1)=0.8\)</span>，那么对于序列1011，我们就可以很容易算出来概率是<span class="math inline">\(0.8 \times 0.6 \times 0.7 \times 0.4=0.1344\)</span></p>
<h1 id="hmm">HMM</h1>
<p>那么隐马尔可夫又是什么呢？上面的马尔科夫是一个可以直接观测到的状态转移序列。那么现在存在一种序列，表面上是我们可以观测到的随机序列，但是背后却有我们无法得知的隐藏序列来生成这一个序列。比如恋爱的经典笑话。 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">男：你怎么了？</span><br><span class="line">女：没事。</span><br><span class="line">男：你真的没事？</span><br><span class="line">女：真的，你睡吧。</span><br><span class="line">男：你确定没事？</span><br><span class="line">女：真的。</span><br><span class="line">男：好吧，那我睡了。</span><br><span class="line">转头女的发了朋友圈，终究还是一个人扛下了所有。</span><br><span class="line">男：到底发生了什么。</span><br><span class="line">女：没事。</span><br><span class="line">男：你不说我没法睡觉。</span><br><span class="line">女：你睡你的。</span><br><span class="line">男：好吧。</span><br><span class="line">女的发了第二条朋友圈，果然还是没有人理解我。</span><br></pre></td></tr></table></figure> 于是，”没事-真的没事-真的-没事”这种序列背后隐藏了怎样的序列呢？马尔科夫链无法解决，所以需要HMM这样的模型来学习隐藏的状态序列。</p>
<p>一个HMM有两个序列，一个是观测序列<span class="math inline">\(O\)</span>，一个是隐藏序列<span class="math inline">\(H\)</span>。HMM要满足以下假设： <span class="math display">\[
\begin{cases}
P(H_t=h_t|H_{1:t-1}=h_{1:t-1}, O_{1:t} = o_{1:t}) = P(H_t=h_t | H_{t-1} = h_{t-1}) &amp; \text{Markovinanity} \\
P(O_t = o_t|H_{1:t} = h_{1:t}, O_{1:t-1}=o_{1:t-1}) = P(O_t=o_t|H_t=h_t) &amp; \text{Output independence} \\
P(H_t=j|H_{t-1}=i) = P(H{t+s}=j|H_{t+s-1}=i), \forall i,j \in H &amp; \text{Stationarity}
\end{cases}
\]</span></p>
<p>一个完整的HMM包含三个要素，transition matrix <span class="math inline">\(A\)</span>，emission matrix <span class="math inline">\(B\)</span>，还有初始状态分布概率<span class="math inline">\(\Pi\)</span>，可以将HMM表示为<span class="math inline">\(\lambda = (A, B, \Pi)\)</span>。</p>
<p>那么HMM就有三个问题需要解决，一个是概率计算问题，也就是likelihood，第二个是参数学习问题，第三个是序列的解码问题。</p>
<h2 id="hmm-likelihood">HMM likelihood</h2>
<p>要计算一个HMM生成序列的概率，首先想到的就是暴力解法，穷举所有可能状态的组合，那么通过暴力运算就可以将所有的可能性算出来。但是暴力运算的问题在于计算复杂度过高，复杂度达到<span class="math inline">\(O(TN^T)\)</span>。所以一般解法有两种，一种是前向算法，另一种是后向算法。</p>
<p>前向算法的过程很简单，首先初始化各个状态下在时间1时候观测状态为o_1的概率，<span class="math inline">\(\alpha(i) = \pi_i b_i(o_1)\)</span>，然后递归求解，<span class="math inline">\(\alpha_{t+1}(j) = \Big[ \sum\limits_{i=1}^N \alpha_t a_{ij} \Big] b_j(o_{t+1})\)</span>，最后到了<span class="math inline">\(T\)</span>时刻，<span class="math inline">\(P(O|\lambda) = \sum\limits_{i=1}^N \alpha_T(i)\)</span>。这样的话复杂度就降低到了<span class="math inline">\(O(TN^2)\)</span>的水平。因为每次只计算两个时刻之间的所有可能性。</p>
<p>这里演示一个简单的前向算法计算过程，假设有红白两种颜色的球，分别有三个盒子。我们可以观测到的球的颜色，隐藏的是球来自哪个盒子。初始概率<span class="math inline">\(\Pi = (0.2, 0.4, 0.4)\)</span>，transition matrix <span class="math inline">\(A = \begin{bmatrix} 0.5 &amp; 0.2 &amp; 0.3 \\ 0.3 &amp; 0.5 &amp; 0.2 \\ 0.2 &amp; 0.3 &amp; 0.5 \end{bmatrix}\)</span>，emission matrix <span class="math inline">\(\begin{bmatrix}0.5 &amp; 0.5 \\ 0.4 &amp; 0.6 \\ 0.7 &amp; 0.3 \end{bmatrix}\)</span>，观测到的序列是<span class="math inline">\(O=\{红，白，红\}\)</span>，所以分步计算如下：</p>
<p>第一步，初始化。 <span class="math display">\[\alpha_1(1) = \pi_1 b_1(o_1) = 0.2 \times 0.5 = 0.1, \ \alpha_1(2) = \pi_2 b_2(o_1) = 0.4 \times 0.4 = 0.16, \ \alpha_1(3) = \pi_3 b_3(o_1) = 0.4 \times 0.7 = 0.28\]</span></p>
<p>第二步，递归。时刻2的观测状态是白球，所以时刻2来自盒子1的概率是<span class="math display">\[\alpha_2(1) = \Big[\sum\limits_{i=1}^3 \alpha_1(i) a_{i1}\Big] b_2(o_2) = (0.1 \times 0.5 + 0.16 \times 0.3 + 0.28 \times 0.2) \times 0.5 = 0.077\]</span>其他盒子类推，得到<span class="math display">\[\alpha_2(2) = 0.1104, \ \alpha_2(3) = 0.0606\]</span> 重复第二步，<span class="math display">\[\alpha_3(1) = 0.04187, \ \alpha_3(2) = 0.03551, \ \alpha_3(3) = 0.05284\]</span></p>
<p>最后我们得到序列的概率<span class="math inline">\(P(O|\lambda) = \sum\limits_{i=1}^3 \alpha_3(i) = 0.13022\)</span>。</p>
<p>那么后向算法与前向算法类似，但是计算起来相对比较反直觉一点。一样的初始化每个状态最后一个时刻的概率<span class="math inline">\(\beta_T(i) = 1, i=1, 2, \ldots, N\)</span>。接着根据<span class="math inline">\(t+1\)</span>时刻的后向概率，递归计算前一个时刻每个隐藏状态的后向概率。也就是<span class="math inline">\(\beta_t(i) = \sum\limits_{j=1}^N a_{ij}b_j(o_{t+1}) \beta_{t+1}(j), i=1,2,\ldots,N\)</span>。最后<span class="math inline">\(P(O|\lambda) = \sum\limits_{i=1}^N \pi_i b_i(o_1) \beta_1(i)\)</span>。</p>
<p>一般来说用一个解法来算概率就好了，可以将这两种统一到一个公式上，也就是<span class="math inline">\(P(O|\lambda) = \sum\limits_{i=1}^N \sum\limits_{i=1}^N \alpha_{t}(i) a_{ij}b_j(o_{t+1}) \beta_{t+1}(j), t=1,2,\ldots,T-1\)</span>。</p>
<h2 id="hmm-learning-problem">HMM learning problem</h2>
<p>HMM的参数学习有两种一种是有监督学习，一种是无监督学习。</p>
<p>有监督学习比较简单，因为HMM是生成模型，所以有监督学习直接根据标注的隐藏状态计算频率就可以了。也就是<span class="math inline">\(a_{ij} = \frac{A_{ij}}{\sum_{j=1}^N A_{ij}}, i=1,2,\ldots,N; j=1,2,\ldots,N\)</span>，<span class="math inline">\(b_i(k) = \frac{B_{ik}}{\sum_{k=1}^M B_{ik}}, i=1,2,\ldots,N;\)</span>，<span class="math inline">\(\pi_i = \frac{Count(h_i)}{\sum_{j=1}^N Count(h_j)}\)</span>。</p>
<p>另一种是用EM算法做无监督学习。一般HMM用的是Baum-Welch算法。</p>
<p>EM算法就包括了两个步骤，一个是E，一个是M。我们假设有一个数据集合是<span class="math inline">\(\{O_1, O_2, \ldots, O_S\}\)</span>，<span class="math inline">\(O_i = o_{i_1}, o_{i_2}, \ldots, o_{i_T}\)</span>，<span class="math inline">\(H_i = h_{i_1}, h_{i_2}, \ldots, h_{i_T}\)</span>，为了方便区分，下面用上标来表示隐藏状态的index。<span class="math inline">\(O=\{o^1, o^2, \ldots, o^M\}\)</span>，<span class="math inline">\(H=\{h^1, h^2, \ldots, h^N\}\)</span>。那么E步就是计算<span class="math display">\[Q(\lambda, \bar{\lambda}) = \sum\limits_{H} P(H|O,\bar{\lambda}) \log P(O,H|\bar{\lambda})\]</span> M步就是找到一个<span class="math inline">\(\bar{\lambda}\)</span>使得上面的期望最大，也就是 <span class="math display">\[
\bar{\lambda} = \arg \max_{\lambda} \sum\limits_H P(H|O,\bar{\lambda})\log P(O,H|\lambda)
\]</span></p>
<p>那么<span class="math inline">\(Q\)</span>函数可以改写成： <span class="math display">\[
\sum\limits_{H} P(H|O,\bar{\lambda}) \log P(O,H|\bar{\lambda}) = \sum\limits_{H} \frac{P(H,O|\bar{\lambda})}{P(O|\bar{\lambda})} \log P(O,H|\lambda)
\]</span> 因为P(O|{})是常数，所以上面等价于 <span class="math display">\[
\sum\limits_{H} P(H,O|\bar{\lambda}) \log P(O,H|\bar{\lambda})
\]</span> 因为<span class="math inline">\(P(O,H|\lambda) = \pi_{h_1}b_{h_1}(o_1)a_{h_1h_2}b_{h_2}(o_2) \cdots a_{h_{T-1}h_T}b_{h_T}(o_T)\)</span>，所以最后将公式可以替换为： <span class="math display">\[
Q(\lambda, \bar{\lambda}) = \sum\limits_{H}P(O,H|\bar{\lambda}) \log \pi_{h_1} + \sum\limits_{H}(\sum\limits_{t=1}^{T-1} \log a_{h_t h_{t+1}})P(O,H|\bar{\lambda}) + \sum\limits_{H}(\sum\limits_{t=1}^T \log b_{h_1}(o_t))P(O,H|\bar{\lambda})
\]</span></p>
<p>那么分步求偏导，我们对第一个部分求偏导， <span class="math display">\[
\sum\limits_{H} \log \pi_{h_1} P(O,H| \bar{\lambda}) = \sum\limits_{i=1}^N \log \pi^{i} P(O, h_1 = h^i | \bar{\lambda})
\]</span> 由于<span class="math inline">\(\sum_{i=1}^N \pi^i = 1\)</span>，所以这是受限制的求解极值问题，用拉格朗日乘子法构建拉格朗日函数如下： <span class="math display">\[
\sum\limits_{i=1}^N \log \pi^i P(O,h_1 = h^i | \bar{\lambda}) + \gamma(\sum\limits_{i=1}^N \pi^i - 1)
\]</span> 接着求导： <span class="math display">\[
\frac{\partial}{\partial\pi^i}[\sum\limits_{i=1}^N\log\pi^i P(O,h_1=h^i|\bar{\lambda})+\gamma(\sum\limits_{i=1}^N\pi^i-1)]=P(O,h_1=h^i|\bar{\lambda})+\gamma\pi^i
\]</span> 让上式等0，且因为有N个，全部求和就可以得到<span class="math inline">\(\gamma\)</span>值也就是<span class="math inline">\(\gamma=-P(O|\bar{\lambda})\)</span>。</p>
<p>所以<span class="math inline">\(\pi^i = \frac{P(O,h_1 = h^i|\bar{\lambda})}{P(O|\bar{\lambda})}\)</span>。</p>
<p>然后按照一样的方法求第二部分： <span class="math display">\[
\sum\limits_{H}(\sum\limits_{t=1}^{T-1}\log a_{h_th_{t+1}})P(O,H|\bar{\lambda})=\sum\limits_{i=1}^N\sum\limits_{j=1}^N\sum\limits_{t=1}^{T-1}\log a_{ij}P(O,h_t=h^i,h_{t+1}=h^j|\bar{\lambda}),\sum\limits_{j=1}^N a_{ij}=1
\]</span></p>
<p>我们设定拉格朗日函数为 <span class="math display">\[
L=\sum\limits_{i=1}^N\sum\limits_{j=1}^N\sum\limits_{t=1}^{T-1}\log a_{ij}P(O,h_t=h^i,h_{t+1}=h^j|\bar{\lambda})+\sum\limits_{i=1}^N\gamma_i(\sum\limits_{j=1}^N a_{ij}-1)
\]</span></p>
<p>然后一样求偏导 <span class="math display">\[
\frac{\partial L}{\partial a_{ij}}=\sum\limits_{t=1}^{T-1}P(O,h_t=h^i,h_{t+1}=h^j|\bar{\lambda})+a_{ij}\sum\limits_{i=1}^N\gamma_i=0
\]</span></p>
<p>同样通过求和得到<span class="math inline">\(\sum\limits_{i=1}^N\gamma_i = -\sum\limits_{j=1}^N\sum\limits_{t=1}^{T-1}P(O,h_t=h^i,h_{t+1}=h^j|\bar{\lambda})\)</span></p>
<p>最后得到 <span class="math display">\[
a_{ij}=\frac{\sum\limits_{t=1}^{T-1}P(O,h_t=h^i,h_{t+1}=h^j|\bar{\lambda})}{\sum\limits_{j=1}^N\sum\limits_{t=1}^{T-1}P(O,h_t=h^i,h_{t+1}=h^j|\bar{\lambda})}=\frac{\sum\limits_{t=1}^{T-1}P(O,h_t=h^i,h_{t+1}=h^j|\bar{\lambda})}{\sum\limits_{t=1}^{T-1}P(O,h_t=h^i|\bar{\lambda})}
\]</span></p>
<p>现在求最后一部分 <span class="math display">\[
\sum\limits_{H}(\sum\limits_{t=1}^{T}\log b_{h_t}(o_t)) P(O,H|\bar{\lambda})=\sum\limits_{i=1}^N \sum\limits_{k=1}^M \sum\limits_{t=1}^T \log b_{ik}P(O,h_t=h^i,o_t=o^k|\bar{\lambda}),\sum\limits_{k=1}^M b_{ik}=1
\]</span></p>
<p>构造拉格朗日函数 <span class="math display">\[
L=\sum\limits_{i=1}^N \sum\limits_{k=1}^M \sum\limits_{t=1}^T \log b_{ik}P(O,h_t=h^i,o_t=o^k|\bar{\lambda})+\sum\limits_{i=1}^N \gamma_i(\sum\limits_{k=1}^Mb_{jk}-1)
\]</span></p>
<p>求偏导 <span class="math display">\[
\frac{\partial L}{\partial b_{ik}}=\sum\limits_{t=1}^T P(O,h_t=h^i,o_t=o^k|\bar{\lambda})+b_{ik}\sum\limits_{i=1}^N\gamma_i=0
\]</span></p>
<p>求和得到<span class="math inline">\(\sum\limits_{i=1}^N\gamma_i = -\sum\limits_{k=1}^M\sum\limits_{t=1}^T P(O,h_t=h^i,o_t=o^k|\bar{\lambda})\)</span></p>
<p>最后可以得到 <span class="math display">\[
b_{ik}=\frac{\sum\limits_{t=1}^T P(O,h_t=h^i,o_t=o^k|\bar{\lambda})}{\sum\limits_{k=1}^M\sum\limits_{t=1}^T P(O,h_t=h^i,o_t=o^k|\bar{\lambda})}=\frac{\sum\limits_{t=1}^T P(O,h_t=h^i,o_t=o^k|\bar{\lambda})}{\sum\limits_{t=1}^T P(O,h_t=h^i|\bar{\lambda})}
\]</span></p>
<p>按照之前的向前先后算法，计算<span class="math inline">\(t\)</span>时刻处于隐藏状态<span class="math inline">\(h^i\)</span>的概率为<span class="math inline">\(\gamma\)</span>，而<span class="math inline">\(\xi\)</span>表示<span class="math inline">\(t\)</span>时刻从<span class="math inline">\(h^i\)</span>转移到<span class="math inline">\(h^j\)</span>的概率。 <span class="math display">\[
P(O,h_t=h^i|\lambda)=\gamma_t(i)=\frac{\alpha_t(i)\beta_t(i)}{\sum_{j=1}^N\alpha_t(j)\beta_t(j)}
\]</span></p>
<p><span class="math display">\[
P(O,h_t=h^i,h_{t+1}=h^j|\lambda)=\xi_t(i,j)=\frac{\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}{\sum\limits_{i=1}^N\sum\limits_{j=1}^N\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}
\]</span></p>
<p>将上面的两个式子带入之前的偏导结果里面，就得到我们想要的参数了。</p>
<h2 id="hmm-decoding-problem">HMM decoding problem</h2>
<p>最后就是HMM的解码问题，一般来说解码都是用viterbi算法来完成。实践上就是每一步都取最大的可能性，然后记下上一个时刻是哪一个隐藏状态有最大的可能性转移到当前状态。</p>
<p>过程就是</p>
<p>1、<span class="math inline">\(\delta_1(i) = \pi_i b_i(o_1), i = 1,2,\ldots, N \\ \psi_1(i) = 0, i = 1, 2, \ldots, N\)</span></p>
<p>2、<span class="math inline">\(\delta_t(i) = \max\limits_{1 \leqslant j \leqslant N}[\delta_{t-1}(j) \alpha_{ji}] b_i(o_t) \\ \psi_t(i) = \arg \max\limits_{1 \leqslant j \leqslant N}[\delta_{t-1}(j)\alpha_{ji}]\)</span></p>
<p>3、<span class="math inline">\(P = \max\limits_{1 \leqslant j \leqslant N} \delta_T(i) \\ i_T = \arg\max\limits_{1 \leqslant j \leqslant N}[\delta_T(i)]\)</span></p>
<h1 id="crf">CRF</h1>
<p>CRF与HMM要解决的问题是类似的，都是要从观测序列中推测出隐藏序列。与HMM不同，CRF是一个典型的有监督学习算法。同样的CRF有一个transition matrix和一个emission matrix。</p>
<p>CRF要优化的loss函数就是一个序列生成的最大概率。 <span class="math display">\[
P(y|x) = \frac{1}{Z(x)} \exp \Big(\sum\limits_{i,k} \lambda_k t_k(y_{i-1},y_i, x,i) +\sum\limits_{i,l}\mu_ls_l(y_i, x,i)\Big) \\
Z(x) =\sum\limits_{y} \exp \Big(\sum\limits_{i,k} \lambda_k t_k(y_{i-1},y_i, x,i) +\sum\limits_{i,l}\mu_ls_l(y_i, x,i)\Big)
\]</span></p>
<p>CRF的序列概率计算方式与HMM类似，也是前向-后向算法。这里举一个简单的例子来说明，假设有三个单词，两个隐藏状态。那么emission matrix和transition matrix如下： <span class="math display">\[
\begin{array}
{|c|c|c|} \hline \ &amp; l_1 &amp; l_2 \\
\hline
w_0 &amp; x_{01} &amp; x_{02} \\
\hline
w_1 &amp; x_{11} &amp; x_{12} \\
\hline
w_2 &amp; x_{21} &amp; x_{22} \\
\hline
\end{array}
\]</span></p>
<p><span class="math display">\[
\begin{array}
{|c|c|c|} \hline \ &amp; l_1 &amp; l_2 \\
\hline
l_1 &amp; t_{11} &amp; t_{12} \\
\hline
l_2 &amp; t_{21} &amp; t_{22} \\
\hline
\end{array}
\]</span></p>
<p>现在一步一步来前向传播运算序列概率，目标是算出<span class="math inline">\(log(e^{S_1} + e^{S_2} + \ldots + e^{S_n})\)</span>。</p>
<p>首先第一步，第一个词是<span class="math inline">\(w_0\)</span>，我们有两个变量，<span class="math inline">\(obs = [x_{01}, x_{02}]\)</span>, <span class="math inline">\(previous = None\)</span>，所以<span class="math inline">\(\text{total_score} = \log(e^{x_{01}} + e^{x_{02}})\)</span>。</p>
<p>第二步从<span class="math inline">\(w_0 \to w_1\)</span>，<span class="math inline">\(obs = [x_{11}, x_{12}]\)</span>，<span class="math inline">\(previous = [x_{01}, x_{02}]\)</span>，接下来为了计算方便，我们对obs和previous做一个broadcast，得到下面的结果： <span class="math display">\[
previous =
\begin{bmatrix}
x_{01} &amp; x_{01} \\
x_{02} &amp; x_{02}
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
obs = \begin{bmatrix}
x_{11} &amp; x_{12} \\
x_{11} &amp; x_{12}
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
score = previous + obs + transition = \begin{bmatrix}
x_{01} + x_{11} + t_{11} &amp; x_{01} + x_{12} + t_{12} \\
x_{02} + x_{11} + t_{21} &amp; x_{02} + x_{22} + t_{22}
\end{bmatrix}
\]</span></p>
<p>然后更新previous，得到 <span class="math display">\[previous = [\log(e^{x_{01} + x_{11} + t_{11}} + e^{x_{02} + x_{11} + t_{21}}), \log(e^{x_{01} + x_{12} + t_{12}} + e^{x_{02} + x_{22} + t_{22}})]\]</span></p>
<p>然后反复迭代得到所有的结果。</p>
<p>这里有个文章里面的图示非常明显：</p>
<p><img data-src='https://i.loli.net/2020/05/23/WfS2vaEUcRpnm4C.jpg'></p>
<p>这样可以完成全部可能路径的概率计算。</p>
<p>接下来就是跟HMM一样的解码问题，同样采用维特比算法就可以解开隐藏序列。如下图： <img data-src="https://i.loli.net/2020/05/23/VxkRP2KrJlM4zh9.jpg" alt="viterbi1" /> <img data-src="https://i.loli.net/2020/05/23/7iokSZULbumlg59.jpg" alt="viterbi2" /> <img data-src="https://i.loli.net/2020/05/23/Y5G1fDkOIwVndKL.jpg" alt="viterbi3" /> <img data-src="https://i.loli.net/2020/05/23/SgO9lj6z4MywUcG.jpg" alt="viterbi4" /></p>
<p>这样就完成了CRF的全过程。至于CRF的参数学习过程，只要用梯度下降去学习那个极大似然函数就可以了。</p>
<h1 id="bilstmcrf">BiLSTM+CRF</h1>
实际上对于命名实体识别任务而言，每一个词后面用BIOES标注，那么是不是直接就可以用LSTM来分类了。实际上也是可以的，用BiLSTM来做如下图：
<p align="center">
<img data-src='https://i.loli.net/2020/05/23/TQK1ZizLWtjERp7.jpg'>
</p>
<p>但是纯粹用LSTM来做的话会有一个问题，就是可能输出的分类是不合理的，比如下图：</p>
<p align="center">
<img data-src='https://i.loli.net/2020/05/23/cRfSJDgZGHm7oCT.jpg'>
</p>
<p>那么这种时候，如果在上层补上CRF的转移矩阵来做限制，就可以得到合理的结果。</p>
<p align="center">
<img data-src='https://i.loli.net/2020/05/23/DfpUkGuExnHA9av.png'>
</p>
<p>总体而言，我习惯把BiLSTM看做是CRF的改良版，用BiLSTM来替代CRF的emission score学习过程，实现比CRF更好的效果。</p>
<p>具体的代码可以直接看PyTorch的官方教程，不过里面的是单个记录的训练，大规模训练比较慢，我这里改了一版基于batch训练的可以参考。<a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/pytorch/NER/NER-101.ipynb">notebook地址</a>。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>未来一段时间更新计划</title>
    <url>/schedule/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>年前换了个坑，结果一堆事情，那段时间烦心事也多，啥都没更新。大概拍了一下未来一段时间的更新计划。主要是5月以后的。</p>
<span id="more"></span>
<p>五一节左右会更新一篇DBSCAN跟OPTICS算法的原理和代码，主要是基于密度的聚类算法。然后如果顺利的话再更新一篇经济思想史笔记。</p>
<p>六月以后看到李宏毅老师的深度学习课程加了好多新的内容，主要是GAN、DRL这一些，之前都是草草看一遍，打算这之后一周一篇到两篇课程笔记。争取一个季度内全部补全吧。另外立个flag吧，如果一个季度内补全了，顺便复习了一遍机器学习的内容，把之前可能理解错误的内容勘定了，给自己买块2080ti。括弧笑～</p>
<p>再有是被同事刺激了，感觉要每天提高专业技能呀。后面看看是不是开个专门记录刷吉米多维奇和概率论的栏目。之前刷leetcode的都半途而废了，有点可惜啊，看看是不是也重启一下。要不一三五数学，二四六leetcode，每天背50个单词这样？！讲道理英语词汇量感觉只剩下机器学习相关的了。</p>
<p>然后是读书方面，简单分了三类：</p>
<ul>
<li>经管类，目前主要在看经济思想史，不过是漫漫长路，一点点看。然后打算看一些量化的东西吧，做点策略什么的，不然钱都是死的，血亏。这部分的书单还在打理中，五一着重安排一下，毕竟是一个有专业门槛的领域。</li>
<li>技术/数学类，主要想研究一下微软研究院出的《分布式机器学习》。换大厂了嘛，还是小机器单机就配不上大厂了。然后其他的机器学习、深度学习教程就当做是看李宏毅老师课程时候的参考资料看好了，可能参考一下邱锡鹏老师的新书，经典的花书这一些。其他技术类讲算法的书随便翻翻，看到之前没实现的算法可能有时间就复现一下，作为彩蛋更新。数学类的书籍就是打算跟同事一样开始刷题了，不然牛逼的人太多，我都不知道以后该怎么混。</li>
<li>杂书类，杂七杂八的书就随意了，看眼缘了。这一个多月看了《思考快与慢》、《黑天鹅》还有《随机游走的傻瓜》，怎么说呢，基本上都是说认知偏差这一大类的，现在看的《世界观》也差不多，大概扫一扫吧。然后可能想看一些行为心理学方面的书吧，改改拖延症，培养节奏感啥的。最后就是随缘翻一翻《易经》。</li>
</ul>
<p>另外就是现在每天记录睡眠时间想逼自己早睡，虽然没啥效果吧，不过每次看看也是挺触目惊心的。尽量把时间排满，不逼自己996么，感觉太懒散了，时间上满一点，有节奏一点，看看会不会以后不熬夜了。</p>
<p>大概就这么个计划吧，现在觉得吧，还是得逼自己一把。</p>
]]></content>
      <categories>
        <category>碎碎念</category>
      </categories>
  </entry>
  <entry>
    <title>文本生成</title>
    <url>/pytorch_text_generation/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>最近诸事不顺，情绪不佳。继续做文本生成的事情。之前用的Char-RNN存在一定的缺陷，那就是你需要给定一个prefix，然后模型就会顺着prefix开始一个个往下预测。但是这样生成的文本随机性是很大的，所以我们希望能够让句子根据我们的关键词或者topic来生成。看了几篇论文，大框架上都是基于Attention的，其他的都是一些小的细节变化。这里打算实现两篇论文里的框架，一篇是哈工大的<a href="http://ir.hit.edu.cn/~xcfeng/xiaocheng%20Feng&#39;s%20Homepage_files/final-topic-essay-generation.pdf">Topic-to-Essay Generation with Neural Networks</a>，另一篇是百度的<a href="https://arxiv.org/pdf/1610.09889.pdf">Chinese Poetry Generation with Planning based Neural Networks</a>。</p>
<hr />
<p>2018年11月8号更新：认真看了一下百度的那篇paper，模型跟TAV的差不了太多，就是先用一个RNN把关键词做个双向的encoding，然后当做第一个词放进去训练。没什么兴趣弄了。</p>
<span id="more"></span>
<p>第一篇论文里面放了三种策略，由简到繁分别是Topic-Averaged LSTM，Attention-based LSTM，以及Multi-Topic-Aware LSTM。</p>
<p>其实策略上来说，TAV-LSTM就是将topic的embedding做一个平均，然后作为prefix来训练，所以基本上网络设计上也和之前的Char-RNN差不多，比较容易实现。TAT-LSTM就是将topic做一个Attention，然后作为一个feature跟hidden并到一起喂到decoder里面去。MTA-LSTM还包含了一个叫做coverage vector的向量来计算topic的信息是否在训练过程中被喂进去了。</p>
<p>官方放了一个很久以前的TensorFlow版本的<a href="https://github.com/hit-computer/MTA-LSTM">MTA-LSTM</a>，一方面我不喜欢TF，另一方面版本太老旧了，所以就用只能自己摸索着写PyTorch版本的了。数据就直接用的这个git上面提供的composition和zhihu两个数据。</p>
<p>然后这里都是用的贪婪法取候选词，没有做束搜索。当然，主要是因为懒，后面糟心事情过去了再说吧。</p>
<h1 id="tav">TAV</h1>
<p>TAV的大概工作原理上面也提到了，这里不赘述。然后同样偷懒，用了之前Char-RNN的模型直接修改。</p>
<p>首先常规套路，训练词向量。说起来，腾讯之前开源了一个800w+的词向量，也可以用用。这个就不多说了，很简单。</p>
<p>然后就是处理一下数据，首先我们要加入四个特殊字符PAD，BOS，EOS，和UNK。都是常规套路。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">fvec = KeyedVectors.load_word2vec_format(<span class="string">&#x27;vec.txt&#x27;</span>, binary=<span class="literal">False</span>)</span><br><span class="line">word_vec = fvec.vectors</span><br><span class="line">vocab = [<span class="string">&#x27;&lt;PAD&gt;&#x27;</span>, <span class="string">&#x27;&lt;BOS&gt;&#x27;</span>, <span class="string">&#x27;&lt;EOS&gt;&#x27;</span>, <span class="string">&#x27;&lt;UNK&gt;&#x27;</span>]</span><br><span class="line">vocab.extend(<span class="built_in">list</span>(fvec.vocab.keys()))</span><br><span class="line">word_vec = np.concatenate((np.array([[<span class="number">0</span>]*word_vec.shape[<span class="number">1</span>]] * <span class="number">4</span>), word_vec))</span><br><span class="line">word_vec = torch.tensor(word_vec)</span><br></pre></td></tr></table></figure>
<p>然后就是要做idx to word和word to idx的转换器。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">word_to_idx = &#123;ch: i <span class="keyword">for</span> i, ch <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)&#125;</span><br><span class="line">idx_to_word = &#123;i: ch <span class="keyword">for</span> i, ch <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)&#125;</span><br></pre></td></tr></table></figure>
<p>然后就是读数据，做iterator。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">essays = []</span><br><span class="line">topics = []</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;composition.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">        essay, topic = line.replace(<span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;&#x27;</span>).split(<span class="string">&#x27; &lt;/d&gt; &#x27;</span>)</span><br><span class="line">        essays.append(essay.split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line">        topics.append(topic.split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line"></span><br><span class="line">corpus_indice = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: [word_to_idx[w] <span class="keyword">for</span> w <span class="keyword">in</span> x], essays[:<span class="number">8000</span>]))</span><br><span class="line">topics_indice = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: [word_to_idx[w] <span class="keyword">for</span> w <span class="keyword">in</span> x], topics[:<span class="number">8000</span>]))</span><br><span class="line">length = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> x: <span class="built_in">len</span>(x), corpus_indice))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tav_data_iterator</span>(<span class="params">corpus_indice, topics_indice, batch_size, num_steps</span>):</span></span><br><span class="line">    epoch_size = <span class="built_in">len</span>(corpus_indice) // batch_size</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch_size):</span><br><span class="line">        raw_data = corpus_indice[i*batch_size: (i+<span class="number">1</span>)*batch_size]</span><br><span class="line">        key_words = topics_indice[i*batch_size: (i+<span class="number">1</span>)*batch_size]</span><br><span class="line">        data = np.zeros((<span class="built_in">len</span>(raw_data), num_steps+<span class="number">1</span>), dtype=np.int64)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(batch_size):</span><br><span class="line">            doc = raw_data[i]</span><br><span class="line">            tmp = [<span class="number">1</span>]</span><br><span class="line">            tmp.extend(doc)</span><br><span class="line">            tmp.extend([<span class="number">2</span>])</span><br><span class="line">            tmp = np.array(tmp, dtype=np.int64)</span><br><span class="line">            _size = tmp.shape[<span class="number">0</span>]</span><br><span class="line">            data[i][:_size] = tmp</span><br><span class="line">        key_words = np.array(key_words, dtype=np.int64)</span><br><span class="line">        x = data[:, <span class="number">0</span>:num_steps]</span><br><span class="line">        y = data[:, <span class="number">1</span>:]</span><br><span class="line">        mask = np.float32(x != <span class="number">0</span>)</span><br><span class="line">        x = torch.tensor(x)</span><br><span class="line">        y = torch.tensor(y)</span><br><span class="line">        mask = torch.tensor(mask)</span><br><span class="line">        key_words = torch.tensor(key_words)</span><br><span class="line">        <span class="keyword">yield</span>(x, y, mask, key_words)</span><br></pre></td></tr></table></figure>
<p>这里也是简单处理了，很多细节慢慢修改吧，然后就是这里的mask，我也是偷懒不去弄了，其实是标识那些词用来训练，哪些是padding的，我后面在loss function那里直接将PAD的权重改成0了。</p>
<p>然后就是定义网络。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TAVLSTM</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hidden_dim, embed_dim, num_layers, weight,</span></span></span><br><span class="line"><span class="params"><span class="function">                 num_labels, bidirectional, dropout=<span class="number">0.5</span>, **kwargs</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(TAVLSTM, self).__init__(**kwargs)</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.embed_dim = embed_dim</span><br><span class="line">        self.num_layers = num_layers</span><br><span class="line">        self.num_labels = num_labels</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        <span class="keyword">if</span> num_layers &lt;= <span class="number">1</span>:</span><br><span class="line">            self.dropout = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.dropout = dropout</span><br><span class="line">        self.embedding = nn.Embedding.from_pretrained(weight)</span><br><span class="line">        self.embedding.weight.requires_grad = <span class="literal">False</span></span><br><span class="line">        self.rnn = nn.GRU(input_size=self.embed_dim, hidden_size=self.hidden_dim,</span><br><span class="line">                          num_layers=self.num_layers, bidirectional=self.bidirectional,</span><br><span class="line">                          dropout=self.dropout)</span><br><span class="line">        <span class="keyword">if</span> self.bidirectional:</span><br><span class="line">            self.decoder = nn.Linear(hidden_dim * <span class="number">2</span>, self.num_labels)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.decoder = nn.Linear(hidden_dim, self.num_labels)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs, topics, hidden=<span class="literal">None</span></span>):</span></span><br><span class="line">        embeddings = self.embedding(inputs)</span><br><span class="line">        topics_embed = self.embedding(topics)</span><br><span class="line">        topics_embed = topics_embed.mean(dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(embeddings.shape[<span class="number">0</span>]):</span><br><span class="line">            embeddings[i][<span class="number">0</span>] = topics_embed[i]</span><br><span class="line">        states, hidden = self.rnn(embeddings.permute([<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]).<span class="built_in">float</span>(), hidden)</span><br><span class="line">        outputs = self.decoder(states.reshape((-<span class="number">1</span>, states.shape[-<span class="number">1</span>])))</span><br><span class="line">        <span class="keyword">return</span>(outputs, hidden)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span>(<span class="params">self, num_layers, batch_size, hidden_dim, **kwargs</span>):</span></span><br><span class="line">        hidden = torch.zeros(num_layers, batch_size, hidden_dim)</span><br><span class="line">        <span class="keyword">return</span> hidden</span><br></pre></td></tr></table></figure>
<p>基本结构没变化，就是forward的时候做了一点小修改，把第一个词变成topic average。</p>
<p>然后定义预测函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_rnn</span>(<span class="params">topics, num_chars, model, device, idx_to_word, word_to_idx</span>):</span></span><br><span class="line">    output = [<span class="number">1</span>]</span><br><span class="line">    topics = [word_to_idx[x] <span class="keyword">for</span> x <span class="keyword">in</span> topics]</span><br><span class="line">    topics = torch.tensor(topics)</span><br><span class="line">    hidden = torch.zeros(num_layers, <span class="number">1</span>, hidden_dim)</span><br><span class="line">    <span class="keyword">if</span> use_gpu:</span><br><span class="line">        hidden = hidden.to(device)</span><br><span class="line">        topics = topics.to(device)</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(num_chars):</span><br><span class="line">        X = torch.tensor(output).reshape((<span class="number">1</span>, <span class="built_in">len</span>(output)))</span><br><span class="line">        <span class="keyword">if</span> use_gpu:</span><br><span class="line">            X = X.to(device)</span><br><span class="line">        pred, hidden = model(X, topics, hidden)</span><br><span class="line">        <span class="keyword">if</span> pred.argmax(dim=<span class="number">1</span>)[-<span class="number">1</span>] == <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output.append(<span class="built_in">int</span>(pred.argmax(dim=<span class="number">1</span>)[-<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">return</span>(<span class="string">&#x27;&#x27;</span>.join([idx_to_word[i] <span class="keyword">for</span> i <span class="keyword">in</span> output[<span class="number">1</span>:]]))</span><br></pre></td></tr></table></figure>
<p>设定一下参数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">embedding_dim = <span class="number">300</span></span><br><span class="line">hidden_dim = <span class="number">256</span></span><br><span class="line">lr = <span class="number">1e2</span></span><br><span class="line">momentum = <span class="number">0.0</span></span><br><span class="line">num_epoch = <span class="number">100</span></span><br><span class="line">use_gpu = <span class="literal">True</span></span><br><span class="line">num_layers = <span class="number">1</span></span><br><span class="line">bidirectional = <span class="literal">False</span></span><br><span class="line">batch_size = <span class="number">8</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">loss_function = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">model = TAVLSTM(hidden_dim=hidden_dim, embed_dim=embedding_dim, num_layers=num_layers,</span><br><span class="line">                num_labels=<span class="built_in">len</span>(vocab), weight=word_vec, bidirectional=bidirectional)</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)</span><br><span class="line"><span class="keyword">if</span> use_gpu:</span><br><span class="line">    model.to(device)</span><br></pre></td></tr></table></figure>
<p>接着训练就好了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">since = time.time()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(num_epoch):</span><br><span class="line">    start = time.time()</span><br><span class="line">    num, total_loss = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line"><span class="comment">#     if epoch == 5000:</span></span><br><span class="line"><span class="comment">#         optimizer.param_groups[0][&#x27;lr&#x27;] = lr * 0.1</span></span><br><span class="line">    data = tav_data_iterator(corpus_indice, topics_indice, batch_size, <span class="built_in">max</span>(length)+<span class="number">1</span>)</span><br><span class="line">    hidden = model.init_hidden(num_layers, batch_size, hidden_dim)</span><br><span class="line">    weight = torch.ones(<span class="built_in">len</span>(vocab))</span><br><span class="line">    weight[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> X, Y, mask, topics <span class="keyword">in</span> tqdm(data):</span><br><span class="line">        num += <span class="number">1</span></span><br><span class="line">        hidden.detach_()</span><br><span class="line">        <span class="keyword">if</span> use_gpu:</span><br><span class="line">            X = X.to(device)</span><br><span class="line">            Y = Y.to(device)</span><br><span class="line">            mask = mask.to(device)</span><br><span class="line">            topics = topics.to(device)</span><br><span class="line">            hidden = hidden.to(device)</span><br><span class="line">            weight = weight.to(device)</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        output, hidden = model(X, topics, hidden)</span><br><span class="line">        l = F.cross_entropy(output, Y.t().reshape((-<span class="number">1</span>,)), weight)</span><br><span class="line">        l.backward()</span><br><span class="line">        norm = nn.utils.clip_grad_norm_(model.parameters(), <span class="number">1e-2</span>)</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += l.item()</span><br><span class="line">    end = time.time()</span><br><span class="line">    s = end - since</span><br><span class="line">    h = math.floor(s / <span class="number">3600</span>)</span><br><span class="line">    m = s - h * <span class="number">3600</span></span><br><span class="line">    m = math.floor(m / <span class="number">60</span>)</span><br><span class="line">    s -= m * <span class="number">60</span></span><br><span class="line">    <span class="keyword">if</span>(epoch % <span class="number">10</span> == <span class="number">0</span>) <span class="keyword">or</span> (epoch == (num_epoch - <span class="number">1</span>)):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch %d/%d, loss %.4f, norm %.4f, time %.3fs, since %dh %dm %ds&#x27;</span></span><br><span class="line">              %(epoch+<span class="number">1</span>, num_epoch, total_loss / num, norm, end-start, h, m, s))</span><br><span class="line">        <span class="built_in">print</span>(predict_rnn([<span class="string">&#x27;妈妈&#x27;</span>, <span class="string">&#x27;希望&#x27;</span>, <span class="string">&#x27;长大&#x27;</span>, <span class="string">&#x27;孩子&#x27;</span>, <span class="string">&#x27;母爱&#x27;</span>], <span class="number">100</span>, model, device, idx_to_word, word_to_idx))</span><br></pre></td></tr></table></figure>
<p>具体还是看我的<a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/pytorch/text%20generater/Topic2Essay_train.ipynb">notebook</a>。不过梯度还是爆炸了，哎。</p>
<h1 id="tat">TAT</h1>
<p>就是在TAV的基础上修改，直接看<a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/pytorch/text%20generater/Topic2Essay_TAT.ipynb">notebook</a>吧。这个模型深刻地表达了我的内心正处在TAT的状态。</p>
<h1 id="mat">MAT</h1>
<p>论文里面的<span class="math inline">\(U_f\)</span>没看懂是什么意思，所以就自己演绎了一下，简单来说，为了让每个topic都有机会出现，那么很自然会想到要去调整Attention的权重，高的压低一点，低的抬高一点。所以我在每个epoch结束后调整一下： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line">params = model.state_dict()</span><br><span class="line">params[<span class="string">&#x27;attn.weight&#x27;</span>].clamp_(<span class="number">0</span>)</span><br><span class="line">params[<span class="string">&#x27;attn.weight&#x27;</span>] *= <span class="number">1</span> / -torch.log(params[<span class="string">&#x27;attn.weight&#x27;</span>] / torch.<span class="built_in">sum</span>(params[<span class="string">&#x27;attn.weight&#x27;</span>]) + <span class="number">0.000001</span>)</span><br></pre></td></tr></table></figure> 也就是说，每个Attention前面加了一个weight，这个weight是<span class="math inline">\(-\log(p+\lambda)\)</span>。加个lambda是避免变成0，而p就是这个topic的Attention在所有topic的Attention的比重。这个其实也可以试试每一个iteration就变化会怎么样。</p>
<p>具体看<a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/pytorch/text%20generater/Topic2Essay_MAT_bleu-adaptive.ipynb">notebook</a>。</p>
<p>另外要说的就是，用全量softmax速度太慢了，改用了PyTorch的adaptive softmax。0.41版本以上自带的一个功能，是一个softmax的优化方案，论文说比hierarchical softmax在GPU上的表现更好，虽然论文没太看懂。这里有篇老外的<a href="https://towardsdatascience.com/speed-up-your-deep-learning-language-model-up-to-1000-with-the-adaptive-softmax-part-1-e7cc1f89fcc9">博客</a>大概讲了一下原理，不过也没证明为什么会更好。大概意思就是将所有的词按照词频排序，然后分成高频和低频两组，然后低频组再拆成两到四个组，然后判断这个词是在哪个组里面。</p>
<p>还有就是TAT的模型改了一下要求Attention必须都是大于等于0的，MAT也是在这个基础上搞的。</p>
<p>差不多就这么一回事吧。诸事不顺，近期要看个病，但愿不是重病。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title>word2vec的PyTorch实现</title>
    <url>/word2vec_pytorch/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>Word Embedding现在是现在NLP的入门必备，这里简单实现一个CBOW的W2V。</p>
<p>2018-07-06更新一发用一篇小说来训练模型的脚本。</p>
<p>2018-08-02更新一发negative sampling版本。</p>
<span id="more"></span>
<h1 id="negtive-sampling版本">negtive sampling版本</h1>
<p><strong>2018-08-02更新基于negative sampling方法的W2V</strong></p>
<p>翻了之前项亮实现的MXNet版本的NCE，看的不甚理解，感觉他写的那个是NEG的样子，然后还是自己写一个简单的negative sampling来做这个事情。关于NCE和NEG的区别，其实NEG就像是NCE的一个特殊情况，这个可以看<a href="https://arxiv.org/pdf/1410.8251.pdf">Notes on Noise Contrastive Estimation and Negative Sampling</a>，或者是谷歌的一篇<a href="https://www.tensorflow.org/extras/candidate_sampling.pdf">总结</a>。</p>
<p>关于negative sampling这里简单介绍一下，其实负采样的思路非常的简单，就是原来我们有多少个词，那么softmax就要算多少个词的概率，用负采样的方法就是将原来这样的巨量分类问题变成一个简单的二分类问题。也就是说，原来正确的label依然保留，接着只要sample出一小部分的负样本出来，然后做一个二分类问题就可以了。至于需要sample多少负样本，谷歌的C版本中是用了5个，好像哪里见过说不超过25个就可以了，但是现在忘了是哪篇文章了，可能不准确O__O "…</p>
<p>具体的公式推导其实很简单，可以看一下gluon关于<a href="http://zh.gluon.ai/chapter_natural-language-processing/word2vec.html">负采样的介绍</a>。</p>
<p>所以实际上要实现这个负采样非常的容易，只要设计一个抽样分布，然后开始抽样就可以了。在很多词向量的资料里面都说到了，采样分布选用的是： <span class="math display">\[
P(w_i) = \frac{f(w_i)^{0.75}}{\sum(f(w_j)^{0.75})}
\]</span> 这个其实非常像softmax，就是说用单个词的词频除以全部词频的和，原来的代码中加入了0.75的这个幂指数，完全是炼丹经验。</p>
<p>然后网上参考了一个开源的代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">NEGLoss</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, ix_to_word, word_freqs, num_negative_samples=<span class="number">5</span>,</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(NEGLoss, self).__init__()</span><br><span class="line">        self.num_negative_samples = num_negative_samples</span><br><span class="line">        self.vocab_size = <span class="built_in">len</span>(word_freqs)</span><br><span class="line">        self.dist = F.normalize(torch.Tensor(</span><br><span class="line">            [word_freqs[ix_to_word[i]] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.vocab_size)]).<span class="built_in">pow</span>(<span class="number">0.75</span>), dim=<span class="number">0</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span>(<span class="params">self, num_samples, positives=[]</span>):</span></span><br><span class="line">        weights = torch.zeros((self.vocab_size, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> positives:</span><br><span class="line">            weights[w] += <span class="number">1.0</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_samples):</span><br><span class="line">            w = torch.multinomial(self.dist, <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">while</span>(w <span class="keyword">in</span> positives):</span><br><span class="line">                w = torch.multinomial(self.dist, <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">            weights[w] += <span class="number">1.0</span></span><br><span class="line">        <span class="keyword">return</span> weights</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>, target</span>):</span></span><br><span class="line">        <span class="keyword">return</span> F.nll_loss(<span class="built_in">input</span>, target,</span><br><span class="line">                          self.sample(self.num_negative_samples,</span><br><span class="line">                                      positives=target.data.numpy()))</span><br></pre></td></tr></table></figure>
<p>但是有个小问题就是，这里采用的其实是很取巧的一个方法，就是说，我每次会生成一个矩阵告诉pytorch究竟有哪6个sample被我拿到了，然后算negative log likelihood的时候就只算这6个。结果上来说，是实现了负采样，但是从算法效率上来说，其实并没有起到减少计算量的效果。</p>
<p>所以这里我们实现一个非常简单，类似nagative sampling，但是不是非常严格的采样函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">neg_sample</span>(<span class="params">num_samples, positives=[]</span>):</span></span><br><span class="line">    freqs_pow = torch.Tensor([freqs[ix_to_word[i]] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(vocab_size)]).<span class="built_in">pow</span>(<span class="number">0.75</span>)</span><br><span class="line">    dist = freqs_pow / freqs_pow.<span class="built_in">sum</span>()</span><br><span class="line">    w = np.random.choice(<span class="built_in">len</span>(dist), (<span class="built_in">len</span>(positives), num_samples), p=dist.numpy())</span><br><span class="line">    <span class="keyword">if</span> positives.is_cuda:</span><br><span class="line">        <span class="keyword">return</span> torch.tensor(w).to(device)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.tensor(w)</span><br></pre></td></tr></table></figure>
<p>然后相应的，我们需要将我们的CBOW也变一下，按照 <span class="math display">\[
-\text{log} \frac{1}{1+\text{exp}\left(-u_c^\top (v_{o_1} + \ldots + v_{o_{2m}}) /(2m)\right)}  - \sum_{k=1, w_k \sim \mathbb{P}(w)}^K \text{log} \frac{1}{1+\text{exp}\left((u_{i_k}^\top (v_{o_1} + \ldots + v_{o_{2m}}) /(2m)\right)}.
\]</span> 这个公式计算最后的loss。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CBOW</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embedding_dim, context_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CBOW, self).__init__()</span><br><span class="line">        self.embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.embeddings.weight.data.uniform_(-<span class="number">0.5</span> / vocab_size, <span class="number">0.5</span> / vocab_size)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs, label</span>):</span></span><br><span class="line">        negs = neg_sample(<span class="number">5</span>, label)</span><br><span class="line">        u_embeds = self.embeddings(label).view(<span class="built_in">len</span>(label), -<span class="number">1</span>)</span><br><span class="line">        v_embeds_pos = self.embeddings(inputs).mean(dim=<span class="number">1</span>)</span><br><span class="line">        v_embeds_neg = self.embeddings(negs).mean(dim=<span class="number">1</span>)</span><br><span class="line">        loss1 = torch.diag(torch.matmul(u_embeds, v_embeds_pos.transpose(<span class="number">0</span>, <span class="number">1</span>)))</span><br><span class="line">        loss2 = torch.diag(torch.matmul(u_embeds, v_embeds_neg.transpose(<span class="number">0</span>, <span class="number">1</span>)))</span><br><span class="line">        loss1 = -torch.log(<span class="number">1</span> / (<span class="number">1</span> + torch.exp(-loss1)))</span><br><span class="line">        loss2 = -torch.log(<span class="number">1</span> / (<span class="number">1</span> + torch.exp(loss2)))</span><br><span class="line">        loss = (loss1.mean() + loss2.mean())</span><br><span class="line">        <span class="keyword">return</span>(loss)</span><br></pre></td></tr></table></figure>
<p>这里我将embedding层的权重进行了标准化，通过这样的标准化可以避免后面计算loss的时候出现无穷大的情况。然后其他参数不用做什么变化，开始训练看看效果。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    total_loss = torch.Tensor([<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> context, target <span class="keyword">in</span> tqdm(data_iter):</span><br><span class="line">        context_ids = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(context[<span class="number">0</span>])):</span><br><span class="line">            context_ids.append(make_context_vector([context[j][i] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(context))], word_to_ix))</span><br><span class="line">        context_ids = torch.stack(context_ids)</span><br><span class="line">        context_ids = context_ids.to(device)</span><br><span class="line">        model.zero_grad()</span><br><span class="line">        label = make_context_vector(target, word_to_ix)</span><br><span class="line">        label = label.to(device)</span><br><span class="line">        loss = model(context_ids, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">    losses.append(total_loss)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch %d loss %.4f&#x27;</span> %(epoch, total_loss))</span><br><span class="line"><span class="built_in">print</span>(losses)</span><br></pre></td></tr></table></figure>
<p>完整的notebook可以看<a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/pytorch/langage%20model/w2v_ngs.ipynb">这个</a>，效率上有质的提升。batchsize还是1024的时候大概压缩到15分钟左右，放到8192的时候大概一个epoch是10分钟。一本满足。</p>
<hr />
<h1 id="toy-版本">toy 版本</h1>
<p>首先import必要的模块： <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.autograd <span class="keyword">as</span> autograd</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br></pre></td></tr></table></figure></p>
<p>CBOW的全称是continuous bag of words。和传统的N-gram相比，CBOW会同时左右各看一部分词。也就是说，根据左右两边的词，猜测中间的词是什么。而传统的N-gram是根据前面的词，猜后面的词是什么。在PyTorch的官网上给出了N-gram的实现。因此我们只需要在这个基础上进行简单的修改就可以得到基于CBOW的W2V模型。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">CONTEXT_SIZE = <span class="number">2</span>  <span class="comment"># 2 words to the left, 2 to the right</span></span><br><span class="line">raw_text = <span class="string">&quot;&quot;&quot;We are about to study the idea of a computational process.</span></span><br><span class="line"><span class="string">Computational processes are abstract beings that inhabit computers.</span></span><br><span class="line"><span class="string">As they evolve, processes manipulate other abstract things called data.</span></span><br><span class="line"><span class="string">The evolution of a process is directed by a pattern of rules</span></span><br><span class="line"><span class="string">called a program. People create programs to direct processes. In effect,</span></span><br><span class="line"><span class="string">we conjure the spirits of the computer with our spells.&quot;&quot;&quot;</span>.split()</span><br><span class="line"></span><br><span class="line"><span class="comment"># By deriving a set from `raw_text`, we deduplicate the array</span></span><br><span class="line">vocab = <span class="built_in">set</span>(raw_text)</span><br><span class="line">vocab_size = <span class="built_in">len</span>(vocab)</span><br><span class="line"></span><br><span class="line">word_to_ix = &#123;word: i <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)&#125;</span><br><span class="line">data = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, <span class="built_in">len</span>(raw_text) - <span class="number">2</span>):</span><br><span class="line">    context = [raw_text[i - <span class="number">2</span>], raw_text[i - <span class="number">1</span>],</span><br><span class="line">               raw_text[i + <span class="number">1</span>], raw_text[i + <span class="number">2</span>]]</span><br><span class="line">    target = raw_text[i]</span><br><span class="line">    data.append((context, target))</span><br><span class="line"><span class="built_in">print</span>(data[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<p>首先定义我们需要的数据。这里的CBOW的Windows是2。因为单词没法直接拿来训练，因此这里我们用id来唯一标识每一个单词。然后我们需要做的一个事情就是将这些id编码成向量。14年谷歌放出来的C那一版我印象中是用的霍夫曼树再降维，现在的PyTorch和gluon都有embedding的类，可以将分类的数据直接编码成向量。所以我们现在用框架实现这个事情就非常简单了。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CBOW</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embedding_dim, context_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CBOW, self).__init__()</span><br><span class="line">        self.embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.linear1 = nn.Linear(context_size * embedding_dim, <span class="number">128</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">128</span>, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        embeds = self.embeddings(inputs).view((<span class="number">1</span>, -<span class="number">1</span>))</span><br><span class="line">        out = F.relu(self.linear1(embeds))</span><br><span class="line">        out = self.linear2(out)</span><br><span class="line">        log_probs = F.log_softmax(out, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span>(log_probs)</span><br></pre></td></tr></table></figure>
<p>这个CBOW的类很简单，继承了PyTorch的Module类，然后第一步我们就做了一个embedding，然后做了一个隐藏层和一个输出层。最后我们做了一个softmax的动作来得到probability。这就是我们需要训练的神经网络。所以一直说W2V是一个单层的神经网络就是这个原因。</p>
<p>然后我们定义一个简单的函数，将单词转变成id <figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_context_vector</span>(<span class="params">context, word_to_ix</span>):</span></span><br><span class="line">    idxs = [word_to_ix[w] <span class="keyword">for</span> w <span class="keyword">in</span> context]</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(idxs, dtype=torch.long)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">make_context_vector(data[<span class="number">0</span>][<span class="number">0</span>], word_to_ix)</span><br></pre></td></tr></table></figure></p>
<p>接着定义一些需要的参数： <figure class="highlight py"><table><tr><td class="code"><pre><span class="line">device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">losses = []</span><br><span class="line">loss_function = nn.NLLLoss()</span><br><span class="line">model = CBOW(<span class="built_in">len</span>(vocab), embedding_dim=<span class="number">10</span>, context_size=CONTEXT_SIZE*<span class="number">2</span>)</span><br><span class="line">model.to(device)</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure></p>
<p>这里需要注意一点，context_size需要windows的大小乘2，因为CBOW同时左右都看了这些词，所以我们放进来的词实际上是windows乘2的数量。</p>
<p>这里我用了GPU来加速计算。如果没有GPU的可以注释掉所有跟device相关的代码，这个数据量不大，体会不到GPU的优势。</p>
<p>然后就是正式训练 <figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    total_loss = torch.Tensor([<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> context, target <span class="keyword">in</span> data:</span><br><span class="line">        context_ids = make_context_vector(context, word_to_ix)</span><br><span class="line">        context_ids = context_ids.to(device)</span><br><span class="line">        model.zero_grad()</span><br><span class="line">        log_probs = model(context_ids)</span><br><span class="line">        label = torch.tensor([word_to_ix[target]], dtype=torch.long)</span><br><span class="line">        label = label.to(device)</span><br><span class="line">        loss = loss_function(log_probs, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">    losses.append(total_loss)</span><br><span class="line"><span class="built_in">print</span>(losses)</span><br></pre></td></tr></table></figure></p>
<p>这样就是一个词向量的训练过程。如果我们需要得到embedding之后的结果，只需要将数据过一遍embedding这一层就可以了。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">model.embeddings(make_context_vector(data[<span class="number">0</span>][<span class="number">0</span>], word_to_ix))</span><br></pre></td></tr></table></figure>
<p>可以对比一下训练前和训练后向量的差异。</p>
<hr />
<h1 id="softmax低效率版本">softmax低效率版本</h1>
<p>2018-07-06更新内容：</p>
<p>之前写的那个是一个非常toy的网络，本质上就是了解一下word2vec是怎么一回事。不过完全不具备实操的能力。下面找了一些开源的语料，稍微修改了一下之前的脚本，还是基于CBOW的模型，这样就可以正常跑日常的数据。语料地址<a href="https://github.com/lxrogers/CS221SAT/tree/master/data/Holmes_Training_Data">https://github.com/lxrogers/CS221SAT/tree/master/data/Holmes_Training_Data</a>。</p>
<p>先import一些必要的包，这里的tqdm是显示进度的。 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.utils.data.dataloader <span class="keyword">as</span> dataloader</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.autograd <span class="keyword">as</span> autograd</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> gc</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br></pre></td></tr></table></figure></p>
<p>然后读入语料数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">text = []</span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> os.listdir(<span class="string">&#x27;Holmes_Training_Data/&#x27;</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(<span class="string">&#x27;Holmes_Training_Data&#x27;</span>, file), <span class="string">&#x27;r&#x27;</span>, errors=<span class="string">&#x27;ignore&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        text.extend(f.read().splitlines())</span><br><span class="line"></span><br><span class="line">text = [x.replace(<span class="string">&#x27;*&#x27;</span>, <span class="string">&#x27;&#x27;</span>) <span class="keyword">for</span> x <span class="keyword">in</span> text]</span><br><span class="line">text = [re.sub(<span class="string">&#x27;[^ \fA-Za-z0-9_]&#x27;</span>, <span class="string">&#x27;&#x27;</span>, x) <span class="keyword">for</span> x <span class="keyword">in</span> text]</span><br><span class="line">text = [x <span class="keyword">for</span> x <span class="keyword">in</span> text <span class="keyword">if</span> x != <span class="string">&#x27;&#x27;</span>]</span><br><span class="line"><span class="built_in">print</span>(text[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>
<p>这里我ignore了一些文本读入的错误，然后过滤掉了符号。</p>
<p>因为语料是英文的，所以这里按照空格分割单词，比中文方便太多。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">raw_text = []</span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> text:</span><br><span class="line">    raw_text.extend(x.split(<span class="string">&#x27; &#x27;</span>))</span><br><span class="line">raw_text = [x <span class="keyword">for</span> x <span class="keyword">in</span> raw_text <span class="keyword">if</span> x != <span class="string">&#x27;&#x27;</span>]</span><br></pre></td></tr></table></figure>
<p>分好词以后就可以开始构建词库</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">vocab = <span class="built_in">set</span>(raw_text)</span><br><span class="line">vocab_size = <span class="built_in">len</span>(vocab)</span><br></pre></td></tr></table></figure>
<p>接着跟之前一样，构建一个提供训练数据的函数，并准备好训练数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">make_context_vector</span>(<span class="params">context, word_to_ix</span>):</span></span><br><span class="line">    idxs = [word_to_ix[w] <span class="keyword">for</span> w <span class="keyword">in</span> context]</span><br><span class="line">    <span class="keyword">return</span> torch.tensor(idxs, dtype=torch.long)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">word_to_ix = &#123;word: i <span class="keyword">for</span> i, word <span class="keyword">in</span> <span class="built_in">enumerate</span>(vocab)&#125;</span><br><span class="line">data = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, <span class="built_in">len</span>(raw_text) - <span class="number">2</span>):</span><br><span class="line">    context = [raw_text[i - <span class="number">2</span>], raw_text[i - <span class="number">1</span>],</span><br><span class="line">               raw_text[i + <span class="number">1</span>], raw_text[i + <span class="number">2</span>]]</span><br><span class="line">    target = raw_text[i]</span><br><span class="line">    data.append((context, target))</span><br><span class="line"><span class="built_in">print</span>(data[:<span class="number">5</span>])</span><br></pre></td></tr></table></figure>
<p>定义网络，这里要注意的是，因为数据比较大，我们是分batch喂进来的，因此之前forward的时候，我们把embedding的数据摊开的时候是摊成一行的，这里需要摊成每个batch_size的大小。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CBOW</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embedding_dim, context_size</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(CBOW, self).__init__()</span><br><span class="line">        self.embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.linear1 = nn.Linear(context_size * embedding_dim, <span class="number">128</span>)</span><br><span class="line">        self.linear2 = nn.Linear(<span class="number">128</span>, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, inputs</span>):</span></span><br><span class="line">        embeds = self.embeddings(inputs).view(<span class="built_in">len</span>(inputs), -<span class="number">1</span>)</span><br><span class="line">        out = F.relu(self.linear1(embeds))</span><br><span class="line">        out = self.linear2(out)</span><br><span class="line">        log_probs = F.log_softmax(out, dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span>(log_probs)</span><br></pre></td></tr></table></figure>
<p>定义各种参数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">CONTEXT_SIZE = <span class="number">2</span></span><br><span class="line">batch_size = <span class="number">1024</span></span><br><span class="line">device = torch.device(<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line">losses = []</span><br><span class="line">loss_function = nn.NLLLoss()</span><br><span class="line">model = CBOW(vocab_size, embedding_dim=<span class="number">100</span>,</span><br><span class="line">             context_size=CONTEXT_SIZE*<span class="number">2</span>)</span><br><span class="line">model.to(device)</span><br><span class="line"><span class="comment"># model = torch.nn.DataParallel(model, device_ids=[0, 1]).cuda()</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<p>我这里本来写了多卡的跑法，但是不知道是不是我写法有问题还是为什么，每次我跑第二块卡的时候，PyTorch都会去第一块卡开一块空间出来，就算我只是在第二块卡跑也会在第一块卡开一些空间。比较神奇，后面再研究一下。</p>
<p>然后定义一下data iterator。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data_iter = torch.utils.data.DataLoader(data, batch_size=batch_size,</span><br><span class="line">                                        shuffle=<span class="literal">False</span>, num_workers=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<p>然后这里要注意的是，shuffle参数会影响每次iter的速度，shuffle会慢很多。另外num_workers越多速度越快，但是很可能会内存爆炸，需要自己调一个合适的。</p>
<p>然后就可以开始训练了：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    total_loss = torch.Tensor([<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">for</span> context, target <span class="keyword">in</span> tqdm(data_iter):</span><br><span class="line">        context_ids = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(context[<span class="number">0</span>])):</span><br><span class="line">            context_ids.append(make_context_vector([context[j][i] <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(context))], word_to_ix))</span><br><span class="line">        context_ids = torch.stack(context_ids)</span><br><span class="line">        context_ids = context_ids.to(device)</span><br><span class="line"><span class="comment">#         context_ids = torch.autograd.Variable(context_ids.cuda())</span></span><br><span class="line">        model.zero_grad()</span><br><span class="line">        log_probs = model(context_ids)</span><br><span class="line">        label = make_context_vector(target, word_to_ix)</span><br><span class="line">        label = label.to(device)</span><br><span class="line"><span class="comment">#         label = torch.autograd.Variable(label.cuda())</span></span><br><span class="line">        loss = loss_function(log_probs, label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">    losses.append(total_loss)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch %d loss %.4f&#x27;</span> %(epoch, total_loss))</span><br><span class="line"><span class="built_in">print</span>(losses)</span><br></pre></td></tr></table></figure>
<p>如果要多卡可以把to(device)的代码改成注释的代码就可以了。</p>
<p>然后就是需要<strong>注意</strong>的点了。</p>
<p><strong>这个网络的确是work的，训练完可以试一下发现queen-woman+man和king的cosine similarity的确比monkey或者其他的单词要高。但是这个网络的效率很低！很低！很低！（你觉得我会告诉你一个epoch需要跑一个半小时么）。</strong></p>
<p>原因在哪呢？其实很简单因为我这里使用的是softmax，也就是说，这个网络每一次训练都需要预测所有的词，比如我这个训练集里面有接近37万个词，那么每次就需要预测37万个类，效率之低可想而知。那么有什么解决方案呢？最早的时候，也就是谷歌C版本的解决方案是基于霍夫曼树的hierarchical softmax。后来DeepMind有一篇介绍把NCE（Noise-contrastive estimation）用来加速的论文<sup id="fnref:1"><a href="#fn:1" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="http://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf
">[1]</span></a></sup>。再后来又出现了negative sampling的论文<sup id="fnref:2"><a href="#fn:2" rel="footnote"><span class="hint--top hint--error hint--medium hint--rounded hint--bounce" aria-label="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf
">[2]</span></a></sup>。不过直观感受上，NCE和negative sampling是很像的，算是殊途同归吧。</p>
<p>后面过段时间更新对这两种方法的理解和代码。</p>
<div id="footnotes">
<hr>
<div id="footnotelist">
<ol style="list-style: none; padding-left: 0; margin-left: 40px">
<li id="fn:1">
<span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">1.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">http://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf<a href="#fnref:1" rev="footnote"> ↩︎</a></span>
</li>
<li id="fn:2">
<span style="display: inline-block; vertical-align: top; padding-right: 10px; margin-left: -40px">2.</span><span style="display: inline-block; vertical-align: top; margin-left: 10px;">http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf<a href="#fnref:2" rev="footnote"> ↩︎</a></span>
</li>
</ol>
</div>
</div>
]]></content>
      <categories>
        <category>PyTorch</category>
      </categories>
  </entry>
  <entry>
    <title>tiny XGBoost以及集成算法回顾</title>
    <url>/tinyxgb/</url>
    <content><![CDATA[<link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><p>XGBoost是GBDT的一个超级加强版，用了很久，一直没细看原理。围观了一波人家的实现，自己也来弄一遍。以后面试上来一句，要不要我现场写个XGBoost，听上去是不是很霸气。</p>
<p>在开源代码的基础上进行了一点修改，大概跑通了，但是有些地方感觉有点诡异。后面会讲。</p>
<span id="more"></span>
<h1 id="回顾一下ensemble算法">回顾一下ensemble算法</h1>
<p>之前也写了一篇笔记，<a href="https://samaelchen.github.io/machine_learning_step17/">台大李宏毅机器学习——集成算法</a>，最近把一些错误做了修改。</p>
<h2 id="bagging">bagging</h2>
<p>bagging的经典算法就是随机森林了，bagging的思路其实非常非常的简单，就是同时随机抽样本和feature，然后建立n个分类器，接着投票就好了。</p>
<p>bagging的做法本质上不会解决bias太大的问题，所以该过拟合还会过拟合。但是bagging会解决variance太大的问题。这个其实是非常直观的一件事情。</p>
<p>然后我突然开了一个脑洞，RF选择feature这种事情吧，如果用一个weight来表示，把树换成逻辑回归，最后voting的事情也用一个weight来表示，感觉，似乎就是单层神经网络的既视感。anyway，无脑揣测，有待探究。</p>
<h2 id="boosting">boosting</h2>
<p>boosting跟bagging的套路就不一样，bagging是同时并行很多分类器，但是boosting是串行多个分类器。bagging的分类器之间没有依赖关系，boosting的分类器是有依赖关系的。</p>
<p>boosting算法比较知名的就是GBDT和Adaboost两个。不过其实Adaboost就是一个特殊的GBDT。</p>
<p>GB的一般流程是这样的：</p>
<blockquote>
<p>初始化一个函数<span class="math inline">\(g_0(x) = 0\)</span><br />
然后按照迭代次数从<span class="math inline">\(t=1\)</span>到<span class="math inline">\(T\)</span>循环，我们的目标是找到一个函数<span class="math inline">\(f_t(x)\)</span>和权重<span class="math inline">\(\alpha_t\)</span>使得我们的函数<span class="math inline">\(g_{t-1}(x)\)</span>的效果更好，也就是说：<br />
<span class="math display">\[g_{t-1}(x) = \sum_{i=1}^{t-1} \alpha_i f_i(x)\]</span> 换个角度来看就是<span class="math inline">\(g_t(x) = g_{t-1}(x) + \alpha_t f_t(x)\)</span><br />
而最后我们优化的损失函数<span class="math inline">\(L(g) = \sum_n l(y_n, g(x_n))\)</span></p>
</blockquote>
<p>那Adaboost就是GB的损失函数<span class="math inline">\(l\)</span>用exponential表示，也就是<span class="math inline">\(\exp(-y_n g(x_n))\)</span>。很美妙的一家子。</p>
<p>那实际上我们看<span class="math inline">\(g_t(x) = g_{t-1}(x) + \alpha_t f_t(x)\)</span>这里，非常像梯度下降，那么如果要做梯度下降的话，其实我们就是对<span class="math inline">\(g(x)\)</span>做偏导，所以我们得到的是<span class="math inline">\(g_t(x) = g_{t-1}(x) - \eta \frac{\partial L(g)}{\partial g(x)} \bigg|_{g(x)=g_{t-1}(x)}\)</span>。那其实只要我们想办法让尾巴后面的那一部分是同一个方向的，我们不就达到了梯度下降的目的了吗？！步子的大小是可以用<span class="math inline">\(\eta\)</span>调的，同样这边也可以调整<span class="math inline">\(\alpha\)</span>。总之，保证他们的方向一致，就可以做梯度下降了。</p>
<h3 id="adaboost">Adaboost</h3>
<p>现在将Adaboost的损失函数放进来推算一下： <span class="math display">\[
\begin{align}
L(g) &amp;= \sum_n \exp(-y_n g_t(x_n)) \\
&amp;= \sum_n \exp(-y_n (g_{t-1}(x_n) + \alpha_t f_t(x))) \\
&amp;= \sum_n \exp(-y_n g_{t-1}(x_n)) \exp(-y_n \alpha_t f_t(x_n)) \\
&amp;= \sum_{f_t(x) \ne y} \exp(-y_n g_{t-1}(x_n)) \exp(\alpha_t) + \sum_{f_t(x) = y} \exp(-y_n g_{t-1}(x_n)) \exp(-\alpha_t)
\end{align}
\]</span></p>
<p>这样一来，我们需要同时寻找一个<span class="math inline">\(\alpha\)</span>和<span class="math inline">\(f(x)\)</span>使得我们的损失函数最小。找一个最优的<span class="math inline">\(f(x)\)</span>这个事情只要让决策树去学习就可以了，那么<span class="math inline">\(\alpha\)</span>怎么办呢？如果我们对<span class="math inline">\(\alpha\)</span>求偏导，让偏导数为0，那么理论上，我们在<span class="math inline">\(\alpha\)</span>这个方向上就是最优的。</p>
<p>具体求偏导这个之前的博客写了，这里就不多搞了。简单说就是刚好会等于<span class="math inline">\(\ln \sqrt{(1-\varepsilon_t) / \varepsilon_t}\)</span>，而这个值刚好是每一轮调整样本权重时候的系数取对数。</p>
<h3 id="gbdt">GBDT</h3>
<p>至于GBDT，实际上跟Adaboost没啥区别，也是一样的搞法，无非是损失函数不一样，优化策略不太一样而已。</p>
<p>GBDT的玩法是用每一棵树去学习上一棵树的残差，通俗的说就是下一棵树学习如何矫正上一棵树的错误。</p>
<p>但是残差这东西也是很妙的一个事情，如果我们损失函数是RMSE，其实我们的残差就是一阶导数。但是呢，如果损失函数是别的函数的时候，其实“残差”这个东西就很难去计算，比如分类的时候，残差究竟是个什么意思？！所以Freidman的梯度提升算法，也就是GBDT前面的GB就是用损失函数对<span class="math inline">\(f(x)\)</span>的一阶导数来替代残差的。</p>
<p>总体来说，我们想优化的损失函数是： <span class="math display">\[
\Theta_t = \arg \min_{\Theta_t} \sum_{t=1}^{T} l(y_n, g_{t-1}(x_n) + \alpha_t f_t(x))
\]</span></p>
<p>如果我们做回归问题，用RMSE为loss function，那么<span class="math inline">\(l(y_n, g_t(x_n))=(y_n - g_t(x_n))^2\)</span>，而<span class="math inline">\(y_n - g_t(x_n)\)</span>就是传说中的残差。所以用一阶导数来替代残差真的是神来之笔。这样每次的树直接去学习梯度，而在回归的时候残差跟一阶导数还是一样的，美滋滋。</p>
<p>然后突然有个很不成熟的想法，GBDT是不是很怕one hot的feature呢？！举个例子，如果所有的feature都是one hot的，同时我们每个DT都只有一层，是不是理论上来说，最后有多少feature就有多少树。</p>
<h1 id="xgboost">XGBoost</h1>
<p>嗯，就是一个增强版GBDT。</p>
<p>强在哪呢，boosting是一个加法训练，原来我们用的是一阶导数，而陈天奇则是用了二阶导并加了个正则项。</p>
<p>但是陈天奇做二阶导的目的怎么说呢，我不是非常理解。是inspired by GBDT的RMSE？！这个手算一下，我们的损失函数用RMSE的时候会有一个<span class="math inline">\((\alpha_t f_t(x))^2\)</span>，而这个东西类比泰勒展开的时候就是那个<span class="math inline">\(\Delta x\)</span>，所以怎么说呢，可能大牛是因为看到这个时候灵感乍现，然后尝试用了二阶导。</p>
<p>然后在XGBoost里面的损失函数就可以用二阶泰勒展开来表示： <span class="math display">\[
\Theta = \sum_{i=1}^n [g_i f_t(x_i) + \frac{1}{2} h_i f^2_t(x_i)] + \Omega(f_t)
\]</span> 其中的<span class="math inline">\(g_i\)</span>和<span class="math inline">\(h_i\)</span>分别是一阶导数和二阶导数。这样定义的好处是，以后不论用什么损失函数，总体的优化方向仅仅跟一阶导数和二阶导数有关。那么在这个大框架底下，任意可以二阶导的损失函数都可以放进来了。那么我看了XGB的<a href="https://arxiv.org/pdf/1603.02754.pdf">论文</a>关于为什么取二阶导的描述： <img data-src='https://i.loli.net/2018/10/11/5bbec3573482d.png'> 直观感受上来说吧，就有点像拟牛顿法是梯度下降的一个功能性提升的样子。</p>
<p>另外就是正则项，这里<span class="math inline">\(\Omega(f) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^T w_j^2\)</span>。这里的<span class="math inline">\(T\)</span>是我们有多少叶节点，<span class="math inline">\(w\)</span>是叶节点的权重。这两个都在XGB的参数里面可以调整的，我们都知道正则项会改变模型的保守程度，或者说就是variance。而<span class="math inline">\(w\)</span>在这里就是<span class="math inline">\(f_t(x)\)</span>。这里要是不理解，可以看<a href="http://www.52cs.org/?p=429">陈天奇的这篇中文解释</a>。</p>
<p>其他XGB的优化很多是工程上的优化，这个不是CS科班出身就看不懂了。如何并行化什么的，一脸懵逼。具体可以看陈天奇的论文，数学的部分写的非常美妙。</p>
<h1 id="tiny-xgb的一些坑">tiny XGB的一些坑</h1>
<p>完全自己写太可怕了，在<a href="https://github.com/lancifollia/tinygbt/blob/master/tinygbt.py">tinygbt</a>的基础上直接修改了一下，原作的代码逻辑非常清晰。anyway，虽然我觉得有些地方貌似是有问题的，做了一点小改动，这个是<a href="https://github.com/SamaelChen/hexo-practice-code/blob/master/fun/tinyxgb.ipynb">我的修改版</a>。</p>
<p>一个事情就是原作的模型里面learning rate是会越跑越小的，我改成了固定的。另外算split gain的时候我按照论文的公式写的。</p>
<p>还有就是我改了原作的梯度和loss，这样我可以传一些其他损失函数进去，虽然我没试过行不行得通吧。理论上应该OK的吧。XD</p>
<p>然后我觉得原作一个地方我没看懂，就是计算weight的地方，按照论文的公式是<span class="math inline">\(w_j = -\frac{G_j}{H_j + \lambda}\)</span>，但是在这里实现的时候是<span class="math inline">\(\frac{G_j}{H_j + \lambda}\)</span>，就是正负之差。但是很神奇的是我用负号的时候，就不能收敛了。没懂为什么，明明我按照公式求偏导就是负的，实现的时候就不对了。</p>
<p>最后算是一个坑吧，原作数据集用的是LightGBM的<a href="https://github.com/Microsoft/LightGBM/tree/master/examples/regression">测试数据</a>。但是！！！LightGBM的分类和回归用的是同一个数据集，所以，实际上这个回归吧，参考意义也就那样吧。我用的是kaggle上面的pokemon数据集来做回归。</p>
<p>anyway，有人可以发现为什么weight那里会那样的话，给条明路。：)</p>
]]></content>
      <categories>
        <category>统计学习</category>
      </categories>
  </entry>
</search>
