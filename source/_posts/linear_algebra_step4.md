---
title: 线性代数 04
categories: 统计学习
mathjax: true
date: 2018-06-19
---

矩阵乘法

<!-- more -->

矩阵的叉乘运算是高中内容，比较简单：
$$
C = AB \\
c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + \cdots + a_{in}b_{nj}
$$
所以这里我们不去花太多时间讨论计算的事情，这里回顾一下矩阵乘法的意义。另外一般而言，叉乘省略乘号。如果是element-wise multiplication一般会用$\odot$。

矩阵可以看作是一个线性系统，因此，一种看法，我们可以把矩阵乘法看作是一组向量通过一个线性系统变换，得到另一组向量。

另外一种视角是，我们将一个矩阵看作是一个线性变换的函数，那么两个矩阵相乘就可以看做是一个线性变换的组合，或者说是函数的组合。但是这里要注意一点，矩阵相乘前后顺序不一致，得到的结果不一样。

矩阵乘法有一些性质：

1. $s(AC) = (sA)C = A(sC)$
2. $(A + B)C = AC + BC$
3. $C(P + Q) = CP + CQ$
4. $IA = A = AI$
5. $A^k = AAA \cdots A(\text{k times})$
6. $(AC)^{\top} = C^{\top}A^{\top}$

另外矩阵可以做增广，也可以做分块。增广很好理解，跟之前线性方程组做增广矩阵非常像，只要两个矩阵的row相等，就可以拼在一起$[ A \ B ]$

矩阵分块也很好理解，就是将一个很大的矩阵分割成好几个小矩阵。实际上，做partition这个事情的好处是，我们可以在一定程度上减少运算量。如下图：

<img src='https://i.imgur.com/X508XpX.png'>

矩阵的乘法其实并不难，而且现在都可以用机器来计算，一般来说GPU比CPU更擅长算这个，这也是为什么深度学习需要用GPU来加速的原因。
