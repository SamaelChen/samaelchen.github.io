---
title: 线性代数 09
category: 统计学习
mathjax: true
date: 2018-07-03
---

矩阵的特征根与特征向量。这两个是线性代数里面非常重要的概念。

<!-- more -->

我们对这两个概念的定义是这样的，如果存在一个矩阵$A$可以使得常数$\lambda$和向量$v$满足：
$$
Av = \lambda v
$$
那么$\lambda$就是矩阵的特征根，而$v$就是特征向量。但是这里需要注意的是，$A$一定是方阵。举个例子：
$$
\begin{bmatrix}5 & 2 & 1 \\ -2 & 1 & -1 \\ 2 & 2 & 4\end{bmatrix} \begin{bmatrix}1 \\ -1 \\ 1\end{bmatrix} = \begin{bmatrix}4 \\ -4 \\ 4\end{bmatrix} = 4\begin{bmatrix}1 \\ -1 \\ 1\end{bmatrix}
$$

不过要注意的是，零向量不能作为特征向量。

所以直观的感受上，特征向量就是经过线性变换以后，只是改变向量长度或者是变成相反的方向。

那么在特征根特征向量在图像中有什么意义呢？下面举四个例子。

首先是扭曲，如下图：

<img src='https://i.imgur.com/4z9YFVw.png'>

那么这样的变换过程中，落在横轴上的向量是没有发生任何变换的，因此图片中蓝色的向量就是特征向量，特征根是1。

第二个变换是映射，如下：

<img src='https://i.imgur.com/KFeF5NV.png'>

在这个变换中，$b_1$是没有变化的，所以是一个特征根为1的特征向量。而$b_2$则是刚好反了一个方向，因此是一个特征根为-1的特征向量。

第三种变换是缩放，如下：

<img src='https://i.imgur.com/hSWgUwf.png'>

在这种情况下，图片的所有向量都是特征向量，特征根就是缩放倍数。

第四种是旋转，如下：

<img src='https://i.imgur.com/03ETOyW.png'>

因为上面的旋转过程中，没有一个向量保持了原来的方向，或者转到完全相反的方向，因此这样的变换过程中，没有特征向量。

从上面的四个例子我们还可以发现一个很重要的事情，就是一个特征向量只有唯一对应一个特征根，但是一个特征根可以有多个特征向量。然后，我们就可以顺势定义一个新的概念，eigenspace。也就是$\lambda$对应的所有特征向量加上零向量构成的subspace。

那么我们要怎么去找到特征向量和特征根呢？

首先我们回顾一下之前的公式：
$$
Av = \lambda v = \lambda I v
$$
所以$(A-\lambda I) v = 0$。这样一来，我们就知道，当我们知道$\lambda$的时候，只要找到上面那个等式的非零解，就是我们的特征根。

那么如果现在要判断一个常数是不是特征根，我们依照上面那个等式一步步向下推理，因为$(A-\lambda I)v = 0$有多个解，因此我们可以知道$\text{Rank} (A - \lambda I) < n$，所以$A - \lambda I$不可逆，也就是说它的行列式为0。

比如说矩阵$A = \begin{bmatrix}-4 & 3 \\ 3 & 6 \end{bmatrix}$，我们计算行列式$\begin{bmatrix}-4-\lambda & 3 \\ 3 & 6-\lambda \end{bmatrix} = 0$。也就是说$(-4-\lambda)(6-\lambda) - 9 = 0$。所以我们就可以求出来，$\lambda = -3$或$\lambda = 5$。

那么特征根有一些特性。首先，一般来说，一个矩阵跟它的RREF的特征根是不一样的。如果是两个矩阵的因式分解一样，那么就有一样的特征根。

假设现在有个矩阵$A$有$n$个特征根（这里只考虑实数根），那么特征根的和刚好就是$\text{trace } A$，也就是$A$的对角线元素的和；特征根的乘积刚好就是$\det A$。

实际上，理解一下特征根和特征向量在图像中的意义，然后知道特征根的解法是$\det(A - \lambda I)$，特征向量的解法是$(A - \lambda I)v = 0$就好了。
